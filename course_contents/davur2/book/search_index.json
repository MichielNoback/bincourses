[["index.html", "Data Analysis and Visualization using R (2) Preface", " Data Analysis and Visualization using R (2) Michiel Noback 2021-04-15 Preface This is the ebook accompanying the course Data Analysis and Visualization using R 2 (DAVuR2). The book will be available during the final test. These packages and subjects will be covered: ggplot2 tidyr dplyr exploratory data analysis (EDA) working with Timeseries and Dates and times programming R: functions, scripts, packages and dockumentation Note that this entire ebook was written in Markdown, using a RMarkdown extension called bookdown (Xie 2019). This is (sort of) a “gitbook” layer on top of the RMarkdown processing package knitr (Xie 2015). "],["ggplot2.html", "Chapter 1 The ggplot2 package 1.1 Introduction 1.2 Getting started 1.3 ggplot2 and the theory of graphics 1.4 Building plots with ggplot2 1.5 Aesthetics 1.6 Geometries 1.7 Inheritance of aesthetics 1.8 Faceting 1.9 Experimenting with geoms and aesthetics 1.10 Multivariate Categorical Data 1.11 Advanced plotting aspects 1.12 Final tweaks", " Chapter 1 The ggplot2 package 1.1 Introduction In this chapter, we’ll explore the package ggplot2. Package ggplot2 is one of the most popular packages of R, and a de facto standard for creating publishable visualizations. Whole books have been written about ggplot2 (e.g. ggplot2 - Elegant Statistics for Data Aanalysis); these will not be repeated here. Instead, I have selected the minimal amount of information and examples to get you going in your own research visualization endeavors in biomedical research. For that reason, this chapter only deals with the base ggplot() function and its most important usage scenarios. In my opinion, you are best prepared when first learning the ggplot “language” structure, not the complete listing of possibilities. You can check these out later on your own. If you are interested in what the package has to offer, type help(package=\"ggplot2\") on the console. Keep the goal in mind You should always remember the purpose with which you create a plot: Communicate results in a visual way. The audience consists of other professionals: fellow scientists, students, project managers, CEO’s. The scope is in reports, publications, presentations etc. Your plots should be immaculately annotated - have a title and/or caption, axis labels with physical quantities (e.g. Temperature) and measurement units (e.g. Celsius), and a legend (if relevant). Create a representation of data for visual inspection. The audience is yourself. This is especially important in Exploratory Data Analysis (EDA). You visualize your data in order to discover patterns, trends, outliers and to generate new questions and hypotheses. The biggest challenge is to select the correct, most appropriate visualization that keeps you moving on your research track. Besides this, you should of course choose a relevant visualization for your data. For instance, generating a boxplot representing only a few data points is a poor choice, as will a scatterplot for millions of data points almost always be. To help your imagination and see what is possible you should really browse through The R Graph Gallery. It has code for all the charts in the gallery. 1.2 Getting started Install the packages ggplot2 and tidyr first, if not already installed. The package ggplot2 is the topic of this chapter of course. Package tidyr is the topic of a later chapter, but we’ll see a use case of it here already. install.packages(&quot;ggplot2&quot;) install.packages(&quot;tidyr&quot;) After installing, you’ll need to load the packages. library(ggplot2) library(tidyr) A first plot Let’s dive right in and create a first plot, and walk through the different parts of this code. ggplot(data = airquality, mapping = aes(x = Temp, y = Ozone)) + geom_point() ## Warning: Removed 37 rows containing missing values (geom_point). Figure 1.1: A scatter plot visualizing Ozone as a function of Temperature There are two chained function calls: ggplot() and geom_point(). They are chained using the + operator. The first function, ggplot(), creates the base layer of the plot It receives the data and defines how it maps to the two axes. By itself, ggplot(), will not display anything of your data. It creates an empty plot where the axes are defined and have the correct scale: ggplot(data = airquality, mapping = aes(x = Temp, y = Ozone)) Figure 1.2: An empty plot pane The next function, geom_point(), builds on the base layer it receives via the + operator and adds a new layer to the plot, a data representation using points. The geom_point() function encounters rows with missing data and issues a warning (Warning: Removed 37 rows...) but proceeds anyway. There are two ways to prevent this annoying warning message. The first is to put a warning=FALSE statement in the RMarkdown chunk header. This is usually not a good idea because you should be explicit about problem handling when implementing a data analysis workflow because it hinders the reproducibility of your work. Therefore, removing the missing values explicitly is a better solution: airquality &lt;- na.omit(airquality) ggplot(data = airquality, mapping = aes(x = Temp, y = Ozone)) + geom_point() Note that this overwrites the build-in dataset airquality for the duration of this R session. To obtain a similar plot as created above with “base” R, you would have done something like this: with(airquality, plot(x = Temp, y = Ozone)) Figure 1.3: The same visualization with base R You can immediately see why ggplot2 has become so popular. When creating more complex plots it becomes more obvious still, as shown below. Adding a dimension using color This plot shows the power of ggplot2: building complex visualizations with minimal code. airquality$Month_f &lt;- as.factor(airquality$Month) airquality$TempFac &lt;- cut(airquality$Temp, breaks = c(50, 75, 100), labels = c(&quot;low&quot;, &quot;high&quot;), ordered_result = T) ggplot(data = airquality, mapping = aes(x = Temp, y = Ozone, color = Month_f)) + geom_point() Figure 1.4: Ozone as function of Temp with plot symbols colored by Month Inspecting and tuning the figure What can you tell about the data and its measurements when looking at this plot? Looking at the above plot, you should notice that the temperature measurement is probably in degrees Fahrenheit. This should be apparent from the plot. The measurement unit for Ozone is missing. You should look both up; the datasets package doc says it is in Parts Per Billion (ppb). temperature is lowest in the fifth month -probably May but once again you should make certain- and highest in months 8 and 9. ozone levels seem positively correlated with temperature (or Month), but not in an obvious linear way a detail: temperature is measured in whole degrees only. This will give plotting artifacts: discrete vertical lines of data points. The plot below fixes and addresses the above issues to create a publication-ready figure. We’ll get to the details of this code as we proceed in this chapter. For now the message is be meticulous in constructing your plot. airquality$Month_f &lt;- factor(airquality$Month, levels = 1:12, labels = month.abb) ggplot(data = airquality, mapping = aes(x = Temp, y = Ozone)) + geom_point(mapping = aes(color = Month_f)) + geom_smooth(method = &quot;loess&quot;, formula = y ~ x) + #the default formula, but prevents a printed message xlab(expression(&quot;Temperature &quot; (degree~F))) + ylab(&quot;Ozone (ppb)&quot;) + labs(color = &quot;Month&quot;) Figure 1.5: Ozone level dependency on Temperature. Grey area: Loess smoother with 95% confidence interval. Source: R dataset “Daily air quality measurements in New York, May to September 1973.” 1.3 ggplot2 and the theory of graphics Philosophy of ggplot2 The author of ggplot2, Hadley Wickham, had a very clear goal in mind when he embarked on the development of this package: “The emphasis in ggplot2 is reducing the amount of thinking time by making it easier to go from the plot in your brain to the plot on the page.” (Wickham, 2012) The way this is achieved is through “The grammar of graphics” The grammar of graphics The grammar of graphics tells us that a statistical graphic is a mapping from data to geometric objects (points, lines, bars) with aesthetic attributes (color, shape, size). The plot may also contain statistical transformations of the data and is drawn on a specific coordinate system. Faceting -grid layout- can be used to generate the same plot for different subsets of the dataset. (Wickham, 2010) 1.4 Building plots with ggplot2 The layered plot architecture A graph in ggplot2 is built using a few “layers,” or building blocks. Table 1.1: The three core ggplot2 layers ggplot2 description data= the data that you want to plot aes() mappings of data to position (axes), colors, sizes geom_…..() shapes (geometries) that will represent the data First, there is the data layer - the input data that you want to visualize: The data layer Next, using the aes() function, the data is mapped to a coordinate system. This encompasses not only the xy-coordinates but also possible extra plot dimensions such as color and shape. The data and aesthetic layers As a third step, the data is visually represented in some way, using a geometry (dealt with by one of the many geom_....() functions). Examples of geometries are point for scatterplots, boxplot, line etc. The data, aesthetic and geometry layers At a minimum, these three layers are used in every plot you create. Besides these fundamental aspects there are other elements you may wish to add or modify: axis labels, legend, titles, etc. These constitute additional, optional layers: All layers Except for Statistics and Coordinates, each of these layers will be discussed in detail in subsequent paragraphs. “Tidy” the data This is a very important aspect of plotting using ggplot2: getting the data in a way that ggplot2 can deal with it. Sometimes it may be a bit challenging to get the data in such a format: some form of data mangling is often required. This is the topic of a next chapter, but here you’ll already see a little preview. The ggplot2 function expects its data to come in a tidy format. A dataset is considered tidy when it is formed according to these rules: Each variable has its own column. Each observation has its own row. Each value has its own cell. Want to know more about tidy data? Read the paper by Hadley Wickham: (Wickham 2014). Here is an example dataset that requires some mangling, or tidying, to adhere to these rules. This dataset is not tidy because there is an independent variable -the dose- that should have its own column; its value is now buried inside two column headers (dose10mg and dose10mg). Also, there is actually a single variable -the response- that is now split into two columns. Thus, a row now contains two observations. Suppose you want to plot the response as a function of the dose. That is not quite possible right now in ggplot2. This is because you want to do something like ggplot(data=dose_response, mapping = aes(x = &quot;&lt;I want to get the dose levels here&gt;&quot;, y = &quot;&lt;I want to get the response here&gt;&quot;)) + geom_boxplot() The problem is you cannot specify the mapping in a straightforward manner. Note that in base R you would probably do this: boxplot(dose_response$dose10mg, dose_response$dose100mg) Figure 1.6: Selecting untidy data So, we need to tidy this dataframe since the dose_10_response and dose_100_response columns actually describe the same variable (measurement) but with different conditions. As an exercise, I tried it using base R. Here is my solution. tidy_my_df &lt;- function(df) { create_tidy_columns &lt;- function(x) { data.frame(patient = rep(x[1], 2), sex = rep(x[2], 2), dose = c(10, 100), response = c(x[3], x[4])) } tmp &lt;- Reduce(function(x, y) merge(x, y, all=TRUE), apply(X = df, MARGIN = 1, FUN = create_tidy_columns)) tmp[order(tmp$dose), ] } DT::datatable(tidy_my_df(dose_response), options = list(pageLength = 15, dom = &#39;tpli&#39;)) Luckily, there is a very nice package that makes this quite easy: tidyr. Tidying data using tidyr::pivot_longer() ## tidy dose_response_tidy &lt;- pivot_longer(data = dose_response, cols = c(&quot;dose10mg&quot;, &quot;dose100mg&quot;), names_pattern = &quot;dose(\\\\d+)mg&quot;, names_to = &quot;dose&quot;, values_to = &quot;response&quot;) DT::datatable(dose_response_tidy, options = list(pageLength = 15, dom = &#39;tpli&#39;)) The data is tidy now, and ready for use within ggplot2. We’ll explore the pivot_longer() function in detail in a next chapter when discussing the tidyr package. Now, creating the plot in ggplot2 is a breeze dr_plot &lt;- ggplot(dose_response_tidy, aes(x = dose, y = response)) dr_plot + geom_boxplot() Would you proceed with this hypothetical drug? 1.5 Aesthetics After you obtain a tidy dataset and pass it to ggplot you must decide what the aesthetics are: the way the data are represented in your plot. Very roughly speaking, you could correlate the aesthetics to the dimensions of the data you want to visualize. For instance, given this chapters’ first example of the airquality dataset, the aesthetics were defined in three “dimensions”: - dimension “X” for temperature, - dimension “Y” for Ozone - dimension “color” for the month. Although color is used most often to represent an extra dimension in the data, other aesthetics you may consider are shape, size, line width, line type and facetting (making a grid of plots). Colors Colors can be defined in a variety of ways in ggpplot (and R in general): color name existing color palette custom color palette Below is a panel displaying all named colors you can use in R When you provide a literal (character) for the color aesthetic it will simply be that color. If you want to map a property (e.g. “Month”) to a range of colors, you should use a color palette. Since ggplot has build-in color palettes, you can simply use color=&lt;my-third-dimension-variable&gt;. This variable mapping to color can be either a factor (discrete scale) or numeric (continuous scale). The ggplot function will map the variable the default color palette. Be aware that there is a big difference in where you specify an aesthetic. When it should be mapped onto a variable (the values within a column) you should put it within the aes() call. When you want to specify a literal -static- aesthetic (e.g. color) you place it outside the aes() call. When you misplace the mapping you get strange behavior: ggplot(iris, aes(Sepal.Length, Sepal.Width)) + geom_point(aes(color = &#39;Green&#39;)) This will not work either (not evaluated because it gives an error): ggplot(iris, aes(Sepal.Length, Sepal.Width)) + geom_point(color = Species) And when you specify it twice the most ‘specific’ will take precedence (but the legend label is incorrect here): ggplot(data = na.omit(airquality), mapping = aes(x = Ozone, y = Solar.R, color = Month_f)) + geom_point(mapping = aes(color = Day)) Have a look at the paragraph “Inheritance of aesthetics” for more detail. Here are some ways to work with color palettes The default palette #store it in variable &quot;sp&quot; for re-use in subsequenct chunks sp &lt;- ggplot(iris, aes(Sepal.Length, Sepal.Width)) + geom_point(aes(color = Species)) sp Manual palettes You can specify your own colors using scale_color_manual() for scatter plots or scale_fill_manual() for boxplots and bar plots. sp + scale_color_manual(values = c(&quot;#00AFBB&quot;, &quot;#E7B800&quot;, &quot;#FC4E07&quot;)) Here the palette is defined using the hexadecimal notation: Each color can be specified as a mix of Red, Green, and Blue values in a range from 0 to 255. In Hexadecimal notation these are the position 1 and 2 (Red), 3 and 4 (Green) and 5 and 6 (Blue) after the hash sign (#). 00 equals zero and FF equals 255 (16*16). This is quite a universal encoding: a gazillion websites style their pages using this notation. Here is a nice set of colors: custom_col &lt;- c(&quot;#FFDB6D&quot;, &quot;#C4961A&quot;, &quot;#F4EDCA&quot;, &quot;#D16103&quot;, &quot;#C3D7A4&quot;, &quot;#52854C&quot;, &quot;#4E84C4&quot;, &quot;#293352&quot;) show_palette(custom_col, cols=length(custom_col)) Here is a colorblind-friendly palette: # The palette with grey: cbp1 &lt;- c(&quot;#999999&quot;, &quot;#E69F00&quot;, &quot;#56B4E9&quot;, &quot;#009E73&quot;, &quot;#F0E442&quot;, &quot;#0072B2&quot;, &quot;#D55E00&quot;, &quot;#CC79A7&quot;) show_palette(cbp1, cols=length(cbp1)) When you pass a palette that is longer than the number of levels in your factor, R will only use as many as required: sp + scale_color_manual(values = cbp1) RColorBrewer palettes R provides the “RColorBrewer” package. The brewer.pal function has several palettes for various applications at your disposal. Have a look at brewer.pal.info, which lists all: library(RColorBrewer) knitr::kable(brewer.pal.info) maxcolors category colorblind BrBG 11 div TRUE PiYG 11 div TRUE PRGn 11 div TRUE PuOr 11 div TRUE RdBu 11 div TRUE RdGy 11 div FALSE RdYlBu 11 div TRUE RdYlGn 11 div FALSE Spectral 11 div FALSE Accent 8 qual FALSE Dark2 8 qual TRUE Paired 12 qual TRUE Pastel1 9 qual FALSE Pastel2 8 qual FALSE Set1 9 qual FALSE Set2 8 qual TRUE Set3 12 qual FALSE Blues 9 seq TRUE BuGn 9 seq TRUE BuPu 9 seq TRUE GnBu 9 seq TRUE Greens 9 seq TRUE Greys 9 seq TRUE Oranges 9 seq TRUE OrRd 9 seq TRUE PuBu 9 seq TRUE PuBuGn 9 seq TRUE PuRd 9 seq TRUE Purples 9 seq TRUE RdPu 9 seq TRUE Reds 9 seq TRUE YlGn 9 seq TRUE YlGnBu 9 seq TRUE YlOrBr 9 seq TRUE YlOrRd 9 seq TRUE For instance, here is Pastel2: show_palette(brewer.pal(8, &quot;Pastel2&quot;), cols = 8) Here, an RColorBrewer palette is used with the Iris data. ggplot(iris, aes(Sepal.Length, Sepal.Width)) + geom_point(aes(color = Species)) + scale_colour_manual(values=brewer.pal(6, &quot;Dark2&quot;)) #only 3 required Shapes These are the shapes available in ggplot2 (and base R as well). shapes &lt;- data.frame( shape = c(0:19, 22, 21, 24, 23, 20), x = 0:24 %/% 5, y = -(0:24 %% 5) ) ggplot(shapes, aes(x, y)) + geom_point(aes(shape = shape), size = 5, fill = &quot;red&quot;) + geom_text(aes(label = shape), hjust = 0, nudge_x = 0.15) + scale_shape_identity() + #expand_limits(x = 4.1) + theme_void() Warning: do not clutter your plot with too many dimensions/aesthetics! Lines Geoms that draw lines have a “linetype” parameter. Legal values are the strings “blank,” “solid,” “dashed,” “dotted,” “dotdash,” “longdash,” and “twodash.” Alternatively, the numbers 0 to 6 can be used (0 for “blank,” 1 for “solid,” …). You can set line type to a constant value. For this you use the linetype geom parameter. For instance, geom_line(data=d, mapping=aes(x=x, y=y), linetype=3) sets the line type of all lines in that layer to 3, which corresponds to a dotted line), but you can also use it dynamically. Here is an example where the female and male deaths in the UK for 72 successive months are plotted. The linetype = sex aesthetic could as well have been defined within the global ggplot call. It may be a bit more logical to specify it where it applies to the geom. deaths &lt;- data.frame( month = rep(1:72, times = 2), sex = rep(factor(c(&quot;m&quot;, &quot;f&quot;)), each = 72), deaths = c(mdeaths, fdeaths) ) ggplot(data = deaths, mapping = aes(x = month, y = deaths)) + geom_line(aes(linetype = sex)) Size The size of the plotting symbol can also be used as an extra dimension in your visualization. Here is an example showing the solar radiation of the airquality data as third dimension. ggplot(data = na.omit(airquality), mapping = aes(x = Wind, y = Ozone, size = Solar.R)) + geom_point(color = &quot;red&quot;, alpha = 0.5) + labs(size = &quot;Solar radiation (Lang)&quot;) 1.6 Geometries What are geometries Geometries are the ways data can be visually represented. Boxplot, scatterplot and histogram are a few examples. There are many geoms available in ggplot2; type geom_ in the console and you will get a listing. Even more are available outside the ggplot2 package. Here we’ll only explore the most used geoms in science. Boxplot Boxplot is one of the most-used data visualizations. It displays the 5-number summary containing from bottom to top: minimum, first quartile, median (= second quartile), third quartile, maximum. Outliers, usually defined as more than 1.5 * IQR from the median, are displayed as separate points. Some color was added in the example below. dr_plot &lt;- ggplot(dose_response_tidy, aes(x = dose, y = response)) dr_plot + geom_boxplot(fill=&#39;#E69F00&#39;) Jitter Jitter is a good alternative to boxplot when you have small sample sizes, or discrete measurements with many exact copies, resulting in much overlap. Use the width and height attributes to adjust the jittering. dr_plot + geom_jitter(width = 0.1, height = 0) Note that vertical jitter was set to zero because the y-axis values are already in a continuous scale. You should use vertical jittering only when these have discreet values that otherwise overlap too much. Below, a split over the sexes is added. Suddenly, a dramatic dosage effect becomes apparent that was smoothed out when the two sexes were combined. dr_plot + geom_jitter(width = 0.1, height = 0, aes(colour = sex)) Alternatively, use a grid of plots to emphasize the contrast further. dr_plot + geom_jitter(width = 0.1, height = 0, aes(colour = sex)) + facet_wrap( . ~ sex) Plot overlays: boxplot + jitter This example shows how you can overlay plots on top of each other as much as you like. The order in which you define the layers is the order in which they are stacked on top of each other in the graph. You could use this as a feature: library(gridExtra) dr_plot &lt;- ggplot(dose_response_tidy, aes(x = dose, y = response)) p1 &lt;- dr_plot + geom_boxplot(fill=&#39;#E69F00&#39;) + geom_jitter(width = 0.1, height = 0, size = 2, alpha = 0.4) p2 &lt;- dr_plot + geom_jitter(width = 0.1, height = 0, size = 2, alpha = 0.6) + geom_boxplot(fill=&#39;#E69F00&#39;) grid.arrange(p1, p2, nrow = 1) #create a panel of plots The gridExtra package is discussed in a more complex setting below, in section “Advanced plotting aspects.” Scatterplot: Points The geom_point() function is used to create the good old scatterplot of which we have seen several examples already. Line plots When points can be logically connected it may be a good idea to use a line to visualize trends, as we have seen in the deaths plot in section Aesthetics. If you want both lines and points you need to overlay them. In this example I take it a bit further bu adding the dimension ‘activity’ to the points geom only. This is a typical case for geom_line since the measurements of the two beavers were taken sequentially, for that particular beaver. b1_start &lt;- beaver1[1, &quot;time&quot;] / 60 b2_start &lt;- beaver1[2, &quot;time&quot;] / 60 suppressMessages(library(dplyr)) #uses dplyr (later this course) beaverA &lt;- beaver1 %&gt;% mutate(time_h = seq(from = b1_start, to = b1_start + (nrow(beaver1)*10)/60, length.out = nrow(beaver1))) beaverB &lt;- beaver2 %&gt;% mutate(time_h = seq(from = b2_start, to = b2_start + (nrow(beaver2)*10)/60, length.out = nrow(beaver2))) beavers_all &lt;- bind_rows(beaverA, beaverB) %&gt;% mutate(beaver = c(rep(&quot;1&quot;, nrow(beaverA)), rep(&quot;2&quot;, nrow(beaverB))), activity = factor(activ, levels = c(0, 1), labels = c(&quot;inactive&quot;, &quot;active&quot;))) ggplot(data = beavers_all, aes(x = time_h, y = temp)) + geom_line(aes(linetype = beaver)) + geom_point(aes(color = activity)) + xlab(&quot;time (h)&quot;) + ylab(expression(&#39;Temperature (&#39;*~degree*C*&#39;)&#39;)) Histograms A histogram is a means to visualize the distribution of a dataset, as are boxplot (geom_boxplot()), violin plot (geom_violin()) and density plot (geom_freqpoly()). Here we look at the eruption intervals of the “faithful” geyser. A binwidth argument is used to adjust the number of bins. Alternative use the bins argument. ggplot(data=faithful, mapping = aes(x = waiting)) + geom_histogram(binwidth = 3) There are some statistics available to adjust what is shown on the y axis. The default that is used by geom_histogram is stat(count), so if you don’t specify anything this will be used. But if you want it scaled to a maximum of 1, use stat(count / max(count)). The stat() function is a flag to ggplot2 that you want to use calculated aesthetics produced by the statistic.You can use any transformation of the statistic, e.g. y = stat(log2(count)). ggplot(data=faithful, mapping = aes(x = waiting)) + geom_histogram(binwidth = 3, aes(y = stat(count / max(count)))) + ylab(label = &quot;normalized proportion&quot;) Alternatively, if you want percentages, you can use y = stat(count / sum(count) * 100). ggplot(data=faithful, mapping = aes(x = waiting)) + geom_histogram(binwidth = 3, mapping = aes(y = stat(count / sum(count) * 100))) + ylab(label = &quot;%&quot;) Violin plot A violin plot is a compact display of a continuous distribution. It is a blend of geom_boxplot() and geom_density(): a violin plot is a mirrored density plot displayed in the same way as a boxplot. It is not seen as often as should be. An example best explains. ggplot(data=airquality, mapping = aes(x = Month_f, y = Temp, fill = Month_f)) + geom_violin() + theme(legend.position = &quot;none&quot;) Barplot The bar plot is similar to a histogram in appearance, but quite different in intent. Where a histogram visualizes the density of a continuous variable, a bar plot tries to visualize the counts or weights of distinct groups. Here is a small example where the ten subjects of the sleep dataset have been charted (the x axis), and the extra column provided the height of the bar, split over the two groups. When no weight is provided, the occurrences of the different group levels will be counted and sued as weight. ggplot(data = sleep, mapping = aes(ID)) + geom_bar(aes(weight = extra, fill = group)) Overview of the main geoms There are many geoms and even more outside the ggplot2 package. Here is a small overview of some of them. Table 1.2: Some more geoms function. description geom_abline() Add reference lines to a plot, either horizontal, vertical, or diagonal geom_bar() A bar plot makes the height of the bar proportional to the number of cases in each group geom_density() Computes and draws kernel density estimate, which is a smoothed version of the histogram geom_line() Connects the observations in order of the variable on the x axis geom_path() Connects the observations in the order in which they appear in the data geom_qq() geom_qq and stat_qq produce quantile-quantile plots geom_smooth() Aids the eye in seeing patterns in the presence of overplotting geom_violin() A violin plot is a compact display of a continuous distribution. It is a blend of geom_boxplot() and geom_density() If you want to know them all, simply type ?geom_ and select the one that looks like the thing you want, or go to the tidyverse ggplot2 reference page. 1.7 Inheritance of aesthetics Like the main ggplot() function, every geom_ function accepts its own mapping = aes(...). The mapping is inherited from the ggplot() function so any aes(...) mapping defined in the main ggplot() call applies to all subsequent layers. However, you can specify your own “local” aesthetic mapping within a geom_xxxx(). Aesthetics defined within a geom_ function are scoped to that function call only. In the plot below you see how this works (it is not a nice plot anymore, I know). Note that any aesthetic value specified outside the aes() function is simply a static property (in that scope). ggplot(data = na.omit(airquality), mapping = aes(x = Solar.R, y = Ozone)) + geom_smooth(aes(linetype = Month_f), method = &quot;lm&quot;, formula = y ~ x) + geom_point(aes(color = Month_f), alpha = 0.7) Also note that you can “override” global (ggplot()) aesthetics in geom_xxx() but this can give unexpected behavior, as seen in the paragraph on Color. 1.8 Faceting Faceting is the process of splitting into multiple plots with exactly the same coordinate system where each plot show a subset of the data. It can be applied to any geom. The figure above could be improved slightly with this technique. ggplot(data = airquality, mapping = aes(x = Solar.R, y = Ozone)) + geom_smooth(aes(linetype = Month_f), method = &quot;lm&quot;, formula = y ~ x) + geom_point(aes(color = Month_f), alpha = 0.7) + facet_wrap(. ~ Month_f) 1.9 Experimenting with geoms and aesthetics The process in plotting using ggplot2 is usually very iterative. You start with the base plot, passing it the aesthetic for x and y, as shown above, and then experiment with geometries, colors and faceting. Look at every result and ask yourself what story does is tell? and is this the story I want to tell?. Only after you finish this phase you should apply make-up (labels, texts). Maybe new questions have arisen as a result of the plot you created? 1.10 Multivariate Categorical Data Visualizing multivariate categorical data requires another approach. Scatter- and line plots and histograms are all unsuitable for factor data. Here are some plotting examples that work well for categorical data. Copied and adapted from STHDA site. The first example deals with the builtin dataset HairEyeColor. It is a contingency table and a table object so it must be converted into a dataframe before use. hair_eye_col_df &lt;- as.data.frame(HairEyeColor) head(hair_eye_col_df) ## Hair Eye Sex Freq ## 1 Black Brown Male 32 ## 2 Brown Brown Male 53 ## 3 Red Brown Male 10 ## 4 Blond Brown Male 3 ## 5 Black Blue Male 11 ## 6 Brown Blue Male 50 1.10.1 Bar plots of contingency tables ggplot(hair_eye_col_df, aes(x = Hair, y = Freq)) + geom_bar(aes(fill = Eye), stat = &quot;identity&quot;, color = &quot;white&quot;, position = position_dodge(0.7)) + #causes overlapping bars facet_wrap(~ Sex) 1.10.2 Balloon plot Here is a dataset called housetasks that contains data on who does what tasks within the household. (housetasks &lt;- read.delim( system.file(&quot;demo-data/housetasks.txt&quot;, package = &quot;ggpubr&quot;), row.names = 1)) ## Wife Alternating Husband Jointly ## Laundry 156 14 2 4 ## Main_meal 124 20 5 4 ## Dinner 77 11 7 13 ## Breakfeast 82 36 15 7 ## Tidying 53 11 1 57 ## Dishes 32 24 4 53 ## Shopping 33 23 9 55 ## Official 12 46 23 15 ## Driving 10 51 75 3 ## Finances 13 13 21 66 ## Insurance 8 1 53 77 ## Repairs 0 3 160 2 ## Holidays 0 1 6 153 A balloon plot is an excellent way to visualize this kind of data. The function ggballoonplot() is part of the ggpubr package (“‘ggplot2’ Based Publication Ready Plots”). Have a look at this page for a nice review of its possibilities. ggpubr::ggballoonplot(housetasks, fill = &quot;value&quot;) As you can see the counts map to both size and color. Balloon plots can also be faceted. ggpubr::ggballoonplot(hair_eye_col_df, x = &quot;Hair&quot;, y = &quot;Eye&quot;, size = &quot;Freq&quot;, fill = &quot;Freq&quot;, facet.by = &quot;Sex&quot;, ggtheme = theme_bw()) + scale_fill_viridis_c(option = &quot;C&quot;) 1.10.3 Mosaic plot A mosaic plot (library vcd) scales the tiles according to the count. suppressMessages(library(vcd)) mosaic(HairEyeColor, #needs an object of type table shade = TRUE, legend = TRUE) 1.10.4 Correspondence analysis This type needs at least 3 columns, otherwise you get hard-to solve errors! Row names should not be in the first column, but assigned as row.names. suppressMessages({library(FactoMineR) library(factoextra)}) res.ca &lt;- CA(housetasks, graph = FALSE) # package FactoMineR performs correspondence analysis fviz_ca_biplot(res.ca, repel = TRUE) # package factoextra visualizes 1.11 Advanced plotting aspects 1.11.1 Plot panels from for loops using gridExtra::grid.arrange() Sometimes you may wish to create a panel of plots using a for loop, similarly to the use of par(mfrow = c(rows, cols)) in base R. There are a few caveats to this seemingly simple notion. For instance, to create a set of boxplots for a few columns of the airquality dataset, you would do something like this in base R: # set the number of rows and columns par(mfrow = c(2, 2)) # iterate the column names for (n in names(airquality[, 1:4])) { boxplot(airquality[, n], xlab = n) } # reset par par(mfrow = c(1, 1)) When you naively migrate this structure to a ggplot setting, it will become something like this. par(mfrow = c(2, 2)) for (n in names(airquality[, 1:4])) { plt &lt;- ggplot(data = airquality, mapping = aes(y = n)) + geom_boxplot() + xlab(n) print(plt) } par(mfrow = c(1, 1)) This is surely not the plot you would have expected: a single straight line, and no panel of plots. It turns out you can not use variables as selectors in aes(). You need to use aes_string() for that purpose. Also note that if you omit the print(plt) call this outputs nothing, which is really quite confusing. You need to explicitely print the plot, not implicitly as you normally can. Here is a second version. par(mfrow = c(2, 2)) for (n in names(airquality[, 1:4])) { plt &lt;- ggplot(data = na.omit(airquality), mapping = aes_string(y = n)) + geom_boxplot() + xlab(n) print(plt) } par(mfrow = c(1, 1)) This works as required except for the panel-of-plots part. The mfrow option to par() does not work with ggplot2. This can be fixed through the use of the gridExtra package, together with the base R do.call() function. library(gridExtra) airquality_no_na &lt;- na.omit(airquality) # a list to store the plots my_plots &lt;- list() #use of indices instead of names is important! for (i in 1:4) { n &lt;- names(airquality)[i] #omitting rows with NA for each single column plt &lt;- ggplot(data = airquality_no_na, mapping = aes_string(y = n)) + geom_boxplot() + xlab(n) my_plots[[i]] &lt;- plt # has to be integer, not name! } #use do.call() to process the list in grid.arrange do.call(grid.arrange, c(my_plots, nrow = 2)) So the rules for usage of a for-loop to create a panel of plots: use aes_string() to specify your columns store the plots in a list use grid.arrange() to create the panel, wrapped in the do.call() function. 1.11.2 The GGally::ggPairs() function The ggpairs() function of the GGally package allows you to build a scatterplot matrix just like the base R pairs() function. Scatterplots of each pair of numeric variable are drawn on the left part of the figure. Pearson correlation is displayed on the right. Variable distribution is available on the diagonal. GGally::ggpairs(airquality_no_na[1:4], progress = FALSE) ## Registered S3 method overwritten by &#39;GGally&#39;: ## method from ## +.gg ggplot2 Look at https://www.r-graph-gallery.com/199-correlation-matrix-with-ggally.html for more examples. 1.11.3 Marginal plots using ggExtra::ggMarginal() You can use ggMarginal() to add marginal distributions to the X and Y axis of a ggplot2 scatterplot. It can be done using histogram, boxplot or density plot using the ggExtra package library(ggExtra) # base plot p &lt;- ggplot(airquality, aes(x=Temp, y=Ozone, color=Month_f)) + geom_point() + theme(legend.position=&quot;none&quot;) p1 &lt;- ggMarginal(p, type=&quot;histogram&quot;) ## Warning: Removed 37 rows containing missing values (geom_point). p2 &lt;- ggMarginal(p, type=&quot;density&quot;) ## Warning: Removed 37 rows containing missing values (geom_point). p3 &lt;- ggMarginal(p, type=&quot;boxplot&quot;) ## Warning: Removed 37 rows containing missing values (geom_point). gridExtra::grid.arrange(p1, p2, p3, nrow = 1) See https://www.r-graph-gallery.com/277-marginal-histogram-for-ggplot2.html for more details. 1.12 Final tweaks This section describes aspects that fall outside the standard realm of plot construction. Scales, Coordinates and Annotations Scales and Coordinates are used to adjust the way your data is mapped and displayed. Here, a log10 scale is applied to the y axis using scale_y_log10() and the x axis is reversed (from high to low values instead of low to high) using scale_x_reverse(). ggplot(data = cars, mapping = aes(x = speed, y = dist)) + geom_point() + scale_y_log10() + scale_x_reverse() In other contexts, such as geographic information analysis, the scale is extremely important. The default coordinate system in ggplot2 is coord_cartesian(). In the plot below, a different coordinate system is used. # function to compute standard error of mean se &lt;- function(x) sqrt(var(x)/length(x)) DF &lt;- data.frame(variable = as.factor(1:10), value = log2(2:11)) ggplot(DF, aes(variable, value, fill = variable)) + geom_bar(width = 1, stat = &quot;identity&quot;, color = &quot;white&quot;) + geom_errorbar(aes(ymin = value - se(value), ymax = value + se(value), color = variable), width = .2) + scale_y_continuous(breaks = 0:nlevels(DF$variable)) + coord_polar() Labels You have seen the xlab(), ylab(), and labs() functions at work already. Themes The theme is used to make changes to the overall appearance of the plot. Two approaches exist. The simplest one is selecting a specific theme and make some minor adjustments at most. Here are is the minimal theme where the text sizes have been modified somewhat. ggplot(data = airquality, mapping=aes(x=Temp, y=Ozone)) + geom_point(mapping = aes(color = Month_f)) + geom_smooth(method = &quot;loess&quot;, formula = y ~ x) + xlab(expression(&quot;Temperature &quot; (degree~F))) + ylab(&quot;Ozone (ppb)&quot;) + labs(color = &quot;Month&quot;) + theme_minimal(base_size = 14) ## Warning: Removed 37 rows containing non-finite values (stat_smooth). ## Warning: Removed 37 rows containing missing values (geom_point). Note that if the color = Month_f aesthetic would have been put in the main ggplot call, the smoother would have been split over the Month groups. Alternatively, the theme can be specified completely, as show below. ggplot(data = na.omit(airquality), mapping = aes(x = Temp, y = Ozone)) + geom_point(mapping = aes(color = Month_f)) + geom_smooth(method = &quot;loess&quot;) + xlab(&quot;Temperature (F)&quot;) + ylab(&quot;Ozone (ppb)&quot;) + labs(color = &quot;Month&quot;) + theme(axis.text.x = element_text(size = 12, colour = &quot;blue&quot;, face = &quot;bold&quot;), axis.text.y = element_text(size = 12, colour = &quot;red&quot;, face = &quot;bold&quot;), axis.title.x = element_text(size = 16, colour = &quot;blue&quot;, face = &quot;bold.italic&quot;), axis.title.y = element_text(size = 14, colour = &quot;red&quot;, face = &quot;bold.italic&quot;), axis.line = element_line(colour = &quot;darkblue&quot;, size = 1, linetype = &quot;solid&quot;), panel.background = element_rect(fill = &quot;lightblue&quot;, size = 0.5, linetype = &quot;solid&quot;), panel.grid.minor = element_blank()) ## `geom_smooth()` using formula &#39;y ~ x&#39; As you can see, there are element_text(), element_line() and element_rect() functions to specify these types of plot elements. The element_blank() function can be used in various theme aspects to prevent it from being displayed. 1.12.0.1 Adjust or set global theme You can specify within your document or R session that a certain theme should be used throughout. You can do this by using the theme_set(), theme_update() and theme_replace() functions, or with the esoteric %+replace% operator. Type ?theme_set to find out more. Annotation A final layer that can be added one containing annotations. Annotations are elements that are added manually to the plot. This can be a text label, a fictitious data point, a shaded box or an arrow indicating a region of interest. In the annotate() method, you specify the geom you wish to add (e.g. “text,” “point”) The panel below demonstrates a few. (outlier &lt;- airquality[!is.na(airquality$Ozone) &amp; airquality$Ozone &gt; 150, ]) ## Ozone Solar.R Wind Temp Month Day Month_f TempFac ## 117 168 238 3.4 81 8 25 Aug high ggplot(data = na.omit(airquality), mapping = aes(x = Temp, y = Ozone)) + annotate(&quot;rect&quot;, xmin = 72, xmax = 77, ymin = 0, ymax = 50, alpha = 0.1, color = &quot;blue&quot;, fill = &quot;blue&quot;) + annotate(&quot;point&quot;, x = outlier$Temp, y = outlier$Ozone, color = &quot;darkred&quot;, size = 4, alpha = 0.3) + geom_point(mapping = aes(color = Month_f)) + geom_smooth(method = &quot;loess&quot;, formula = y ~ x) + xlab(&quot;Temperature (F)&quot;) + ylab(&quot;Ozone (ppb)&quot;) + annotate(&quot;text&quot;, x = outlier$Temp, y = outlier$Ozone -5, label = &quot;Outlier&quot;) + annotate(&quot;segment&quot;, x = outlier$Temp + 5, xend = outlier$Temp + 1, y = outlier$Ozone + 4, yend = outlier$Ozone, color = &quot;darkred&quot;, size = 2, arrow = arrow()) Note there is a geom_rectangle() as well, but as I have discovered after much sorrow, it behaves quite unexpectedly when using the alpha = argument on its fill color. For annotation puyrposes you should always use the annotate() function. "],["developing-a-custom-visualization.html", "Chapter 2 Developing a custom visualization 2.1 An experimental PTSD treatment 2.2 Last tweaks: fonts and legend 2.3 The code", " Chapter 2 Developing a custom visualization 2.1 An experimental PTSD treatment This chapter shows the iterative process of building a visualization where both the audience and the data story are taken into consideration. The story revolves around data that was collected in a research effort investigating the effect of some treatment of subjects with PTSD (Post-traumatic stress disorder). Only one variable of that dataset is shown here, a stress score. Since the group size was very low, and there was no control group, statistical analysis was not really feasible. But the question was: is there an indication of positive effect and a reason to continue the investigations? An attempt was made by developing a visualization to answer this question. The data The collected data was a distress score collected at three time points: 0 months (null measure, T0), 3 months (T1) and 12 months (T2) through questionnaires. Table 2.1: Distress data Clientcode T0 T1 T2 1 2.60 2.44 2.16 2 1.84 1.72 1.52 3 2.04 2.12 1.32 4 2.60 1.48 2.08 5 2.08 1.04 2.04 6 2.20 1.84 2.04 7 2.08 1.80 1.44 9 2.28 2.04 1.96 10 3.16 1.68 2.12 11 2.60 2.04 2.44 12 2.28 2.16 2.04 13 1.76 1.88 1.68 14 2.12 1.24 1.04 15 1.96 1.32 2.04 16 2.12 1.40 1.68 17 2.64 2.40 2.12 18 2.64 1.64 1.44 19 1.52 1.60 1.72 20 1.84 1.68 1.16 21 1.44 1.68 1.40 22 2.88 2.00 1.28 23 2.08 2.12 2.64 24 1.80 1.32 1.72 25 1.88 1.56 1.68 26 2.84 2.36 1.80 27 2.32 1.80 2.20 28 1.76 1.56 1.92 29 2.36 1.60 1.72 30 1.80 1.20 1.32 31 2.36 1.96 2.28 32 2.56 2.56 2.52 33 1.72 1.16 1.40 34 2.32 2.00 2.36 You can see this is a really small dataset. Choose a visualization Before starting the visualization, several aspects should be considered: The audience: people do not want to read lots of numbers in a table in this case no knowledge of statistics (and this is usually the case) The data: here, small sample size is an issue this dataset has connected measurements (timeseries-like) For this dataset I chose a jitterplot as basis because it is well suited for small samples. A boxplot tends to be indicative of information that simply is not there with small datasets. Moreover, a boxplot has a certain complexity that people who are not schooled in statistics have problems with. Tidy the data To work with ggplot2, a tidy (“long”) version of the data is required. In the next chapter this will be dealt with in detail. Here the T0, T1 and T2 columns are gathered into a single column because they actually represent a single variable: Time. All measured stress values, also a single variable, are gathered into a single column as well. This causes a flattening of the data (less columns, more rows). distress_data_tidy &lt;- gather(distress_data, key=Timepoint, value=Stress, &quot;T0&quot;, &quot;T1&quot;, &quot;T2&quot;) distress_data_tidy$Timepoint &lt;- factor(distress_data_tidy$Timepoint, ordered = T) knitr::kable(head(distress_data_tidy, n = 10), caption = &quot;Tidied data&quot;) Table 2.2: Tidied data Clientcode Timepoint Stress 1 T0 2.60 2 T0 1.84 3 T0 2.04 4 T0 2.60 5 T0 2.08 6 T0 2.20 7 T0 2.08 9 T0 2.28 10 T0 3.16 11 T0 2.60 A first version This is the first version of the visualization. The jitter has been created with geom_jitter. The plot symbols have been made transparent to keep overlapping points visible. The plot symbols have been made bigger to support embedding in (PowerPoint) presentations. A little horizontal jitter was introduced to have less overlap of the symbols, but not too much - the discrete time points still stand out well. Vertical jitter omitted since the data are already measured in a continuous scale. A typical use case for vertical jitter is when you have discrete (and few) y-axis measurements. ggplot(distress_data_tidy, aes(x=Timepoint, y=Stress)) + geom_jitter(width = 0.1, size = 2, alpha = 0.6) Figure 2.1: A first attempt Add mean and SD To emphasize the trend in the timeseries, means and standard deviations from the mean were added using stat_summary(). Always be aware of the orders of layers of your plot! Here, the stat_summary was placed “below” the plot symbols. Again, size was increased for enhanced visibility in presentations. Why not the median? Because of the audience! Everybody knows what a mean is, but few know what a median is - especially at management level. mean.sd &lt;- function(x) { c(y = mean(x), ymin = (mean(x) - sd(x)), ymax = (mean(x) + sd(x))) } ggplot(distress_data_tidy, aes(x = Timepoint, y = Stress)) + stat_summary(fun.data = mean.sd, color = &quot;darkred&quot;, size = 1.5) + geom_jitter(width = 0.1, size = 2, alpha = 0.6) Figure 2.2: With mean and standard deviation Emphasize worst cases To emphasize the development of subjects who were in the worst shape at the onset of the research (T0), the top 25% with respect to distress score at T0 were highlighted. distress_data$high_at_T0 &lt;- ifelse(distress_data$T0 &gt; quantile(distress_data$T0, 0.75), &quot;Q4&quot;, &quot;Q1-Q3&quot;) distress_data_tidy &lt;- gather(distress_data, key=Timepoint, value=Stress, &quot;T0&quot;, &quot;T1&quot;, &quot;T2&quot;) distress_data_tidy$Timepoint &lt;- factor(distress_data_tidy$Timepoint, ordered = T) knitr::kable(head(distress_data)) Clientcode T0 T1 T2 high_at_T0 1 2.60 2.44 2.16 Q4 2 1.84 1.72 1.52 Q1-Q3 3 2.04 2.12 1.32 Q1-Q3 4 2.60 1.48 2.08 Q4 5 2.08 1.04 2.04 Q1-Q3 6 2.20 1.84 2.04 Q1-Q3 The color is added using aes(color = high_at_T0) within the geom_jitter() call. p &lt;- ggplot(distress_data_tidy, aes(x=Timepoint, y=Stress)) + stat_summary(fun.data=mean.sd, color = &quot;darkred&quot;, size = 1.5) + geom_jitter(width = 0.1, size = 2, alpha = 0.6, aes(color = high_at_T0)) p 2.2 Last tweaks: fonts and legend The plot is more or less ready. Now is the time to adjust the plot “theme.” p + theme_minimal(base_size = 14) + theme(legend.position = &quot;top&quot;) + labs(color=&quot;Group&quot;) 2.3 The code Here is the code used for data preparation: distress_data$high_at_T0 &lt;- ifelse( distress_data$T0 &gt; quantile(distress_data$T0, 0.75), &quot;Q4&quot;, &quot;Q1-Q3&quot;) distress_data_tidy &lt;- gather(distress_data, key=Timepoint, value=Stress, &quot;T0&quot;, &quot;T1&quot;, &quot;T2&quot;) distress_data_tidy$Timepoint &lt;- factor(distress_data_tidy$Timepoint, ordered = T) mean.sd &lt;- function(x) { c(y = mean(x), ymin=(mean(x)-sd(x)), ymax=(mean(x)+sd(x))) } This is the final code for the plot ggplot(distress_data_tidy, aes(x=Timepoint, y=Stress)) + stat_summary(fun.data=mean.sd, color = &quot;darkred&quot;, size = 1.5) + geom_jitter(width = 0.1, size = 2, alpha = 0.6, aes(color = high_at_T0)) + labs(color=&quot;Group&quot;) + theme_minimal(base_size = 14) + theme(legend.position = &quot;top&quot;) + labs(color=&quot;Group&quot;) "],["tidyr.html", "Chapter 3 Package tidyr 3.1 Introduction 3.2 The pivot_longer() function 3.3 A more complex problem: two sets of columns 3.4 Widening with pivot_wider() 3.5 Split columns with separate() 3.6 Combine columns 3.7 Drop rows with missing values: drop_na()", " Chapter 3 Package tidyr 3.1 Introduction This chapter explores the package tidyr. It is also part of the tidyverse set of packages. This package is an essential tool in the process of organizing you data in a tidy way. A dataset is considered tidy when it is formed according to these rules: Each variable has its own column. Each observation has its own row. Each value has its own cell. Here is an example dataset, downloaded from the WHO website here Note: the original data was exported from Excel to csv, and a text editor was used to replace single quote occurrences (“’”) with an underscore. Otherwise, data will be corrupted. The data concerns disease occurrences for two diseases: measles and rubella (“rode hond” in Dutch). Suppose I would like to analyse distribution and timeline of these two diseases, in relation to each other. There are two problems here: 1. The data needs to be clean 2. The data needs to be combined from two files into one dataframe. Let’s start with the first: measles. Here is part of the measles data. It is not tidy. Why not? Table 3.1: Why is this not a tidy dataset? Region ISO3 Country Year January February March AFR AGO Angola 2011 17 19 37 AFR AGO Angola 2012 373 289 381 AFR AGO Angola 2013 725 646 734 AFR AGO Angola 2014 1161 1101 1319 AFR AGO Angola 2015 4 15 0 AFR AGO Angola 2016 3 2 0 AFR AGO Angola 2017 1 7 2 AFR AGO Angola 2018 3 5 8 AFR AGO Angola 2019 120 94 281 AFR BDI Burundi 2011 6 2 8 AFR BDI Burundi 2012 4 3 10 AFR BDI Burundi 2013 0 0 0 AFR BDI Burundi 2014 0 0 0 AFR BDI Burundi 2015 0 0 2 AFR BDI Burundi 2016 0 0 0 The monthly counts are in separate columns. However, they are really all the same variable: measles cases. So this data needs to be tidied: There should be only one column called “cases” and another column called “month.” Or maybe even a single column “Date?” Because year and month are actually elements of a single unit of course. We’ll leave that for a later chapter. 3.2 The pivot_longer() function Tidying has never been simpler, using the pivot_longer function: measles_tidy &lt;- pivot_longer(data = measles, cols = 5:16, # or use -(1:4) names_to = &quot;Month&quot;, values_to = &quot;Cases&quot;) knitr::kable(head(measles_tidy, n=15)) Region ISO3 Country Year Month Cases AFR AGO Angola 2011 January 17 AFR AGO Angola 2011 February 19 AFR AGO Angola 2011 March 37 AFR AGO Angola 2011 April 41 AFR AGO Angola 2011 May 11 AFR AGO Angola 2011 June 8 AFR AGO Angola 2011 July 5 AFR AGO Angola 2011 August 4 AFR AGO Angola 2011 September 32 AFR AGO Angola 2011 October 10 AFR AGO Angola 2011 November 8 AFR AGO Angola 2011 December 0 AFR AGO Angola 2012 January 373 AFR AGO Angola 2012 February 289 AFR AGO Angola 2012 March 381 In the pivot_longer function, you provide three pieces of information: You need to tell which columns to collect the data and headers from. Here, all the month columns are collected: cols = 5:16. you can also use the names of the columns you want to gather. The name of the column to hold the “old” column headers: names_to. This is the name of the newly created column that will hold the information that is now present in the column headers that you are going to collect - in this case, the months names January-December. Hence, names_to = Month. The name for values_to. The value_to argument is the name of the column that will hold the actual measurements. In this case, the number of cases from each of the 12 month columns will be “gathered” in this column. Hence, values_to = Cases. The result is a “flattened but elongated” data structure. All data from the non-gathered columns (Region, ISO3, Country, and Year) will be expanded/duplicated for each of the 12 month rows that will be created. Here you can see the numbers as verification of the process: ## The original dimensions dim(measles) ## [1] 1746 16 ## The tidied data dim(measles_tidy) ## [1] 20952 6 ## 12 times 1746 is 20952 rows nrow(measles_tidy) / 12 ## [1] 1746 Obviously, the data you gather into a single column must be a single measurement type! As an extra example, here is the dose_response data again. dose_response &lt;- read.table(&quot;data/dose_response2.txt&quot;, header = T, sep=&quot;;&quot;, colClasses=c(&quot;character&quot;, &quot;factor&quot;, &quot;integer&quot;, &quot;integer&quot;)) names(dose_response) ## [1] &quot;patient&quot; &quot;sex&quot; &quot;dose10mg&quot; &quot;dose100mg&quot; It is processed by pivot_longer as well. This time, a Regular Expression is used to parse the dose as numbers only out of the header names. tmp &lt;- pivot_longer(data = dose_response, cols = -c(&quot;patient&quot;, &quot;sex&quot;), names_to = &quot;dose&quot;, names_pattern = &quot;dose(10|100)mg&quot;, values_to = &quot;response&quot;) knitr::kable(tmp[1:10, ]) patient sex dose response 001 f 10 12 001 f 100 88 002 f 10 11 002 f 100 54 003 m 10 54 003 m 100 14 004 m 10 71 004 m 100 21 005 f 10 19 005 f 100 89 Note however they are still character values so this requires a conversion, dealt with in chapter dplyr: class(tmp$dose) ## [1] &quot;character&quot; 3.3 A more complex problem: two sets of columns How about a case with two sets of columns? Here, to create an example, a tibble is used. This is an extension of the well-known data.frame type. It is also part of the tidyverse and will be discussed in more detail as well. set.seed(1234) two_two_col &lt;- tibble(subject = letters[1:5], T0_Control = rnorm(5, 10, 1), T0_Treated = rnorm(5, 9.5, 1.2), T1_Control = rnorm(5, 11, 1.5), T1_Treated = rnorm(5, 16, 2)) knitr::kable(two_two_col) subject T0_Control T0_Treated T1_Control T1_Treated a 8.79 10.11 10.28 15.8 b 10.28 8.81 9.50 15.0 c 11.08 8.84 9.84 14.2 d 7.65 8.82 11.10 14.3 e 10.43 8.43 12.44 20.8 The measurements are spread over 4 columns and the column names actually represent the levels of two variables: Treatment and Time. Thus, to be tidy we need to mangle the data in this form: subject time treatment response a T0 Control 10.764 a T0 Treated 8.681 a T1 Control 13.057 a T1 Treated 15.188 ... To solve this it requires a bit of knowledge of regular expressions (fortunately dealt with in a previous course): dose_response_long &lt;- pivot_longer(data = two_two_col, cols = -&quot;subject&quot;, names_pattern = &quot;(T.)_(Control|Treated)&quot;, names_to = c(&quot;Time&quot;, &quot;Treatment&quot;), values_to = &quot;Response&quot;) knitr::kable(head(dose_response_long, 8)) subject Time Treatment Response a T0 Control 8.79 a T0 Treated 10.11 a T1 Control 10.28 a T1 Treated 15.78 b T0 Control 10.28 b T0 Treated 8.81 b T1 Control 9.50 b T1 Treated 14.98 The expression names_pattern = \"(T.).(Control|Treated)\" tells the function there are two groups in the column names to be widened. These groups are defined by the two sets of parentheses (). The first group is defined by the letter “T” follows by a single character specified by the dot . which says “any character.” The second group is (Control|Treated) which tells the function that it is formed by the word Control or Treated. In between is a random single underscore \"_\". Alternatively, the names_sep argument could have been used. For instance, names_sep = 2 tells to split the column names after the second character, yielding this slightly less aesthetic result: alt &lt;- pivot_longer(data = two_two_col, cols = -&quot;subject&quot;, names_sep = 2, names_to = c(&quot;Time&quot;, &quot;Treatment&quot;), values_to = &quot;Response&quot;) knitr::kable(head(alt, 8)) subject Time Treatment Response a T0 _Control 8.79 a T0 _Treated 10.11 a T1 _Control 10.28 a T1 _Treated 15.78 b T0 _Control 10.28 b T0 _Treated 8.81 b T1 _Control 9.50 b T1 _Treated 14.98 3.4 Widening with pivot_wider() The opposite of pivot_longer() is pivot_wider(). First the simple case of a single column: head(dose_response_long) ## # A tibble: 6 x 4 ## subject Time Treatment Response ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 a T0 Control 8.79 ## 2 a T0 Treated 10.1 ## 3 a T1 Control 10.3 ## 4 a T1 Treated 15.8 ## 5 b T0 Control 10.3 ## 6 b T0 Treated 8.81 Here is the reverse operation with pivot_wider: tmp &lt;- pivot_wider(data = dose_response_long, names_from = c(Time, Treatment), values_from = Response) knitr::kable(head(tmp, 5)) subject T0_Control T0_Treated T1_Control T1_Treated a 8.79 10.11 10.28 15.8 b 10.28 8.81 9.50 15.0 c 11.08 8.84 9.84 14.2 d 7.65 8.82 11.10 14.3 e 10.43 8.43 12.44 20.8 3.5 Split columns with separate() The function separate() will split on the non-word character if no separator is specified. Here a separator is prided telling to split on an X with two other characters on either side. tmp &lt;- data.frame(x = 10:15, a = paste0(1:6, &quot;zXb&quot;, letters[1:6])) tmp ## x a ## 1 10 1zXba ## 2 11 2zXbb ## 3 12 3zXbc ## 4 13 4zXbd ## 5 14 5zXbe ## 6 15 6zXbf separate(data = tmp, col = a, sep = &quot;.X.&quot;, into=c(&quot;b&quot;, &quot;c&quot;)) ## x b c ## 1 10 1 a ## 2 11 2 b ## 3 12 3 c ## 4 13 4 d ## 5 14 5 e ## 6 15 6 f 3.6 Combine columns When you have two columns that should be logically combined into one, use the unite() function. For instance, dates and times are often placed in separate columns but they are two aspects of a single measurement: time. Here is a small example. In a later presentation, working with dates and times will be extensively treated. activity &lt;- data.frame(date = c(&quot;30/12/2019&quot;, &quot;31/12/2019&quot;, &quot;01/01/2020&quot;, &quot;02/01/2020&quot;, &quot;02/01/2020&quot;), time = c(&quot;14:55&quot;, &quot;21:01&quot;, &quot;08:22&quot;, &quot;11:56&quot;, &quot;16:38&quot;), activity = c(34, 48, 5, 19, 22)) unite(activity, &quot;datetime&quot;, date, time, sep = &quot;T&quot;) ## datetime activity ## 1 30/12/2019T14:55 34 ## 2 31/12/2019T21:01 48 ## 3 01/01/2020T08:22 5 ## 4 02/01/2020T11:56 19 ## 5 02/01/2020T16:38 22 3.7 Drop rows with missing values: drop_na() This function is extremely simple and useful. It drops rows with missing from a dataframe and returns a cured copy: head(drop_na(airquality)) ## Ozone Solar.R Wind Temp Month Day Month_f TempFac ## 1 41 190 7.4 67 5 1 May low ## 2 36 118 8.0 72 5 2 May low ## 3 12 149 12.6 74 5 3 May low ## 4 18 313 11.5 62 5 4 May low ## 5 23 299 8.6 65 5 7 May low ## 6 19 99 13.8 59 5 8 May low There is of course an equivalent in base R: na.omit(). "],["dplyr.html", "Chapter 4 Data mangling with package dplyr 4.1 Tibbles 4.2 The chaining operator %&gt;% 4.3 Selecting 4.4 Adding and changing variables 4.5 Operations on groups 4.6 Summarizing and counting 4.7 Combining data", " Chapter 4 Data mangling with package dplyr This package, which is also in the tidyverse, is quite versatile. You can use it for a wide range of activities. Some examples are summarizing data; e.g. counting, ranking, selecting, filtering and sampling cases manipulating data; creating new or changing existing variables combining tables In this chapter only a small selection of this package will be discussed. There is an excellent cheat sheet for this package. You can find it here. For convenience, primarily because this gitbook is also used in offline mode during examinations, I included it here as well: dplyr-data-transformation.pdf Before embarking on an overview of the most important functions, let’s first look at the tibble and the %&gt;% chaining operator. The sections below are copied (and adapted) for in part from the dplyr and tibble vignettes which can be found here and here In this chapter I will often use the term case instead of row and variable instead of column since they more precisely describe the essence. Also, these terms are used more in the tidyverse packages. 4.1 Tibbles Tibbles are a modern take on data frames. They keep the features that have stood the test of time, and drop the features that used to be convenient but are now frustrating (i.e. converting character vectors to factors). There is an entire package dedicated to tibbles, not surprisingly called tibble you usually do not have to load the package because dplyr and tidyr do that already (they depend on it themselves). Use the tibble() constructor to create them as literals. There are several advantages over the old data.frame constructor: It never changes an input’s type (i.e., no more stringsAsFactors = FALSE!). It never adjusts the names of variables: name with space does not become name.with.space. It evaluates its arguments lazily and sequentially: tibble(x = 1:5, y = x ^ 2) ## # A tibble: 5 x 2 ## x y ## &lt;int&gt; &lt;dbl&gt; ## 1 1 1 ## 2 2 4 ## 3 3 9 ## 4 4 16 ## 5 5 25 It never uses row.names(). The whole point of tidy data is to store variables in a consistent way. So it never stores a variable as special attribute. It only recycles vectors of length 1. This is because recycling vectors of greater lengths is a frequent source of bugs. Coercion To complement tibble(), tibble provides as_tibble() to coerce objects into tibbles. 4.1.1 By-row constructor There is a third function, tribble() that you can use to define a table in an alternative way: row-wise. tribble( ~colA, ~colB, &quot;a&quot;, 1, &quot;b&quot;, 2, &quot;c&quot;, 3 ) ## # A tibble: 3 x 2 ## colA colB ## &lt;chr&gt; &lt;dbl&gt; ## 1 a 1 ## 2 b 2 ## 3 c 3 4.1.2 Tibbles vs data frames There are three key differences between tibbles and data frames: printing, subsetting, and recycling rules. Printing - When you print a tibble, it only shows the first ten rows and all the columns that fit on one screen. It also prints an abbreviated description of the column type, and uses font styles and color for highlighting. - You can control the default appearance with options: - options(tibble.print_max = n, tibble.print_min = m): if there are more than n rows, print only the first m rows. Use options(tibble.print_max = Inf) to always show all rows. - options(tibble.width = Inf) will always print all columns, regardless of the width of the screen. Subsetting Tibbles are quite strict about subsetting. [ always returns another tibble. Contrast this with a data frame: sometimes [ returns a data frame and sometimes it just returns a vector. Recycling When constructing a tibble, only values of length 1 are recycled. The first column with length different to one determines the number of rows in the tibble, conflicts lead to an error. This also extends to tibbles with zero rows, which is sometimes important for programming: 4.1.3 The str() equivalent: glimpse() The glimpse() function is the dplyr equivalent of str(): glimpse(dose_response_long) ## Rows: 52 ## Columns: 4 ## $ patient &lt;chr&gt; &quot;001&quot;, &quot;002&quot;, &quot;003&quot;, &quot;004&quot;, &quot;005&quot;, &quot;006&quot;, &quot;007&quot;, &quot;008&quot;, &quot;009… ## $ sex &lt;fct&gt; f, f, m, m, f, f, f, m, f, m, m, m, m, f, m, f, f, m, m, f, … ## $ Dose &lt;chr&gt; &quot;dose10mg&quot;, &quot;dose10mg&quot;, &quot;dose10mg&quot;, &quot;dose10mg&quot;, &quot;dose10mg&quot;, … ## $ Response &lt;int&gt; 12, 11, 54, 71, 19, 22, 23, 68, 30, 83, 72, 48, 67, 13, 73, … 4.2 The chaining operator %&gt;% In any workflow, it happens all the time that you apply some function to a dataframe, store the result in a new variable (or overwrite the first) and apply a second function to this dataframe. And so on. There are two undesirable results with this. The first is cluttered code: many variables; how are you going to name them? Just have a look at the previous chapter and you’ll understand. The second -and much worse if you are working with big dataframes- is cluttering of the environment and memory footprint. This is where the chaining operator comes in. It helps you create clean workflows where intermediate results are only stored when opportune. It comes down to this simple notion: x %&gt;% f(y)) is equivalent to f(x, y) where f is any function. Here is the good old dose-response example again, converted to a tibble. dose_response_long ## # A tibble: 52 x 4 ## patient sex Dose Response ## &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;int&gt; ## 1 001 f dose10mg 12 ## 2 002 f dose10mg 11 ## 3 003 m dose10mg 54 ## 4 004 m dose10mg 71 ## 5 005 f dose10mg 19 ## 6 006 f dose10mg 22 ## 7 007 f dose10mg 23 ## 8 008 m dose10mg 68 ## 9 009 f dose10mg 30 ## 10 010 m dose10mg 83 ## # … with 42 more rows Suppose I want to remove cases with missing values (there aren’t any - this is for the sake of argument), select the female subjects and then calculate the mean response for the two doses. In base R, you could do something like this. dose_response_long_no_na &lt;- na.omit(dose_response_long) dose_response_long_no_na_only_female &lt;- subset(x = dose_response_long_no_na, subset = sex == &quot;f&quot;) aggregate(Response ~ Dose, data = dose_response_long_no_na_only_female, FUN = mean) ## Dose Response ## 1 dose100mg 80.9 ## 2 dose10mg 19.6 I know, I exaggerated a bit with the variable names. Here is the same workflow, using dplyr, but with the intermediate variables. It even has an explicit operation extra (group_by()). dose_response_long_no_na &lt;- drop_na(dose_response_long) dose_response_long_no_na_only_female &lt;- filter(dose_response_long_no_na, sex == &quot;f&quot;) dose_response_long_no_na_only_female_grouped &lt;- group_by(dose_response_long_no_na_only_female, Dose) summarize(dose_response_long_no_na_only_female_grouped, mean_response = mean(Response)) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 2 x 2 ## Dose mean_response ## &lt;chr&gt; &lt;dbl&gt; ## 1 dose100mg 80.9 ## 2 dose10mg 19.6 And, finally, how dplyr is supposed to be used. dose_response_long %&gt;% drop_na() %&gt;% filter(sex == &quot;f&quot;) %&gt;% group_by(Dose) %&gt;% summarize(mean_response = mean(Response)) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 2 x 2 ## Dose mean_response ## &lt;chr&gt; &lt;dbl&gt; ## 1 dose100mg 80.9 ## 2 dose10mg 19.6 Isn’t that a treat for your eyes? A highly readable, minimal piece of code, and what’s more - no environment clogged with data you forget to clean up. Note that drop_na is actually from the tidyr package. However, it works seamlessly in the chaining context of dplyr functions. This is the power of dplyr and the chaining operator! If you do actually want the result stored, you only need to assign to a single variable at the beginning of the chain. Does it work with ggplot2 as well? dose_response_long %&gt;% drop_na() %&gt;% ggplot(mapping = aes(x = sex, y = Response)) + geom_boxplot() + facet_wrap(Dose ~ .) I don’t know about you, but this kind of thing makes me happy! The only thing that bothers me slightly is the + instead of %&gt;% in ggplot2 context. On the other hand it is layering, not chaining what ggplot2 does, so there is clear distinction. You have seen the essence of the tidyverse: clean chained workflows. The sections below are copied (and adapted) for a large part from the dplyr vignette which can be found here Dplyr aims to provide a function for each basic verb of data manipulation: filter() and unique() to select cases based on (the uniqueness of) their values. arrange() to reorder the cases. select() and rename() to select variables based on their names. mutate() and transmute() to add new variables that are functions of existing variables. summarise() to condense multiple values to a single value. sample_n() and sample_frac() to take random samples. 4.3 Selecting 4.3.1 Selecting rows by index: slice() If you simply want to select rows by index, use slice() slice(dose_response_long, 2:4) ## # A tibble: 3 x 4 ## patient sex Dose Response ## &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;int&gt; ## 1 002 f dose10mg 11 ## 2 003 m dose10mg 54 ## 3 004 m dose10mg 71 The functions head() and tail() work as expected; they work with tibbles as well (by overloading) and in the context of chained actions. 4.3.2 Get unique cases with distinct() The distinct() function retains only unique/distinct cases from an input tbl or data.frame. You provide variables to use when determining uniqueness. If there are multiple cases for a given combination of inputs, only the first case will be preserved. If omitted, will use all variables. The .keep_all argument specifies whether all variables in the tbl should be kept. dose_response_long %&gt;% distinct(sex, Dose, .keep_all = T) ## # A tibble: 4 x 4 ## patient sex Dose Response ## &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;int&gt; ## 1 001 f dose10mg 12 ## 2 003 m dose10mg 54 ## 3 001 f dose100mg 88 ## 4 003 m dose100mg 14 dose_response_long %&gt;% distinct(Dose) ## # A tibble: 2 x 1 ## Dose ## &lt;chr&gt; ## 1 dose10mg ## 2 dose100mg 4.3.3 filter() cases This function is similar to the subset argument of the subset() function. The filter function filter() allows you to select a subset of cases in a data frame. The first argument is the tibble or data frame. The second and subsequent arguments refer to variables within that data frame, selecting cases where the expression is TRUE. dose_response_long %&gt;% filter(Dose == &quot;dose10mg&quot; &amp; Response &gt; 60) ## # A tibble: 7 x 4 ## patient sex Dose Response ## &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;int&gt; ## 1 004 m dose10mg 71 ## 2 008 m dose10mg 68 ## 3 010 m dose10mg 83 ## 4 011 m dose10mg 72 ## 5 013 m dose10mg 67 ## 6 015 m dose10mg 73 ## 7 024 m dose10mg 61 When you want to filter rows based on a regular expression pattern matching a character value you can do something like the chunk below, because the only thing select needs is a logical vector. (t &lt;- tibble(x = c(&quot;abb&quot;, &quot;bbc&quot;, &quot;dbbd&quot;, &quot;aacc&quot;), y = 1:4)) ## # A tibble: 4 x 2 ## x y ## &lt;chr&gt; &lt;int&gt; ## 1 abb 1 ## 2 bbc 2 ## 3 dbbd 3 ## 4 aacc 4 t %&gt;% filter(grepl(&quot;bb&quot;, x)) ## # A tibble: 3 x 2 ## x y ## &lt;chr&gt; &lt;int&gt; ## 1 abb 1 ## 2 bbc 2 ## 3 dbbd 3 Using str_detect() from the stringr tidyverse package this is also possible: t %&gt;% filter(str_detect(x, &quot;bb&quot;)) ## # A tibble: 3 x 2 ## x y ## &lt;chr&gt; &lt;int&gt; ## 1 abb 1 ## 2 bbc 2 ## 3 dbbd 3 4.3.4 Selecting variables: select() This function is similar to the select argument of the subset() function. Choose variables from a table. Closely related to rename() discussed below; select() keeps only the listed variables and rename() keeps all variables. When you use the key = value format this will result in a rename of the variable. select(dose_response_long, patient, gender = sex) ## # A tibble: 52 x 2 ## patient gender ## &lt;chr&gt; &lt;fct&gt; ## 1 001 f ## 2 002 f ## 3 003 m ## 4 004 m ## 5 005 f ## 6 006 f ## 7 007 f ## 8 008 m ## 9 009 f ## 10 010 m ## # … with 42 more rows Use the minus sign when you want to select everything but a variable: select(dose_response_long, -patient, -sex) ## # A tibble: 52 x 2 ## Dose Response ## &lt;chr&gt; &lt;int&gt; ## 1 dose10mg 12 ## 2 dose10mg 11 ## 3 dose10mg 54 ## 4 dose10mg 71 ## 5 dose10mg 19 ## 6 dose10mg 22 ## 7 dose10mg 23 ## 8 dose10mg 68 ## 9 dose10mg 30 ## 10 dose10mg 83 ## # … with 42 more rows ## same as #select(dose_response_long, -c(patient, sex)) You can use the colon operator to indicate a range of variables: select(dose_response_long, patient:Dose) ## # A tibble: 52 x 3 ## patient sex Dose ## &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; ## 1 001 f dose10mg ## 2 002 f dose10mg ## 3 003 m dose10mg ## 4 004 m dose10mg ## 5 005 f dose10mg ## 6 006 f dose10mg ## 7 007 f dose10mg ## 8 008 m dose10mg ## 9 009 f dose10mg ## 10 010 m dose10mg ## # … with 42 more rows 4.3.4.1 Tidyselect helpers Both with select() and rename() and also with mutate_at() you can use the special helper functions of the tidyselect package: starts_with(): Starts with a prefix. ends_with(): Ends with a suffix. contains(): Contains a literal string. matches(): Matches a regular expression. num_range(): Matches a numerical range like x01, x02, x03. one_of(): Matches variable names in a character vector. everything(): Matches all variables. last_col(): Select last variable, possibly with an offset. select(dose_response_long, contains(&quot;o&quot;)) ## # A tibble: 52 x 2 ## Dose Response ## &lt;chr&gt; &lt;int&gt; ## 1 dose10mg 12 ## 2 dose10mg 11 ## 3 dose10mg 54 ## 4 dose10mg 71 ## 5 dose10mg 19 ## 6 dose10mg 22 ## 7 dose10mg 23 ## 8 dose10mg 68 ## 9 dose10mg 30 ## 10 dose10mg 83 ## # … with 42 more rows 4.3.5 Renaming variables: rename() Rename variables from a table. dose_response_long %&gt;% rename(Patient = patient, Gender = sex) ## # A tibble: 52 x 4 ## Patient Gender Dose Response ## &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;int&gt; ## 1 001 f dose10mg 12 ## 2 002 f dose10mg 11 ## 3 003 m dose10mg 54 ## 4 004 m dose10mg 71 ## 5 005 f dose10mg 19 ## 6 006 f dose10mg 22 ## 7 007 f dose10mg 23 ## 8 008 m dose10mg 68 ## 9 009 f dose10mg 30 ## 10 010 m dose10mg 83 ## # … with 42 more rows 4.3.6 Selecting from ranked data The top_n() function makes it easy to select a few cases that based on the ranking of a value: dose_response_long %&gt;% top_n(3, Response) ## # A tibble: 4 x 4 ## patient sex Dose Response ## &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;int&gt; ## 1 005 f dose100mg 89 ## 2 006 f dose100mg 99 ## 3 017 f dose100mg 96 ## 4 021 f dose100mg 89 We see 4 cases returned because the third rank is the same for two cases. This is especially interesting with grouped data: dose_response_long %&gt;% group_by(Dose) %&gt;% top_n(3, Response) ## # A tibble: 7 x 4 ## # Groups: Dose [2] ## patient sex Dose Response ## &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;int&gt; ## 1 010 m dose10mg 83 ## 2 011 m dose10mg 72 ## 3 015 m dose10mg 73 ## 4 005 f dose100mg 89 ## 5 006 f dose100mg 99 ## 6 017 f dose100mg 96 ## 7 021 f dose100mg 89 4.3.7 Extract a column as vector Using pull() you can obtain atomic vectors. pull(dose_response_long, Response) ## [1] 12 11 54 71 19 22 23 68 30 83 72 48 67 13 73 20 22 40 57 26 17 29 54 61 57 ## [26] 11 88 54 14 21 89 99 69 31 85 18 37 28 16 79 22 84 96 14 12 63 89 77 21 10 ## [51] 36 80 This is of course the same as dose_response_long[[4]] or dose_response_long$Response but the difference is that pull() can be applied in a %&gt;% pipeline. With dose_response_long[, 4] it matters whether you are working with a tibble or a dataframe; a tibble returns a tibble and a dataframe returns a vector. 4.3.8 Sorting with arrange() If you want to sort the rows of a dataframe/tibble by the values of one or more columns, use arrange() dose_response_long %&gt;% arrange(Response) %&gt;% slice(1:3) ## # A tibble: 3 x 4 ## patient sex Dose Response ## &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;int&gt; ## 1 024 m dose100mg 10 ## 2 002 f dose10mg 11 ## 3 026 f dose10mg 11 Use the function desc() to reverse the ordering dose_response_long %&gt;% arrange(desc(Response)) %&gt;% head(1) ## # A tibble: 1 x 4 ## patient sex Dose Response ## &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;int&gt; ## 1 006 f dose100mg 99 Note that the previous chunk is equivalent to this dose_response_long %&gt;% top_n(1, Response) ## # A tibble: 1 x 4 ## patient sex Dose Response ## &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;int&gt; ## 1 006 f dose100mg 99 So natural ordering is from low to high, but the top_n() function always orders from high to low. You can reverse this as well using the desc() function. 4.3.9 Random sampling There are two functions available for random sampling: sample_n() and sample_frac(). sample_frac(dose_response_long, 0.05, replace = TRUE) ## # A tibble: 3 x 4 ## patient sex Dose Response ## &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;int&gt; ## 1 014 f dose100mg 79 ## 2 020 f dose10mg 26 ## 3 022 f dose100mg 77 The workings of sample_n() are obvious I guess. 4.4 Adding and changing variables 4.4.1 Window functions A new variable is usually the result of some operation on one or more previous variables. The data in an original variables is processed such that for each old value a new value is generated. Functions that carry out this kind of operation are called window functions. Thus, window functions are functions that take a vector and return another vector of the same length. For instance, the cumsum() function returns the cumulative sum of a numeric vector: cumsum(1:5) ## [1] 1 3 6 10 15 Here are the main window functions. Note that some of them come from base R. Later you will see the use of several of them in concert with the mutate() function. dplyr::lead Copy with values shifted by 1. lead(c(1, 4, 2, 8)) ## [1] 4 2 8 NA dplyr::lag Copy with values lagged by 1. lag(c(1, 4, 2, 8)) ## [1] NA 1 4 2 dplyr::min_rank Ranks on values, from low to high. Use desc() to reverse. min_rank(c(5, 4, 2, 8)) ## [1] 3 2 1 4 dplyr::ntile Bin vector into n buckets. ntile(c(5, 4, 2, 8, 1), 3) ## [1] 2 2 1 3 1 dplyr::between Are values between a and b? between(c(5, 4, 2, 8, 1), 3, 5) ## [1] TRUE TRUE FALSE FALSE FALSE dplyr::cummean Cumulative mean cummean(c(5, 4, 2, 8, 1)) ## [1] 5.00 4.50 3.67 4.75 4.00 cumsum Cumulative sum cumsum(c(5, 4, 2, 8, 1)) ## [1] 5 9 11 19 20 cummax Cumulative maximum cummax(c(5, 4, 2, 8, 1)) ## [1] 5 5 5 8 8 cummin Cumulative minimum cummin(c(5, 4, 2, 8, 1)) ## [1] 5 4 2 2 1 cumprod Cumulative product cumprod(c(5, 4, 2, 8, 1)) ## [1] 5 20 40 320 320 pmax Element-wise maximum pmax(c(5, 4, 2, 8, 1), c(2, 2, 3, 4, 3)) ## [1] 5 4 3 8 3 pmin Element-wise minimum pmin(c(5, 4, 2, 8, 1), c(2, 2, 3, 4, 3)) ## [1] 2 2 2 4 1 4.4.2 Add one or more variables: mutate() The function mutate() can be used to calculate and append one or more columns.The window functions from the previous section are often-used helpers. For instance, given the ChickWeight dataset which shows weight gain for 50 chicks: chicks &lt;- as_tibble(ChickWeight) chicks %&gt;% head(5) ## # A tibble: 5 x 4 ## weight Time Chick Diet ## &lt;dbl&gt; &lt;dbl&gt; &lt;ord&gt; &lt;fct&gt; ## 1 42 0 1 1 ## 2 51 2 1 1 ## 3 59 4 1 1 ## 4 64 6 1 1 ## 5 76 8 1 1 Suppose we want to know the daily weight gain of these chicks (as a challenge, you could try to do this in base R). Using lag() and mutate() this is a breeze (or so it seems): (chicks &lt;- chicks %&gt;% mutate(weight_gain = weight - lag(weight))) ## # A tibble: 578 x 5 ## weight Time Chick Diet weight_gain ## &lt;dbl&gt; &lt;dbl&gt; &lt;ord&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 42 0 1 1 NA ## 2 51 2 1 1 9 ## 3 59 4 1 1 8 ## 4 64 6 1 1 5 ## 5 76 8 1 1 12 ## 6 93 10 1 1 17 ## 7 106 12 1 1 13 ## 8 125 14 1 1 19 ## 9 149 16 1 1 24 ## 10 171 18 1 1 22 ## # … with 568 more rows …but the devil is in the details: chicks %&gt;% slice(10:15) ## # A tibble: 6 x 5 ## weight Time Chick Diet weight_gain ## &lt;dbl&gt; &lt;dbl&gt; &lt;ord&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 171 18 1 1 22 ## 2 199 20 1 1 28 ## 3 205 21 1 1 6 ## 4 40 0 2 1 -165 ## 5 49 2 2 1 9 ## 6 58 4 2 1 9 The transition from chick 1 to chick 2 is not taken into account! So to get the weight gain for each chick, we need to split the data first. This is dealt with in a later section but here is a preview: chicks &lt;- chicks %&gt;% group_by(Chick) %&gt;% #split on chicks mutate(weight_gain = weight - lag(weight)) %&gt;% ungroup() #put together again slice(chicks, 10:15) ## # A tibble: 6 x 5 ## weight Time Chick Diet weight_gain ## &lt;dbl&gt; &lt;dbl&gt; &lt;ord&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 171 18 1 1 22 ## 2 199 20 1 1 28 ## 3 205 21 1 1 6 ## 4 40 0 2 1 NA ## 5 49 2 2 1 9 ## 6 58 4 2 1 9 Can you use a custom function in a mutate context? Of course you can! my_z &lt;- function(x) { abs((abs(x - mean(x)) / sd(x))) } women %&gt;% mutate(z_score = my_z(weight)) %&gt;% head() ## height weight z_score ## 1 58 115 1.402 ## 2 59 117 1.273 ## 3 60 120 1.080 ## 4 61 123 0.886 ## 5 62 126 0.693 ## 6 63 129 0.499 And what’s more, you can make multiple columns in one operation where the calculations for the subsequent columns are interdependent. women %&gt;% mutate(z_score = my_z(weight), z_bin = ntile(z_score, 3)) %&gt;% head() ## height weight z_score z_bin ## 1 58 115 1.402 3 ## 2 59 117 1.273 3 ## 3 60 120 1.080 2 ## 4 61 123 0.886 2 ## 5 62 126 0.693 2 ## 6 63 129 0.499 1 4.4.3 Create new variables based on more columns 4.4.3.1 mutate_all() The mutate_all() function is similar to the baser R apply() function. Its syntax is a bit puzzling at first, and this has not been made easier by the introduction of new ways to code it. Suppose you want to calculate the log2 of all numeric values in the iris dataset. In base R you would probably do it like this: head(apply(iris[, -5], MARGIN = 2, FUN = log2)) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## [1,] 2.35 1.81 0.485 -2.32 ## [2,] 2.29 1.58 0.485 -2.32 ## [3,] 2.23 1.68 0.379 -2.32 ## [4,] 2.20 1.63 0.585 -2.32 ## [5,] 2.32 1.85 0.485 -2.32 ## [6,] 2.43 1.96 0.766 -1.32 When you do it with mutate_all() this is the solution you will encounter most in a Google search: iris %&gt;% select(-Species) %&gt;% mutate_all(funs(log2(.))) %&gt;% head(3) ## Warning: `funs()` is deprecated as of dplyr 0.8.0. ## Please use a list of either functions or lambdas: ## ## # Simple named list: ## list(mean = mean, median = median) ## ## # Auto named with `tibble::lst()`: ## tibble::lst(mean, median) ## ## # Using lambdas ## list(~ mean(., trim = .2), ~ median(., na.rm = TRUE)) ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_warnings()` to see where this warning was generated. ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## 1 2.35 1.81 0.485 -2.32 ## 2 2.29 1.58 0.485 -2.32 ## 3 2.23 1.68 0.379 -2.32 It gives a deprecation warning, so apparently there are new and improved ways to specify the function(s) to be applied. Let’s review these. A simple named list. This is actually much like the apply() approach. iris %&gt;% select(-Species) %&gt;% mutate_all(list(mean = mean)) %&gt;% head(2) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Sepal.Length_mean ## 1 5.1 3.5 1.4 0.2 5.84 ## 2 4.9 3.0 1.4 0.2 5.84 ## Sepal.Width_mean Petal.Length_mean Petal.Width_mean ## 1 3.06 3.76 1.2 ## 2 3.06 3.76 1.2 So this adds 4 additional rows to the selection without Species and generates variable names by appending _mean. This is different from the first approach in that the old variables are kept beside the new ones. When you supply two functions, e.g. list(mean = mean, sd = sd), a variable is added for another four columns (8 in total). Personally I do not like this. I like it when a method is called directly, with parentheses. Auto named with tibble::lst(). This is similar to the previous one; you supply a list of function names. The names of these functions define the postfix of the variable name. iris %&gt;% select(-Species) %&gt;% mutate_all(lst(mean, sd)) %&gt;% head(2) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Sepal.Length_mean ## 1 5.1 3.5 1.4 0.2 5.84 ## 2 4.9 3.0 1.4 0.2 5.84 ## Sepal.Width_mean Petal.Length_mean Petal.Width_mean Sepal.Length_sd ## 1 3.06 3.76 1.2 0.828 ## 2 3.06 3.76 1.2 0.828 ## Sepal.Width_sd Petal.Length_sd Petal.Width_sd ## 1 0.436 1.77 0.762 ## 2 0.436 1.77 0.762 Using lambdas. Lambda’s are anonymous function-like expressions. Although this form is harder to grasp, it does make it easier to pass arguments to the function you want executed by mutate_all (in this case trim = .2 to mean). iris %&gt;% select(-Species) %&gt;% mutate_all(list(~ mean(., trim = 0.2), ~ sd(.))) %&gt;% head(2) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Sepal.Length_mean ## 1 5.1 3.5 1.4 0.2 5.8 ## 2 4.9 3.0 1.4 0.2 5.8 ## Sepal.Width_mean Petal.Length_mean Petal.Width_mean Sepal.Length_sd ## 1 3.04 3.84 1.2 0.828 ## 2 3.04 3.84 1.2 0.828 ## Sepal.Width_sd Petal.Length_sd Petal.Width_sd ## 1 0.436 1.77 0.762 ## 2 0.436 1.77 0.762 The expression ~ mean(., trim = 0.2) is therefore equivalent to something like function (.) mean(., trim = 0.2) so the tilde is shorthand for function (.) Strangely enough, when experimenting further, I find that this simple solution also works for a single function to be applied: iris %&gt;% select(-Species) %&gt;% mutate_all(my_z) %&gt;% # or mutate_all(function(x){abs((abs(x - mean(x)) / sd(x)))}) head(2) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## 1 0.898 1.016 1.34 1.31 ## 2 1.139 0.132 1.34 1.31 Again, this gives only the new variables, not the preexisting ones. It can be concluded that, whatever the approach, a list of executables of some form needs to be provided. Choose the one of your liking. 4.4.3.2 mutate_at() In the previous examples, a selection of the numeric columns was required before mutate_all() could be executed. An alternative approach is to use mutate_at() which has an additional argument for the column selection. iris %&gt;% mutate_at(vars(contains(&quot;.&quot;)), my_z) %&gt;% head(2) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 0.898 1.016 1.34 1.31 setosa ## 2 1.139 0.132 1.34 1.31 setosa The selection can be specified in several ways: mutate_at(vars(-Species), my_z) mutate_at(1:4, my_z) mutate_at(vars(-Species), my_z) mutate_at(vars(matches(\"Width|Length\")), my_z) mutate_at(vars(contains(\".\")), my_z) and there are probably more ways to make a column selection. You can see that the original variables are replaced with the original variables - that is, in the modified copy of the tibble. As you know it is customary in R to return a modified copy, not to overwrite original data unless you are explicitly saying so. 4.4.3.3 mutate_if() Even simpler in the case of the iris data is the mutate_if() function. After all, the entire point was to modify the numeric variables: iris %&gt;% mutate_if(is.numeric, my_z) %&gt;% head(2) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 0.898 1.016 1.34 1.31 setosa ## 2 1.139 0.132 1.34 1.31 setosa 4.4.4 Change a variable: recode() and recode_factor() These two functions help you to quickly change the values of a variable. Here, the Dose variable is overwritten with new -numeric- values. Note that the original dose_response_long tibble has not been modified! head(dose_response_long, 2) ## # A tibble: 2 x 4 ## patient sex Dose Response ## &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;int&gt; ## 1 001 f dose10mg 12 ## 2 002 f dose10mg 11 dose_response_long %&gt;% mutate(Dose = recode(Dose, dose10mg = 10, dose100mg = 100)) %&gt;% head(2) ## # A tibble: 2 x 4 ## patient sex Dose Response ## &lt;chr&gt; &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; ## 1 001 f 10 12 ## 2 002 f 10 11 Similarly, the sex variable can be recoded with the recode_factor() function. In the example below, - the Response variable is created based on the sex variable, - the sex variable is deleted and - the variables are reordered to the original format dose_response_long %&gt;% mutate(Gender = recode_factor(sex, f = &quot;female&quot;, m = &quot;male&quot;), sex = NULL) %&gt;% select(patient, Gender, Dose, Response) %&gt;% head(3) ## # A tibble: 3 x 4 ## patient Gender Dose Response ## &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;int&gt; ## 1 001 female dose10mg 12 ## 2 002 female dose10mg 11 ## 3 003 male dose10mg 54 4.5 Operations on groups Often you want to know something about groups in your data. For instance, in the previous examples with the chickens, it is interesting to know the average weight gain of the chickens for each diet. This is where the group_by() function comes in, and its counterpart ungroup(). One important thing to realize is the group_by() does not split your data. instead, it adds metadata to a tibble (or data.frame) that marks how rows should be grouped. As long as that metadata is there -i.e. you have not called ungroup()- you won’t be able to change the factors of the columns involved in the grouping. If you really want to split, like the base R split() function, you should use the group_split() function. 4.5.1 group_by() and ungroup() This is the most used set of grouping functions. It is usually followed by some summary function -discussed below- but sometimes, as in the case of the weight gain outlined in section 4.4.2, you want to progress with all data combined. In that case you need to ungroup again. Here is the summary use case. No need for ungrouping. dose_response_long %&gt;% group_by(sex) %&gt;% summarise(mean = mean(Response)) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 2 x 2 ## sex mean ## &lt;fct&gt; &lt;dbl&gt; ## 1 f 50.3 ## 2 m 41.7 Of course you can group by multiple variables. dose_response_long %&gt;% group_by(Dose, sex) %&gt;% summarize(mean = mean(Response)) ## `summarise()` regrouping output by &#39;Dose&#39; (override with `.groups` argument) ## # A tibble: 4 x 3 ## # Groups: Dose [2] ## Dose sex mean ## &lt;chr&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 dose100mg f 80.9 ## 2 dose100mg m 21.5 ## 3 dose10mg f 19.6 ## 4 dose10mg m 61.9 After grouping on a variable, you will sometimes be interested in the top or bottom n rows. The head() function will not work then, surprisingly enough. Suppose you want the bottom 2 responses from both the males and females in the dose100mg group. Surprisingly enough this does not work: dose_response_long %&gt;% filter(Dose == &quot;dose100mg&quot;) %&gt;% group_by(sex) %&gt;% arrange(Response) %&gt;% head(2) %&gt;% ungroup() ## # A tibble: 2 x 4 ## patient sex Dose Response ## &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;int&gt; ## 1 024 m dose100mg 10 ## 2 019 m dose100mg 12 Instead, to select the top n values after grouping you need to use either slice() or filter(row_number() &lt;some-logical-test&gt;). Here is the correct solution. dose_response_long %&gt;% filter(Dose == &quot;dose100mg&quot;) %&gt;% group_by(sex) %&gt;% arrange(Response) %&gt;% slice(1:2) %&gt;% #filter(row_number() %in% 1:2) %&gt;% #also works ungroup() #ungrouping not required but I added it for clarity of code ## # A tibble: 4 x 4 ## patient sex Dose Response ## &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;int&gt; ## 1 002 f dose100mg 54 ## 2 020 f dose100mg 63 ## 3 024 m dose100mg 10 ## 4 019 m dose100mg 12 Here is rather lengthy use case for ungrouping: the z-score is calculated per sex group, and then there is some Dose-specific calculation before summarizing. This required an ungroup in between. dose_response_long %&gt;% group_by(sex) %&gt;% mutate(Response_Z = my_z(Response)) %&gt;% ungroup() %&gt;% ##ungroup for other grouping group_by(Dose) %&gt;% mutate(Response_scaled = scale(Response)) %&gt;% summarize(Z = mean(Response_Z), Scaled_Mean_sd = sd(Response_scaled)) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 2 x 3 ## Dose Z Scaled_Mean_sd ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 dose100mg 0.906 1.00 ## 2 dose10mg 0.912 1.00 4.5.2 group_split() and group_keys() The group_split() function is equivalent to the base R split() function. But it uses the grouping structure from group_by() and therefore is subject to the data mask. Also, it does not name the elements of the list based on the grouping. So in the example below (unevaluated), split() will return a list with two named elements ($m and $f), but the list returned by group_split() will only be accessible with [[1]] and [[2]]. dose_response_long %&gt;% group_split(sex) ##same as split(as.data.frame(dose_response_long), dose_response_long$sex) The last grouping-related function, group_keys(), only returns a tibble explaining the grouping structure: dose_response_long %&gt;% group_keys(sex, Dose) ## Warning: The `...` argument of `group_keys()` is deprecated as of dplyr 1.0.0. ## Please `group_by()` first ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_warnings()` to see where this warning was generated. ## # A tibble: 4 x 2 ## sex Dose ## * &lt;fct&gt; &lt;chr&gt; ## 1 f dose100mg ## 2 f dose10mg ## 3 m dose100mg ## 4 m dose10mg 4.5.3 Apply a function to each group group_map(), group_modify() and group_walk() are functions that can be used to iterate grouped tibbles. group_modify() returns a grouped tibble. Therefore the supplied function must return a data frame/tibble to be able to combine the results of the individual subsets. group_map() returns a list of results, each element being the result of calling the supplied function on each group group_walk() calls the supplied function for side effects and returns the input .tbl, invisibly. This makes it possible to, for instance, print the intermediate results or write them to file without breaking the workflow. Here is an example of group_map(). As you can see it takes a lambda expression as seen before: dose_response_long %&gt;% group_by(Dose) %&gt;% group_map(~ quantile(.x$Response, probs = seq(0, 1, 0.2))) ## [[1]] ## 0% 20% 40% 60% 80% 100% ## 10 18 31 69 85 99 ## ## [[2]] ## 0% 20% 40% 60% 80% 100% ## 11 19 26 54 67 83 Use group_walk() when you only want to have a peek at each group before processing further. dose_response_long %&gt;% group_by(Dose) %&gt;% group_walk(~ print(head(.x, 2))) %&gt;% summarise(avg = mean(Response)) ## # A tibble: 2 x 3 ## patient sex Response ## &lt;chr&gt; &lt;fct&gt; &lt;int&gt; ## 1 001 f 88 ## 2 002 f 54 ## # A tibble: 2 x 3 ## patient sex Response ## &lt;chr&gt; &lt;fct&gt; &lt;int&gt; ## 1 001 f 12 ## 2 002 f 11 ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 2 x 2 ## Dose avg ## &lt;chr&gt; &lt;dbl&gt; ## 1 dose100mg 51.2 ## 2 dose10mg 40.8 4.6 Summarizing and counting You have seen the summarize() function at work a few times. In essence, it creates one or more summary statistics, for each group, if existing. For instance, to calculate the mean and SD of relative weight gain for different diets, this is the workflow for it: chicks %&gt;% group_by(Chick) %&gt;% #split on chicks mutate(rel_weight_gain = (weight - lag(weight))/weight) %&gt;% ungroup() %&gt;% group_by(Diet) %&gt;% summarize(mean_rel_gain = mean(rel_weight_gain, na.rm = T), sd = sd(rel_weight_gain, na.rm = T)) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 4 x 3 ## Diet mean_rel_gain sd ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0.111 0.0743 ## 2 2 0.131 0.0742 ## 3 3 0.153 0.0657 ## 4 4 0.142 0.0748 There is an equivalent that creates a summary for a group of variables: summarize_all(), summarize_at() and summareize_if() which are analogous to the mutate_xxx() methods. Here is one example. iris %&gt;% summarize_if(is.numeric, list(mean = mean, sd = sd)) ## Sepal.Length_mean Sepal.Width_mean Petal.Length_mean Petal.Width_mean ## 1 5.84 3.06 3.76 1.2 ## Sepal.Length_sd Sepal.Width_sd Petal.Length_sd Petal.Width_sd ## 1 0.828 0.436 1.77 0.762 You should be able to figure out the rest of them by now. 4.6.0.1 Simple counting of occurrences An often used operation is simple counting of occurrences of course: dose_response_long %&gt;% group_by(sex, Dose) %&gt;% summarize(count = n()) ## `summarise()` regrouping output by &#39;sex&#39; (override with `.groups` argument) ## # A tibble: 4 x 3 ## # Groups: sex [2] ## sex Dose count ## &lt;fct&gt; &lt;chr&gt; &lt;int&gt; ## 1 f dose100mg 13 ## 2 f dose10mg 13 ## 3 m dose100mg 13 ## 4 m dose10mg 13 There is a shorthand for this type of count: tally(): dose_response_long %&gt;% group_by(sex, Dose) %&gt;% tally() ## # A tibble: 4 x 3 ## # Groups: sex [2] ## sex Dose n ## &lt;fct&gt; &lt;chr&gt; &lt;int&gt; ## 1 f dose100mg 13 ## 2 f dose10mg 13 ## 3 m dose100mg 13 ## 4 m dose10mg 13 And even shorter is this: dose_response_long %&gt;% count(sex, Dose) ## # A tibble: 4 x 3 ## sex Dose n ## &lt;fct&gt; &lt;chr&gt; &lt;int&gt; ## 1 f dose100mg 13 ## 2 f dose10mg 13 ## 3 m dose100mg 13 ## 4 m dose10mg 13 When you want the counts sorted, use the sort = TRUE argument to count, e.g. mtcars %&gt;% count(cyl, sort = TRUE) If you want such a count as a column in your dataset, use add_tally() or add_count() dose_response_long %&gt;% group_by(sex) %&gt;% add_tally() ## # A tibble: 52 x 5 ## # Groups: sex [2] ## patient sex Dose Response n ## &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 001 f dose10mg 12 26 ## 2 002 f dose10mg 11 26 ## 3 003 m dose10mg 54 26 ## 4 004 m dose10mg 71 26 ## 5 005 f dose10mg 19 26 ## 6 006 f dose10mg 22 26 ## 7 007 f dose10mg 23 26 ## 8 008 m dose10mg 68 26 ## 9 009 f dose10mg 30 26 ## 10 010 m dose10mg 83 26 ## # … with 42 more rows ##same as below, but slightly more readable: #add_count(sex) add_count() is useful for groupwise filtering, for instance when you want to show details of the group that occurs the least. mtcars %&gt;% add_count(cyl) %&gt;% filter(n == min(n)) %&gt;% select(1:5, n) ## mpg cyl disp hp drat n ## 1 21.0 6 160 110 3.90 7 ## 2 21.0 6 160 110 3.90 7 ## 3 21.4 6 258 110 3.08 7 ## 4 18.1 6 225 105 2.76 7 ## 5 19.2 6 168 123 3.92 7 ## 6 17.8 6 168 123 3.92 7 ## 7 19.7 6 145 175 3.62 7 4.7 Combining data 4.7.1 Merging There are a few functions, left_join(), right_join(), inner_join(), full_join(); only full_join() is discussed here; the others are variations behaving differently with missing data in one or the other data frames to combine. In base R you have merge() that is responsible for this kind of operations. Suppose you have two tibbles: (trA &lt;- tribble( ~ID, ~var1, &quot;a&quot;, 4, &quot;b&quot;, 6, &quot;c&quot;, 1, )) ## # A tibble: 3 x 2 ## ID var1 ## &lt;chr&gt; &lt;dbl&gt; ## 1 a 4 ## 2 b 6 ## 3 c 1 (trB &lt;- tribble( ~ID, ~var2, &quot;a&quot;, 7, &quot;b&quot;, 3, &quot;d&quot;, 5 )) ## # A tibble: 3 x 2 ## ID var2 ## &lt;chr&gt; &lt;dbl&gt; ## 1 a 7 ## 2 b 3 ## 3 d 5 Since there is a common variable “ID” we can simply combine these two into a single tibble: full_join(trA, trB, by = &quot;ID&quot;) ## # A tibble: 4 x 3 ## ID var1 var2 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 a 4 7 ## 2 b 6 3 ## 3 c 1 NA ## 4 d NA 5 When the column names differ you can specify it as full_join(x, y, by = c(\"a\" = \"b\")) which will match variable a on tibble x to variable b on tibble y. You can of course also do this in a workflow setting: trA %&gt;% full_join(trB, by = &quot;ID&quot;) ## # A tibble: 4 x 3 ## ID var1 var2 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 a 4 7 ## 2 b 6 3 ## 3 c 1 NA ## 4 d NA 5 4.7.2 Adding rows and columns To add rows, use bind_rows() and to add columns, use bind_cols() have a look at the docs for the details. They are pretty much self-explanatory, and analogous to the base R cbind() and rbind() functions. "],["date-time.html", "Chapter 5 Working with date-time data: lubridate 5.1 Current time 5.2 Getting elements from date-time objects 5.3 Calculations with date-time", " Chapter 5 Working with date-time data: lubridate When working in a lab you will often encounter time-measured data. For instance, growth curves, animal behavior observations etc. The tidyverse package lubridate can help you with that. It is quite an expensive package; only the bare essentials will be dealt with here. For a bit more complete overview have a look at the cheat sheet. A small fictitious dataset will be used here to explore some of the concepts. It can be found in file data/time_series_drug_test.txt. ts &lt;- read.table(&quot;data/time_series_drug_test.txt&quot;, header = T, sep = &quot;;&quot;, as.is = 2:3) ts &lt;- as_tibble(ts) ts ## # A tibble: 48 x 5 ## subject date time control response ## &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; &lt;fct&gt; &lt;int&gt; ## 1 S001 29/12/2019 07:44 n 2 ## 2 S001 29/12/2019 12:06 n 68 ## 3 S001 29/12/2019 17:28 n 112 ## 4 S001 29/12/2019 22:01 n 82 ## 5 S001 30/12/2019 07:22 n 63 ## 6 S001 30/12/2019 12:05 n 57 ## 7 S001 30/12/2019 17:55 n 91 ## 8 S001 30/12/2019 22:55 n 48 ## 9 S002 29/12/2019 07:46 n 5 ## 10 S002 29/12/2019 12:09 n 73 ## # … with 38 more rows If you simply want to show the timeseries as a plot, the date and time columns need to be combined first. The tidyr package has unite(), or do it with mutate() ts %&gt;% unite(&quot;date_time&quot;, date, time, sep = &quot;T&quot;) %&gt;% head(3) ## # A tibble: 3 x 4 ## subject date_time control response ## &lt;fct&gt; &lt;chr&gt; &lt;fct&gt; &lt;int&gt; ## 1 S001 29/12/2019T07:44 n 2 ## 2 S001 29/12/2019T12:06 n 68 ## 3 S001 29/12/2019T17:28 n 112 As you can see, the original data columns are lost in this operation. Using mutate() and paste() does keep them intact however. I will take this approach because I want the old columns for demonstration purposes. ts &lt;- ts %&gt;% mutate(date_time = paste(date, time, sep=&quot; &quot;)) head(ts, 3) ## # A tibble: 3 x 6 ## subject date time control response date_time ## &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; &lt;fct&gt; &lt;int&gt; &lt;chr&gt; ## 1 S001 29/12/2019 07:44 n 2 29/12/2019 07:44 ## 2 S001 29/12/2019 12:06 n 68 29/12/2019 12:06 ## 3 S001 29/12/2019 17:28 n 112 29/12/2019 17:28 Let’s reshuffle. I like it when my dependent variable comes last. ts &lt;- ts %&gt;% select(subject, date, time, date_time, control, response) head(ts, 3) ## # A tibble: 3 x 6 ## subject date time date_time control response ## &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;fct&gt; &lt;int&gt; ## 1 S001 29/12/2019 07:44 29/12/2019 07:44 n 2 ## 2 S001 29/12/2019 12:06 29/12/2019 12:06 n 68 ## 3 S001 29/12/2019 17:28 29/12/2019 17:28 n 112 If I attempt to plot the response over time for each of the subjects I get into trouble: ggplot(data = ts, mapping = aes(x = date, y = response, linetype = subject)) + geom_line() Also, when I would like to calculate something like the change per hour in response level over the different measurements this would be difficult indeed. Take a minute to think about this. Do you know any technique in base R that you’ve encountered so far that supports this? Actually there are some functions in baser R that can do this, but we’ll stick to the tidyverse. So this is the area of the lubridate package. I included a library(lubridate) statement at the top of this document so that’s not needed anymore. Let’s start with reading in date and time variables. There are many dedicated (wrapper) functions for reading date and time data. The date variable has the format “Day/Month/Year” so the function dmy() should be fine here: ts &lt;- ts %&gt;% mutate(date = dmy(ts$date)) ts %&gt;% head(3) ## # A tibble: 3 x 6 ## subject date time date_time control response ## &lt;fct&gt; &lt;date&gt; &lt;chr&gt; &lt;chr&gt; &lt;fct&gt; &lt;int&gt; ## 1 S001 2019-12-29 07:44 29/12/2019 07:44 n 2 ## 2 S001 2019-12-29 12:06 29/12/2019 12:06 n 68 ## 3 S001 2019-12-29 17:28 29/12/2019 17:28 n 112 As you can see, the type has changed into date. Let’s try the plot again: ggplot(data = ts, mapping = aes(x = date, y = response, linetype = subject)) + geom_line() Now we’re getting somewhere. The times are not taken into account yet, so it is simply mapped to 00:00 hours on that day. Also, we’re not really interested in the specific date, we want to know the change from T = 0. We’ll get to that. Parsing time is the same as parsing the date, we have an “Hour:Minute” format so the function to take is hm() (there is also an hms()): hm(ts$time)[1:5] ## [1] &quot;7H 44M 0S&quot; &quot;12H 6M 0S&quot; &quot;17H 28M 0S&quot; &quot;22H 1M 0S&quot; &quot;7H 22M 0S&quot; And yes, there is also an dmy_hm() function: dmy_hm(ts$date_time)[1:5] ## [1] &quot;2019-12-29 07:44:00 UTC&quot; &quot;2019-12-29 12:06:00 UTC&quot; ## [3] &quot;2019-12-29 17:28:00 UTC&quot; &quot;2019-12-29 22:01:00 UTC&quot; ## [5] &quot;2019-12-30 07:22:00 UTC&quot; As long as the format is not too esoteric, lubridate will figure out the separator quite well. If you want full control -or simply want to remember only one or two functions- you could take either one of fast_strptime() or parse_date_time(). Both take a format string as argument: a string in which you specify the way your date-time character data are structured The most used symbols are these (although you should really also look at the help page): b or B Abbreviated or full month name in the current locale. The C parser currently understands only English month names. d Day of the month as decimal number (01–31 or 0–31) H Hours as decimal number (00–24 or 0–24). I Hours as decimal number (01–12 or 1–12). m Month as decimal number (01–12 or 1–12). M Minute as decimal number (00–59 or 0–59). p AM/PM indicator in the locale. Normally used in conjunction with I and not with H. S Second as decimal number (00–61 or 0–61). y Year without century (00–99 or 0–99). In parse_date_time() also matches year with century (Y format). Y Year with century. Function parse_date_time() is the most lenient of the two with respect to the format string: parse_date_time(ts$date_time, &quot;d.m-y H:M&quot;)[1:5] ##doesn&#39;t care I have a typo in my format string ## [1] &quot;2019-12-29 07:44:00 UTC&quot; &quot;2019-12-29 12:06:00 UTC&quot; ## [3] &quot;2019-12-29 17:28:00 UTC&quot; &quot;2019-12-29 22:01:00 UTC&quot; ## [5] &quot;2019-12-30 07:22:00 UTC&quot; ts &lt;- ts %&gt;% mutate(date_time = parse_date_time(ts$date_time, &quot;d/m/y H:M&quot;)) head(ts, 3) ## # A tibble: 3 x 6 ## subject date time date_time control response ## &lt;fct&gt; &lt;date&gt; &lt;chr&gt; &lt;dttm&gt; &lt;fct&gt; &lt;int&gt; ## 1 S001 2019-12-29 07:44 2019-12-29 07:44:00 n 2 ## 2 S001 2019-12-29 12:06 2019-12-29 12:06:00 n 68 ## 3 S001 2019-12-29 17:28 2019-12-29 17:28:00 n 112 The fast_strptime() is much more picky, but much faster. You specify the date(-time) format using% sign together with one of the symbols listed above: fast_strptime(&#39;29/12/2019 07:44&#39;, &#39;%d/%m/%Y %H:%M&#39;) ## [1] &quot;2019-12-29 07:44:00 UTC&quot; ## When in tibble context, you need to set `lt = F`... ts %&gt;% mutate(date_time = fast_strptime(date_time, &#39;%d/%m/%Y %H:%M&#39;, lt = F)) I think sticking to the first will suffice. 5.1 Current time The functions today() and now() are your friends. ##output will vary! paste(&quot;the day is &quot;, today()) ## [1] &quot;the day is 2021-04-15&quot; paste(&quot;and to be really exact: &quot;, now()) ## [1] &quot;and to be really exact: 2021-04-15 15:41:47&quot; 5.2 Getting elements from date-time objects Once you have the date-time object processed, you can look at the individual elements. There are -again- many functions related to this. Have a look at the cheat sheet for details. Here are the most-used components: date(x) Date component leaving the time out. year(x) The year. month(x, label, abbr) The month. When called without other argument, you get the month number (e.g. 2). With the label = TRUE argument you will get text (e.g. Feb) and abbr = FALSE you get the full name of the month (February). day(x) The day of the month. wday(x,label,abbr) The day of week. Label and abbr behaves the same as with month(). hour(x) The hour minute(x) Minutes. second(x) Seconds. week(x) Week of the year. A use case demonstration: On what days were the samples taken of the times series used so far? ts %&gt;% group_by(day_of_week = wday(date_time, label=T)) %&gt;% summarize(sample_count = n()) ## `summarise()` ungrouping output (override with `.groups` argument) # or, shorter #ts %&gt;% group_by(day_of_week = wday(date_time, label=T)) %&gt;% tally() ## # A tibble: 2 x 2 ## day_of_week sample_count ## &lt;ord&gt; &lt;int&gt; ## 1 Sun 24 ## 2 Mon 24 What was the mean response on the different times of the day: ts %&gt;% group_by(hour_of_day = hour(date_time)) %&gt;% summarize(mean = mean(response)) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 4 x 2 ## hour_of_day mean ## &lt;int&gt; &lt;dbl&gt; ## 1 7 19.8 ## 2 12 41.5 ## 3 17 68.8 ## 4 22 52 And when split over the control/treatment group as well: ts %&gt;% group_by(control, hour_of_day = hour(date_time)) %&gt;% summarize(mean = median(response), n = n()) ## `summarise()` regrouping output by &#39;control&#39; (override with `.groups` argument) ## # A tibble: 8 x 4 ## # Groups: control [2] ## control hour_of_day mean n ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 n 7 31 6 ## 2 n 12 60 6 ## 3 n 17 97 6 ## 4 n 22 65.5 6 ## 5 y 7 6.5 6 ## 6 y 12 20.5 6 ## 7 y 17 42.5 6 ## 8 y 22 34.5 6 Finally, let’s close off with a plot ggplot(data = ts, mapping = aes(x = date_time, y = response, color = subject)) + geom_line() As practice, you could try to generate a response plot like this where subjects 1 to 3 have corrected values based on the non-treated group. 5.3 Calculations with date-time The example below shows the most basic calculation: the difference between two date-time objects. ts %&gt;% group_by(subject) %&gt;% select(subject, date_time) %&gt;% mutate(lagged = lag(date_time), diff = date_time - lagged) ## # A tibble: 48 x 4 ## # Groups: subject [6] ## subject date_time lagged diff ## &lt;fct&gt; &lt;dttm&gt; &lt;dttm&gt; &lt;drtn&gt; ## 1 S001 2019-12-29 07:44:00 NA NA hours ## 2 S001 2019-12-29 12:06:00 2019-12-29 07:44:00 4.37 hours ## 3 S001 2019-12-29 17:28:00 2019-12-29 12:06:00 5.37 hours ## 4 S001 2019-12-29 22:01:00 2019-12-29 17:28:00 4.55 hours ## 5 S001 2019-12-30 07:22:00 2019-12-29 22:01:00 9.35 hours ## 6 S001 2019-12-30 12:05:00 2019-12-30 07:22:00 4.72 hours ## 7 S001 2019-12-30 17:55:00 2019-12-30 12:05:00 5.83 hours ## 8 S001 2019-12-30 22:55:00 2019-12-30 17:55:00 5.00 hours ## 9 S002 2019-12-29 07:46:00 NA NA hours ## 10 S002 2019-12-29 12:09:00 2019-12-29 07:46:00 4.38 hours ## # … with 38 more rows You can define periods, durations and work with time intervals. However, this is outside the scope of this course. For this more advanced time math you should refer to the cheat sheet or the official docs. "],["stringr.html", "Chapter 6 Processing text with: stringr and regex 6.1 Introduction 6.2 Review of regular expressions 6.3 The stringr essentials", " Chapter 6 Processing text with: stringr and regex 6.1 Introduction This is the last presentation in the tidyverse series. It revolves around processing textual data: finding, extracting, and replacing patterns. Central to this task is pattern matching using regular expressions. Pattern matching is the process of finding, locating, extracting and replacing patterns in character data that usually cannot be literally described. Regular expression syntax is the language in which patterns are described in a wide range of programming languages, including R. This topic has been dealt with in an introductory manner previously (course DAVuR1). And is repeated and expanded here. Instead of the base R functions we now switch to the stringr package. As all packages from the tidyverse, stringr has many many functions (type help(package = \"stringr\") to see which). this package has a great cheat sheet as well. Here, a few of them will be reviewed. 6.1.1 A few remarks on “locale” Many functions of the tidyverse packages related to time and text (and currency) accept arguments specifying the locale. The locale is a container for all location-specific display of information. Think Character set of the language Time zone, Daylight savings time Thousands separator and decimal symbol Currency symbol Dealing with locales is a big challenge indeed for any programming language. However, since this is only an introductory course we will stick to US English and work with the current locale for times only. This note is to make you aware of the concept so that you remember this when the appropriate time comes. 6.2 Review of regular expressions Many of the stringr functions take regular expression as one of the arguments. Regular expression syntax has been dealt with in a previous course/presentation. For your convenience, an overview is presented here as well. 6.2.1 Regex syntax elements A regex can be build out of any combination of character sequences - Literal sequences, such as ‘chimp’ character classes - A listing of possibilities for a single position. alternatives - Are defined by the pipe symbol |. quantifiers - How many times the preceding block should occur. anchors - ^ means matching at the start of a string. $ means at the end. The stringr cheat sheet also contains a summary of regex syntax. 6.2.2 Character classes and negation Characters classes -groups of matching characters for a single position- are placed between brackets: [adgk] means ‘a’ or ‘d’ or ‘g’ or ‘k.’ Use a hyphen to create a series: [3-9] means digits 3 through 9 and [a-zA-Z] means all alphabet characters. Character classes can be negated by putting a ^ at the beginning of the list: [^adgk] means anything but the letters a, d, g or k. There is a special character Since character classes such as [0-9] occur so frequently they have dedicated character classes -also called metacharacters- such as [[:digit:]] or (equivalently) \\\\d. The most important other ones are these any character (wildcard) is specified by .. If you want to search for a literal dot, you need to escape its special meaning using two backslashes: \\\\. digits [[:digit:]] or \\\\d: equivalent to [0-9] alphabet characters [[:alpha:]]: equivalent to [a-zA-Z] lowercase characters [[:lower:]]: equivalent to [a-z] uppercase characters [[:upper:]]: equivalent to [A-Z] whitespace characters [[:space:]] or \\\\s: Space, tab, vertical tab, newline, form feed, carriage return punctuation characters [[:punct:]]: One of !\"#$%&amp;’()*+,-./:;&lt;=&gt;?@[]^_`{|}~ (have a look at the cheat sheet for all) 6.2.3 Quantifiers Quantifiers specify how often a (part of) a pattern should occur. *: 0 or more times +: 1 or more times ?: 0 or 1 time {n}: exactly n times {n,}: at least n times {,n}: at most n times {n, m}: at least n and at most m times. The * zero or more times and ? zero or one time quantifiers are sometimes confusing. Why zero? A good example is the Dutch postal code. These are all valid postal codes pc &lt;- c(&quot;1234 AA&quot;, &quot;2345-BB&quot;, &quot;3456CC&quot;, &quot;4567 dd&quot;) pc ## [1] &quot;1234 AA&quot; &quot;2345-BB&quot; &quot;3456CC&quot; &quot;4567 dd&quot; and therefore a pattern could be \"\\\\d{4}[ -]?[a-zA-Z]{2}\" where the question mark specifies that either a space or a hyphen may occur zero or one time: It may or may not be present. The stringr package provides two nice utility functions to visualize regex matches in a character: str_view_all() and str_view(). The difference is that the latter function only shows the first match - if present. str_view_all(pc, &quot;^\\\\d{4}[ -]?[a-zA-Z]{2}$&quot;) As you can see, the last element (“56789aa”) is not a good postal code. Note that [a-zA-Z] could have been replaced by [[:alpha:]]. 6.2.4 Anchoring Using anchoring, you can make sure the matching string is not longer than you explicitly state. ^ anchors a pattern to the start of a string $ anchors a regex to the end of a string sntc &lt;- &quot;the path of the righteous man is beset on all sides by the iniquities of the selfish, and the tyranny of evil men. --quote from?&quot; str_view(sntc, &quot;evil&quot;) ##matches str_view(sntc, &quot;evil$&quot;) ## does not match 6.2.5 Alternatives To apply two alternative choices for a single regex element you use the pipe symbol |. You can us parentheses (foo[]) to fence alternatives off. str_view_all(sntc, &quot;(y\\\\s)|(\\\\sf)&quot;) 6.3 The stringr essentials 6.3.1 Case conversion These functions all change the capitalization of (some of) the word characters of an input string. They all ignore non-word characters such as punctuation and other symbols. str_to_upper() converts all word characters to uppercase str_to_lower() converts all word characters to lowercase str_to_title() capitalizes all first characters of words str_to_sentence() capitalizes the first character in the string, not after every period str_to_title(sntc) ## [1] &quot;The Path Of The Righteous Man Is Beset On All Sides By The Iniquities Of The Selfish, And The Tyranny Of Evil Men. --Quote From?&quot; str_to_sentence(sntc) ## [1] &quot;The path of the righteous man is beset on all sides by the iniquities of the selfish, and the tyranny of evil men. --quote from?&quot; 6.3.2 Split, join and substring Combining two vectors into one, one vector into one, or doing the reverse: splitting. These are all string-based operation that are carried out in scripting quite often. Here are some joining operations, using str_c(): l1 &lt;- letters[1:5] l2 &lt;- letters[6:10] str_c(l1, collapse = &quot;=&quot;) ## [1] &quot;a=b=c=d=e&quot; str_c(l1, l2, sep = &quot;+&quot;) ## [1] &quot;a+f&quot; &quot;b+g&quot; &quot;c+h&quot; &quot;d+i&quot; &quot;e+j&quot; str_c(l1, l2, sep = &quot;+&quot;, collapse = &quot;=&quot;) ## [1] &quot;a+f=b+g=c+h=d+i=e+j&quot; When you want to combine variables and text str_glue() comes in handy: str_glue(&quot;The value of pi is {pi} and the first month of the year is {month.name[1]}&quot;) ## The value of pi is 3.14159265358979 and the first month of the year is January This is a more friendly approach than with paste(). Splitting is slightly more tricky since it accepts a regex pattern as split argument. For instance, you can get the words of a sentence by splitting like this: words &lt;- str_split(sntc, &quot;([[:punct:]]|[[:space:]])+&quot;) words ##alternative #str_split(sntc, &quot;[^a-zA-Z]+&quot;) ## [[1]] ## [1] &quot;the&quot; &quot;path&quot; &quot;of&quot; &quot;the&quot; &quot;righteous&quot; ## [6] &quot;man&quot; &quot;is&quot; &quot;beset&quot; &quot;on&quot; &quot;all&quot; ## [11] &quot;sides&quot; &quot;by&quot; &quot;the&quot; &quot;iniquities&quot; &quot;of&quot; ## [16] &quot;the&quot; &quot;selfish&quot; &quot;and&quot; &quot;the&quot; &quot;tyranny&quot; ## [21] &quot;of&quot; &quot;evil&quot; &quot;men&quot; &quot;quote&quot; &quot;from&quot; ## [26] &quot;&quot; There are two ways to get parts of character strings, or substrings. The first is by index. You can omit both start and end arguments; they will default to start and end of the string, respectively. nucs &lt;- c(&quot;Adenine&quot;, &quot;Guanine&quot;, &quot;Cytosine&quot;, &quot;Thymine&quot;) str_sub(nucs, end = 3) ## [1] &quot;Ade&quot; &quot;Gua&quot; &quot;Cyt&quot; &quot;Thy&quot; You can even use this function to change the substring that is removed str_sub(nucs, start = 4) &lt;- &quot;......&quot; nucs ## [1] &quot;Ade......&quot; &quot;Gua......&quot; &quot;Cyt......&quot; &quot;Thy......&quot; This does not work with literals! The following chunk gives and error: str_sub(c(&quot;Adenine&quot;, &quot;Guanine&quot;, &quot;Cytosine&quot;, &quot;Thymine&quot;), start = 4) &lt;- &quot;......&quot; ## Error in str_sub(c(&quot;Adenine&quot;, &quot;Guanine&quot;, &quot;Cytosine&quot;, &quot;Thymine&quot;), start = 4) &lt;- &quot;......&quot;: target of assignment expands to non-language object 6.3.3 Matching When you match a pattern to a string, you usually want to know if it is there, which elements have it, where it is located in those elements or how often it is present. For each of these question there is a dedicated function: str_detect(string, pattern) detects the presence of a pattern match in a string. str_detect(fruits, &quot;[Aa]&quot;) ## [1] TRUE TRUE TRUE FALSE str_subset(string, pattern) returns only the strings that contain a pattern match str_subset(fruits, &quot;[Aa]&quot;) ## [1] &quot;Banana&quot; &quot;Apple&quot; &quot;Orange&quot; str_which(string, pattern) finds the indexes of strings that contain a pattern match. str_which(fruits, &quot;[Aa]&quot;) ## [1] 1 2 3 str_count(string, pattern) counts the number of matches in a string. str_count(fruits, &quot;[Aa]&quot;) ## [1] 3 1 1 0 str_locate(string, pattern) and str_locate_all(string, pattern) locate the positions of pattern matches in a string str_locate_all(fruits, &quot;[Aa]&quot;) ## [[1]] ## start end ## [1,] 2 2 ## [2,] 4 4 ## [3,] 6 6 ## ## [[2]] ## start end ## [1,] 1 1 ## ## [[3]] ## start end ## [1,] 3 3 ## ## [[4]] ## start end 6.3.4 Extracting and replacing If you want to obtain the character sequences matching your pattern you can use the str_extract() and str_extract_all() functions: str_extract_all(fruits, &quot;an&quot;) ## [[1]] ## [1] &quot;an&quot; &quot;an&quot; ## ## [[2]] ## character(0) ## ## [[3]] ## [1] &quot;an&quot; ## ## [[4]] ## character(0) Finally, replacing occurrences of a pattern is carried out using str_replace() or str_replace_all(). str_replace_all(fruits, &quot;an&quot;, &quot;..&quot;) ## [1] &quot;B....a&quot; &quot;Apple&quot; &quot;Or..ge&quot; &quot;Cherry&quot; "],["eda.html", "Chapter 7 Exploratory Data Analysis 7.1 Introduction 7.2 Getting to know the dataset 7.3 Exploring variables 7.4 Variable engineering 7.5 The dependent variable 7.6 Exploring relationships between variables 7.7 Look for patterns with the dependent variable 7.8 Density plots show class distinction 7.9 Advanced Explorations", " Chapter 7 Exploratory Data Analysis 7.1 Introduction In this last chapter we’ll step back from the data mangling and have a look at an activity called Exploratory Data Analysis, EDA in short. All theory that is presented in previous chapters will be applied to two datasets In statistics, exploratory data analysis (EDA) is an approach to analyzing data sets to summarize their main characteristics, often with visual methods. A statistical model can be used or not, but primarily EDA is for seeing what the data can tell us beyond the formal modeling or hypothesis testing task. s. (Wikipedia) 7.1.0.1 The Cervical Cancer dataset The UCI machine learning public dataset “Cervical cancer (Risk Factors) Data Set”: “… collected at ‘Hospital Universitario de Caracas’ in Caracas, Venezuela. The dataset comprises demographic information, habits, and historic medical records of 858 patients. Several patients decided not to answer some of the questions because of privacy concerns.” This dataset contains more than thirty variables of 858 subjects who participated in this research. The goal was to identify variables that could possibly be related to cervical cancer. Typically in this type of research there is one dependent variable - the variable you want to “explain.” However in this dataset the last four variables are all markers for cervical cancer. 7.1.0.2 Libraries These are the packages used in this EDA: library(tidyr) library(dplyr) library(ggplot2) library(gridExtra) library(stringr) 7.2 Getting to know the dataset 7.2.1 The codebook A codebook describes the contents, structure, and layout of a dataset. A well-documented codebook contains information intended to be complete and self-explanatory for each variable in a data file. You should always create a file called Codebook.txt to store names, descriptions and types of variables, if it is not yet present alongside the dataset you received. This makes it so much easier to add human-readable labels to plots, and column headers of tables that are supposed to be integrated in a report. The label Hormonal Contraceptives (years) is so much better than Hormonal.Contraceptives..years. Is the following -part of the original codebook downloaded with the data- a good codebook? (int) Age (int) Number of sexual partners (int) First sexual intercourse (age) (int) Num of pregnancies (int) Hormonal Contraceptives (years) (int) IUD (years) (bool) STDs (int) STDs (number) ... (bool) Hinselmann: target variable (bool) Schiller: target variable (bool) Cytology: target variable (bool) Biopsy: target variable 7.2.1.1 Elements of a good codebook entry Name the column name (num.sex.partner) Full name what it abbreviates (“Number of sexual partners”) (optionally) Label the label you want to use in graphs and above tables. Data type One of the R data (derived) data types: int/numeric/Date/factor/boolean… Unit the unit of measurement (e.g. “mg/l plasma”) Description A full description of what is measured, and the way its value was collected (e.g. questionnaire, lab protocol xxxx). Now look again at the codebook above and answer again: is this a good codebook? What if the codebook is not present or not complete? There are several ways to try and fix this: read the original publication see whether this publication has (online) supplements contact the primary investigators. as a last resort: try to deduce from context and domain knowledge I have cleaned it up a bit and came to this. It is still not perfect; how would you improve on this? codebook &lt;- read.csv(&quot;data/cerv_cancer_codebook.csv&quot;, as.is = 1:3) head(codebook) ## abbreviation class full.name ## 1 age int Age ## 2 num.partners int Number of sexual partners ## 3 first.sex int First sexual intercourse (age) ## 4 num.preg int Num of pregnancies ## 5 smoker logical Smokes ## 6 smoke.years int Smokes (years) 7.2.2 Load and inspect the data Load the data and check the data type of the columns. Always beware of unusual encodings for missing data! This is one of the most common causes of erroneous analyses, besides data entry errors and sample swaps. You should get data loading right in an iterative process, using several functions to inspect the result each time you have adjusted a parameter. Use head() and -especially- str() to do this. Tibbles print their data type automatically so that gives an advantage over data.frame. Here is a first attempt: datafile &lt;- &quot;data/risk_factors_cervical_cancer.csv&quot; data &lt;- read.table(datafile, sep=&quot;,&quot;, header = TRUE) str(data) ## &#39;data.frame&#39;: 858 obs. of 36 variables: ## $ Age : int 18 15 34 52 46 42 51 26 45 44 ... ## $ Number.of.sexual.partners : chr &quot;4.0&quot; &quot;1.0&quot; &quot;1.0&quot; &quot;5.0&quot; ... ## $ First.sexual.intercourse : chr &quot;15.0&quot; &quot;14.0&quot; &quot;?&quot; &quot;16.0&quot; ... ## $ Num.of.pregnancies : chr &quot;1.0&quot; &quot;1.0&quot; &quot;1.0&quot; &quot;4.0&quot; ... ## $ Smokes : chr &quot;0.0&quot; &quot;0.0&quot; &quot;0.0&quot; &quot;1.0&quot; ... ## $ Smokes..years. : chr &quot;0.0&quot; &quot;0.0&quot; &quot;0.0&quot; &quot;37.0&quot; ... ## $ Smokes..packs.year. : chr &quot;0.0&quot; &quot;0.0&quot; &quot;0.0&quot; &quot;37.0&quot; ... ## $ Hormonal.Contraceptives : chr &quot;0.0&quot; &quot;0.0&quot; &quot;0.0&quot; &quot;1.0&quot; ... ## $ Hormonal.Contraceptives..years. : chr &quot;0.0&quot; &quot;0.0&quot; &quot;0.0&quot; &quot;3.0&quot; ... ## $ IUD : chr &quot;0.0&quot; &quot;0.0&quot; &quot;0.0&quot; &quot;0.0&quot; ... ## $ IUD..years. : chr &quot;0.0&quot; &quot;0.0&quot; &quot;0.0&quot; &quot;0.0&quot; ... ## $ STDs : chr &quot;0.0&quot; &quot;0.0&quot; &quot;0.0&quot; &quot;0.0&quot; ... ## $ STDs..number. : chr &quot;0.0&quot; &quot;0.0&quot; &quot;0.0&quot; &quot;0.0&quot; ... ## $ STDs.condylomatosis : chr &quot;0.0&quot; &quot;0.0&quot; &quot;0.0&quot; &quot;0.0&quot; ... ## $ STDs.cervical.condylomatosis : chr &quot;0.0&quot; &quot;0.0&quot; &quot;0.0&quot; &quot;0.0&quot; ... ## $ STDs.vaginal.condylomatosis : chr &quot;0.0&quot; &quot;0.0&quot; &quot;0.0&quot; &quot;0.0&quot; ... ## $ STDs.vulvo.perineal.condylomatosis: chr &quot;0.0&quot; &quot;0.0&quot; &quot;0.0&quot; &quot;0.0&quot; ... ## $ STDs.syphilis : chr &quot;0.0&quot; &quot;0.0&quot; &quot;0.0&quot; &quot;0.0&quot; ... ## $ STDs.pelvic.inflammatory.disease : chr &quot;0.0&quot; &quot;0.0&quot; &quot;0.0&quot; &quot;0.0&quot; ... ## $ STDs.genital.herpes : chr &quot;0.0&quot; &quot;0.0&quot; &quot;0.0&quot; &quot;0.0&quot; ... ## $ STDs.molluscum.contagiosum : chr &quot;0.0&quot; &quot;0.0&quot; &quot;0.0&quot; &quot;0.0&quot; ... ## $ STDs.AIDS : chr &quot;0.0&quot; &quot;0.0&quot; &quot;0.0&quot; &quot;0.0&quot; ... ## $ STDs.HIV : chr &quot;0.0&quot; &quot;0.0&quot; &quot;0.0&quot; &quot;0.0&quot; ... ## $ STDs.Hepatitis.B : chr &quot;0.0&quot; &quot;0.0&quot; &quot;0.0&quot; &quot;0.0&quot; ... ## $ STDs.HPV : chr &quot;0.0&quot; &quot;0.0&quot; &quot;0.0&quot; &quot;0.0&quot; ... ## $ STDs..Number.of.diagnosis : int 0 0 0 0 0 0 0 0 0 0 ... ## $ STDs..Time.since.first.diagnosis : chr &quot;?&quot; &quot;?&quot; &quot;?&quot; &quot;?&quot; ... ## $ STDs..Time.since.last.diagnosis : chr &quot;?&quot; &quot;?&quot; &quot;?&quot; &quot;?&quot; ... ## $ Dx.Cancer : int 0 0 0 1 0 0 0 0 1 0 ... ## $ Dx.CIN : int 0 0 0 0 0 0 0 0 0 0 ... ## $ Dx.HPV : int 0 0 0 1 0 0 0 0 1 0 ... ## $ Dx : int 0 0 0 0 0 0 0 0 1 0 ... ## $ Hinselmann : int 0 0 0 0 0 0 1 0 0 0 ... ## $ Schiller : int 0 0 0 0 0 0 1 0 0 0 ... ## $ Citology : int 0 0 0 0 0 0 0 0 0 0 ... ## $ Biopsy : int 0 0 0 0 0 0 1 0 0 0 ... As you can see, many of the variables have been read incorrectly. These can be recognized by the fact that they have been read as Factor instead of the expected int. A closer look at the output also tells me why: missing values are apparently encoded with a question mark. This is fixed in iteration two: data &lt;- read.table(datafile, sep=&quot;,&quot;, header = TRUE, na.strings = &quot;?&quot;) str(data) ## &#39;data.frame&#39;: 858 obs. of 36 variables: ## $ Age : int 18 15 34 52 46 42 51 26 45 44 ... ## $ Number.of.sexual.partners : num 4 1 1 5 3 3 3 1 1 3 ... ## $ First.sexual.intercourse : num 15 14 NA 16 21 23 17 26 20 15 ... ## $ Num.of.pregnancies : num 1 1 1 4 4 2 6 3 5 NA ... ## $ Smokes : num 0 0 0 1 0 0 1 0 0 1 ... ## $ Smokes..years. : num 0 0 0 37 0 ... ## $ Smokes..packs.year. : num 0 0 0 37 0 0 3.4 0 0 2.8 ... ## $ Hormonal.Contraceptives : num 0 0 0 1 1 0 0 1 0 0 ... ## $ Hormonal.Contraceptives..years. : num 0 0 0 3 15 0 0 2 0 0 ... ## $ IUD : num 0 0 0 0 0 0 1 1 0 NA ... ## $ IUD..years. : num 0 0 0 0 0 0 7 7 0 NA ... ## $ STDs : num 0 0 0 0 0 0 0 0 0 0 ... ## $ STDs..number. : num 0 0 0 0 0 0 0 0 0 0 ... ## $ STDs.condylomatosis : num 0 0 0 0 0 0 0 0 0 0 ... ## $ STDs.cervical.condylomatosis : num 0 0 0 0 0 0 0 0 0 0 ... ## $ STDs.vaginal.condylomatosis : num 0 0 0 0 0 0 0 0 0 0 ... ## $ STDs.vulvo.perineal.condylomatosis: num 0 0 0 0 0 0 0 0 0 0 ... ## $ STDs.syphilis : num 0 0 0 0 0 0 0 0 0 0 ... ## $ STDs.pelvic.inflammatory.disease : num 0 0 0 0 0 0 0 0 0 0 ... ## $ STDs.genital.herpes : num 0 0 0 0 0 0 0 0 0 0 ... ## $ STDs.molluscum.contagiosum : num 0 0 0 0 0 0 0 0 0 0 ... ## $ STDs.AIDS : num 0 0 0 0 0 0 0 0 0 0 ... ## $ STDs.HIV : num 0 0 0 0 0 0 0 0 0 0 ... ## $ STDs.Hepatitis.B : num 0 0 0 0 0 0 0 0 0 0 ... ## $ STDs.HPV : num 0 0 0 0 0 0 0 0 0 0 ... ## $ STDs..Number.of.diagnosis : int 0 0 0 0 0 0 0 0 0 0 ... ## $ STDs..Time.since.first.diagnosis : num NA NA NA NA NA NA NA NA NA NA ... ## $ STDs..Time.since.last.diagnosis : num NA NA NA NA NA NA NA NA NA NA ... ## $ Dx.Cancer : int 0 0 0 1 0 0 0 0 1 0 ... ## $ Dx.CIN : int 0 0 0 0 0 0 0 0 0 0 ... ## $ Dx.HPV : int 0 0 0 1 0 0 0 0 1 0 ... ## $ Dx : int 0 0 0 0 0 0 0 0 1 0 ... ## $ Hinselmann : int 0 0 0 0 0 0 1 0 0 0 ... ## $ Schiller : int 0 0 0 0 0 0 1 0 0 0 ... ## $ Citology : int 0 0 0 0 0 0 0 0 0 0 ... ## $ Biopsy : int 0 0 0 0 0 0 1 0 0 0 ... This looks pretty OK to me. Be alert for surprises down the line, though! 7.2.2.1 Should you correct typos? What do you think about this - it was taken from the Attribute Information on the website: (bool) Smokes (years) (bool) Smokes (packs/year) They are advertised as a boolean, but are they? This kind of inspection should make you aware of possible problems that are going to arise later on. Booleans are very different from numbers after all. What is even more dangerous is that they can be treated interchangeably - in R you can treat a logical as an integer and an integer in a boolean context. This kind of discrepancy is the first thing you should address after loading your data. We’ll get back to this shortly. I will now first change the column names to something shorter, using the codebook. Also, the data will be converted into a tibble because this is really a nicer data structure: names(data) &lt;- codebook[,1] data &lt;- as_tibble(data) data ## # A tibble: 858 x 36 ## age num.partners first.sex num.preg smoker smoke.years smokes.packs.ye… ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 18 4 15 1 0 0 0 ## 2 15 1 14 1 0 0 0 ## 3 34 1 NA 1 0 0 0 ## 4 52 5 16 4 1 37 37 ## 5 46 3 21 4 0 0 0 ## 6 42 3 23 2 0 0 0 ## 7 51 3 17 6 1 34 3.4 ## 8 26 1 26 3 0 0 0 ## 9 45 1 20 5 0 0 0 ## 10 44 3 15 NA 1 1.27 2.8 ## # … with 848 more rows, and 29 more variables: horm.contracept.ever &lt;dbl&gt;, ## # horm.contracept.years &lt;dbl&gt;, IUD.ever &lt;dbl&gt;, IUD.years &lt;dbl&gt;, ## # STD.ever &lt;dbl&gt;, STDs.number &lt;dbl&gt;, STD_condylomatosis &lt;dbl&gt;, ## # STD_cervical.cond &lt;dbl&gt;, STD_vaginal.cond &lt;dbl&gt;, STD_vulvo.per.cond &lt;dbl&gt;, ## # STD_syph &lt;dbl&gt;, STD_pid &lt;dbl&gt;, STD_gen.herpes &lt;dbl&gt;, STD_moll.cont &lt;dbl&gt;, ## # STD_aids &lt;dbl&gt;, STD_hiv &lt;dbl&gt;, STD_hepB &lt;dbl&gt;, STD_HPV &lt;dbl&gt;, ## # STD.num.diagn &lt;int&gt;, STD.time.since.first &lt;dbl&gt;, STD.time.since.last &lt;dbl&gt;, ## # Dx.cancer &lt;int&gt;, Dx.cin &lt;int&gt;, Dx.hpv &lt;int&gt;, Dx &lt;int&gt;, ## # target.hinselman &lt;int&gt;, target.schiller &lt;int&gt;, target.cytology &lt;int&gt;, ## # target.biopsy &lt;int&gt; Now let’s have a look at the smoke.years variable: ggplot(data, aes(x=smoke.years)) + geom_histogram(binwidth = 4, na.rm = T) + ylim(0, 50) Figure 7.1: A histogram of the ‘smoke.years’ attribute From the figure it is obvious that this attribute is really the number of years that the subject has smoked. As you can see, the zero-years smoking is absent - this data is only relevant for subjects where smoker == 1. Where 1 is actually not a logical (did you spot that already in the data?). Obviously, smoke.years should be an int . The same counts for variable smokes.packs.year. 7.2.2.2 Getting it right in the end Here, for demonstration purposes I use a wrapper function read.csv() instead of read.table(). The tidyverse has data reading facilities as well of course, but these were not dealt with in this course. data &lt;- read.csv(datafile, na.strings = &quot;?&quot;) names(data) &lt;- codebook[,1] for(i in 1:nrow(codebook)) { if(codebook[i, 2] == &quot;logical&quot;){ data[,i] &lt;- as.logical(data[,i]) } } (data &lt;- as_tibble(data)) ## # A tibble: 858 x 36 ## age num.partners first.sex num.preg smoker smoke.years smokes.packs.ye… ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 18 4 15 1 FALSE 0 0 ## 2 15 1 14 1 FALSE 0 0 ## 3 34 1 NA 1 FALSE 0 0 ## 4 52 5 16 4 TRUE 37 37 ## 5 46 3 21 4 FALSE 0 0 ## 6 42 3 23 2 FALSE 0 0 ## 7 51 3 17 6 TRUE 34 3.4 ## 8 26 1 26 3 FALSE 0 0 ## 9 45 1 20 5 FALSE 0 0 ## 10 44 3 15 NA TRUE 1.27 2.8 ## # … with 848 more rows, and 29 more variables: horm.contracept.ever &lt;lgl&gt;, ## # horm.contracept.years &lt;dbl&gt;, IUD.ever &lt;lgl&gt;, IUD.years &lt;dbl&gt;, ## # STD.ever &lt;lgl&gt;, STDs.number &lt;dbl&gt;, STD_condylomatosis &lt;lgl&gt;, ## # STD_cervical.cond &lt;lgl&gt;, STD_vaginal.cond &lt;lgl&gt;, STD_vulvo.per.cond &lt;lgl&gt;, ## # STD_syph &lt;lgl&gt;, STD_pid &lt;lgl&gt;, STD_gen.herpes &lt;lgl&gt;, STD_moll.cont &lt;lgl&gt;, ## # STD_aids &lt;lgl&gt;, STD_hiv &lt;lgl&gt;, STD_hepB &lt;lgl&gt;, STD_HPV &lt;lgl&gt;, ## # STD.num.diagn &lt;int&gt;, STD.time.since.first &lt;dbl&gt;, STD.time.since.last &lt;dbl&gt;, ## # Dx.cancer &lt;lgl&gt;, Dx.cin &lt;lgl&gt;, Dx.hpv &lt;lgl&gt;, Dx &lt;lgl&gt;, ## # target.hinselman &lt;lgl&gt;, target.schiller &lt;lgl&gt;, target.cytology &lt;lgl&gt;, ## # target.biopsy &lt;lgl&gt; 7.2.2.3 Why such a hassle? It makes it so much easier to run analyses from here on! Also, the code will be more readable making your research more transparent and reproducible. Two very important aspects of any analysis! Besides this, it will make adding labels to plots a breeze. 7.3 Exploring variables 7.3.1 First steps The first phase of any analysis should be to inspect all variables with respect to data distribution and datacorruption. You should also take special care to notice outliers, skewed data and the amount of missing data. These functions and visualizations are used most often for this purpose: summary() quantile() histogram boxplot (or jitter, stripchart) density plot (contingency) tables As you can see, in this phase only univariate analyses and visualizations are employed. 7.3.1.1 Inspecting some columns Here, I will only inspect some columns since this entire process is for demonstration purposes only. I will start with columns 2 and three. summary(data[, 2:3]) ## num.partners first.sex ## Min. : 1.00 Min. :10 ## 1st Qu.: 2.00 1st Qu.:15 ## Median : 2.00 Median :17 ## Mean : 2.53 Mean :17 ## 3rd Qu.: 3.00 3rd Qu.:18 ## Max. :28.00 Max. :32 ## NA&#39;s :26 NA&#39;s :7 Here is a visualization. I used grid.arrange() from the gridExtra package for arranging two plots in a single panel. tmp &lt;- data %&gt;% select(2:3) %&gt;% drop_na() p1 &lt;- ggplot(tmp, mapping = aes(x = &quot;&quot;, y = num.partners)) + geom_boxplot() + geom_jitter(width = 0.2, height = 0.1, alpha = 0.4, color = &quot;blue&quot;) + ylab(&quot;number of partners&quot;) + xlab(NULL) p2 &lt;- ggplot(tmp, mapping = aes(x = &quot;&quot;, y = first.sex)) + geom_boxplot() + geom_jitter(width = 0.2, height = 0.1, alpha = 0.4, color = &quot;blue&quot;) + ylab(&quot;age of first sex&quot;) + xlab(NULL) grid.arrange(p1, p2, nrow = 1) Figure 7.2: The ‘num.partners’ (left) and ‘first.sex’ (right) attributes Now look at these results and ask yourself: Are these summaries and distributions you would expect? Do you see (hints of) outliers? Are these outliers “real data” or - for instance- data entry errors? Do you think the questions were answered truthfully? Here is another boxplot, the workhorse of data visualization. The problem with discrete data is that they overlap 100% in plots, as is the case with the outliers here. Instead of overlaying with all jittered data you could apply this trick to only jitter the outliers. The trick there is to provide a data = function(x) dplyr::filter(x, outlier) argument in geom_jitter() where only the rows with outliers are selected. Note that I already moved away from univeriate analysis. ##define outlier function my_outlier &lt;- function(x) { x &gt; median(x) + IQR(x) * 1.5 } tmp &lt;- data %&gt;% select(c(horm.contracept.years, target.biopsy)) %&gt;% drop_na() %&gt;% mutate(outlier = my_outlier(horm.contracept.years)) %&gt;% ungroup() ggplot(tmp, mapping = aes(x=target.biopsy, y=horm.contracept.years)) + geom_boxplot(outlier.shape = NA) + ##no outliers plotted here geom_jitter(data = function(x) dplyr::filter(x, outlier), width = 0.2, height = 0.1, alpha = 0.5) Let’s also look at the STDs. summary(data$STDs.number) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 0.0 0.0 0.0 0.2 0.0 4.0 105 These are strange numbers! Many missing values and many outliers apparently. A simple table view may help get insight here. data %&gt;% group_by(STDs.number) %&gt;% tally() ## # A tibble: 6 x 2 ## STDs.number n ## &lt;dbl&gt; &lt;int&gt; ## 1 0 674 ## 2 1 34 ## 3 2 37 ## 4 3 7 ## 5 4 1 ## 6 NA 105 Why would there be o many missing values? I think people don’t like to admit they’ve had an STD. 7.3.2 Smoking data corrupted? summary(data[, 5:7]) ## smoker smoke.years smokes.packs.year ## Mode :logical Min. : 0.0 Min. : 0.0 ## FALSE:722 1st Qu.: 0.0 1st Qu.: 0.0 ## TRUE :123 Median : 0.0 Median : 0.0 ## NA&#39;s :13 Mean : 1.2 Mean : 0.5 ## 3rd Qu.: 0.0 3rd Qu.: 0.0 ## Max. :37.0 Max. :37.0 ## NA&#39;s :13 NA&#39;s :13 Obviously, these three variables describe the exact same thing: smoking behavior. But there is something funny going on. Let’s investigate this further and deal with it. data %&gt;% group_by(smoker) %&gt;% tally() ## # A tibble: 3 x 2 ## smoker n ## &lt;lgl&gt; &lt;int&gt; ## 1 FALSE 722 ## 2 TRUE 123 ## 3 NA 13 perc &lt;- length(data$smoker) / sum(data$smoker, na.rm = T) So 6.98% of the subjects in this study is smoker. Verify the number of smokers via another route: sum(data$smoker &amp; data$smoke.years &gt; 0 &amp; data$smokes.packs.year &gt; 0, na.rm=T) ## [1] 123 So is the smoker data corrupted? Probably not, but row 4 is dodgy at the least, and worth further investigation: data[4, c(1, 5, 6, 7)] ## # A tibble: 1 x 4 ## age smoker smoke.years smokes.packs.year ## &lt;int&gt; &lt;lgl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 52 TRUE 37 37 Let’s look at the values in the the smoker columns in another way: data %&gt;% select(smoker, smoke.years, smokes.packs.year) %&gt;% drop_na() %&gt;% filter(smoke.years &gt; 0 &amp; smoke.years == smokes.packs.year) ## # A tibble: 14 x 3 ## smoker smoke.years smokes.packs.year ## &lt;lgl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 TRUE 37 37 ## 2 TRUE 19 19 ## 3 TRUE 21 21 ## 4 TRUE 15 15 ## 5 TRUE 12 12 ## 6 TRUE 5 5 ## 7 TRUE 3 3 ## 8 TRUE 3 3 ## 9 TRUE 5 5 ## 10 TRUE 5 5 ## 11 TRUE 9 9 ## 12 TRUE 6 6 ## 13 TRUE 3 3 ## 14 TRUE 22 22 As reminder, here is the equivalent base R code - which do you prefer? data[!is.na(data$smoke.years) &amp; data$smoke.years &gt; 0 &amp; data$smoke.years == data$smokes.packs.year, c(5,6,7)] What do you think happened here? I personally think there was a sleepy person doing data entry in the wrong column, or something like that. Now the packs per year attribute. data %&gt;% filter(smokes.packs.year &gt; 0) %&gt;% ggplot(aes(x = smokes.packs.year)) + geom_histogram(bins = 25) Do you know any smoker? Do they smoke at most 37 packs per year? I think not! This cannot be packs per year! Most smokers smoke 1-7 packs per week! This is exactly what this histogram shows. 7.3.2.1 Dealing with data corruption Two options remain for the smokes.packs.year column. the first is to adjust the units manually as I think is correct. The other is to simply discard the column. Since the smoking data is redundant, I will choose the latter. if you have this kind of problems with crucial data columns, you should probably try to contact the authors/data collection team before changing the units yourself. 7.4 Variable engineering 7.4.1 Recoding strategies Often you will have to recode (or transform) some or all of your variables. This can be to be able to compare groups instead of numbers on a continuous scale (factorization), to make them comparable (normalization) or to get them in a more linear distribution (log transformation). Several techniques exist for different challenges: Factorization: convert to factor seen with Smoking data Normalization min-max normalization scaling normalization Log transformation (log2, log10) Dummy coding: when numeric attributes are required instead of factor data 7.4.1.1 min-max normalization All data is scaled from 0 to 1, where the lowest value in your data is mapped to zero and the highest to one. This method is easy and transparent, but the danger lies in unseen data. When new data is encountered with a wider distribution analyses with this approach may break. This is how to do it in R scale_min_max &lt;- function(x) { (x - min(x)) / (max(x) - min(x)) } Here is a demo of min-max normalization. x &lt;- c(2, -1, 3, 5, 0, 4) scale_min_max(x) ## [1] 0.500 0.000 0.667 1.000 0.167 0.833 7.4.1.2 scaling normalization A slightly more used normalization technique is scaling normalization. It scales all variable to a mean of zero and with the same standard deviation. \\[x&#39; = \\frac{x-\\bar{x}}{\\sigma}\\] It is built right into R: x &lt;- c(2, -1, 3, 5, 0, 4) scale(x) ## [,1] ## [1,] -0.0719 ## [2,] -1.3669 ## [3,] 0.3597 ## [4,] 1.2231 ## [5,] -0.9353 ## [6,] 0.7914 ## attr(,&quot;scaled:center&quot;) ## [1] 2.17 ## attr(,&quot;scaled:scale&quot;) ## [1] 2.32 7.4.1.3 dummy encoding How to change this factor in a numeric representation usable for techniques that require numeric input such as clustering, linear modelling or regression? This is done with a technique called dummy coding and the essence is binary splitting. Here is a small data set: pet_favour &lt;- tibble(subject = c(&quot;Mike&quot;, &quot;Roger&quot;, &quot;Rose&quot;, &quot;Megan&quot;, &quot;Caitlin&quot;), favour = factor(c(&quot;dog&quot;, &quot;cat&quot;, &quot;cat&quot;, &quot;dog&quot;, &quot;rat&quot;))) pet_favour ## # A tibble: 5 x 2 ## subject favour ## &lt;chr&gt; &lt;fct&gt; ## 1 Mike dog ## 2 Roger cat ## 3 Rose cat ## 4 Megan dog ## 5 Caitlin rat What we need here is three columns with 0 or 1 values for dog/not dog cat/not cat rat/not rat Below you can see an approach to this using some techniques from base R. encode_dummy &lt;- function(x) { lvls &lt;- levels(x) tmp &lt;- as.data.frame(sapply(lvls, function(y) as.integer(x == y))) names(tmp) &lt;- paste0(lvls, &quot;_y&quot;) tmp } bind_cols(subject = pet_favour$subject, encode_dummy(pet_favour$favour)) ## subject cat_y dog_y rat_y ## 1 Mike 0 1 0 ## 2 Roger 1 0 0 ## 3 Rose 1 0 0 ## 4 Megan 0 1 0 ## 5 Caitlin 0 0 1 There are of course also packages that can do this; e.g. have a look at dummies. 7.4.2 “factorization” Especially for visualization purposes, it can be more convenient to have a variable in a factor form instead of numeric form. For instance, this is the case with the smoking-related data in the current dataset. I will reduce all three ‘smoking’ variables to one variable and start building a ‘clean’ dataset. Here, the new variable smoking is the result of cutting the smoke.years variable into factor levels. smoking_f &lt;- cut(data$smoke.years, breaks = c(0, 1, 5, 12, 100), labels = c(&quot;never&quot;, &quot;short&quot;, &quot;medium&quot;, &quot;long&quot;), ordered_result = T, right = F) clean_data &lt;- data %&gt;% mutate(smoking = smoking_f) %&gt;% select(age:num.preg, smoking, horm.contracept.ever:target.biopsy) table(clean_data$smoking, useNA = &quot;always&quot;) ## ## never short medium long &lt;NA&gt; ## 726 42 44 33 13 7.4.3 Data redundancy in STD variables There are many variables related to STDs: (std_columns &lt;- codebook %&gt;% select(abbreviation) %&gt;% filter(str_detect(abbreviation, &quot;STD_&quot;))) ## abbreviation ## 1 STD_condylomatosis ## 2 STD_cervical.cond ## 3 STD_vaginal.cond ## 4 STD_vulvo.per.cond ## 5 STD_syph ## 6 STD_pid ## 7 STD_gen.herpes ## 8 STD_moll.cont ## 9 STD_aids ## 10 STD_hiv ## 11 STD_hepB ## 12 STD_HPV std_columns &lt;- std_columns$abbreviation ##alternative with base R: #codebook[grep(pattern = &quot;STD_&quot;, x = codebook$abbreviation), 1] Some of these are almost completely redundant, and most with extremely low -and thus useless- counts. Seen in absolute numbers, not many have multiple STDs, but statistically seen they are highly over represented - there are even more with two than with one STD: table(data$STDs.number, useNA = &quot;always&quot;) ## ## 0 1 2 3 4 &lt;NA&gt; ## 674 34 37 7 1 105 Only 79 subjects have STDs (luckily!). These are the counts of occurrences of individual STDs: They are listed below. You can see a nice application of the gather() function to put the results in long format instead of wide. clean_data %&gt;% summarise_at(std_columns, function(x) sum(x, na.rm = T)) %&gt;% gather(key = &quot;disease&quot;, value = &quot;count&quot;, everything()) ## # A tibble: 12 x 2 ## disease count ## &lt;chr&gt; &lt;int&gt; ## 1 STD_condylomatosis 44 ## 2 STD_cervical.cond 0 ## 3 STD_vaginal.cond 4 ## 4 STD_vulvo.per.cond 43 ## 5 STD_syph 18 ## 6 STD_pid 1 ## 7 STD_gen.herpes 1 ## 8 STD_moll.cont 1 ## 9 STD_aids 0 ## 10 STD_hiv 18 ## 11 STD_hepB 1 ## 12 STD_HPV 2 This is the same in base R using apply(): apply(data[ ,std_columns], MARGIN = 2, FUN = function(x) sum(x&gt;0, na.rm=T)) 7.5 The dependent variable In many datasets there is a single dependent variable, the variable you try to explain or model using all the other variables (hence the name “target” used here. In this case it is the question whether the subject has cervical cancer yes or no. Unfortunately there are four dependent variables: last &lt;- length(names(data)) (target_vars &lt;- names(data)[(last-3):last]) ## [1] &quot;target.hinselman&quot; &quot;target.schiller&quot; &quot;target.cytology&quot; &quot;target.biopsy&quot; Which one(s) are you going to use? Let’s investigate the pairwise correlations between them. target_names &lt;- c(&quot;Hinselman&quot;, &quot;Schiller&quot;, &quot;cytology&quot;, &quot;biopsy&quot;) cor_matrix &lt;- cor(data[, 33:36]) colnames(cor_matrix) &lt;- target_names (cor_matrix &lt;- as_tibble(cor_matrix)) ## # A tibble: 4 x 4 ## Hinselman Schiller cytology biopsy ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0.650 0.192 0.547 ## 2 0.650 1 0.361 0.733 ## 3 0.192 0.361 1 0.327 ## 4 0.547 0.733 0.327 1 cor_matrix &lt;- cor_matrix %&gt;% mutate(target_name = target_names) %&gt;% select(5, 1:4) A map is a nice visualization for this type of data. Of course, ggplot really likes the long format so that needs to be done first with gather() (try to do this with pivot_longer() as an exercise): map, fig.asp=.9, out.width=&#39;70%&#39;, fig.align=&#39;center&#39;, fig.cap=&quot;A correlation map of the four explanatory variables&quot;} cor_matrix_long &lt;- gather(cor_matrix, key = &quot;method&quot;, value = &quot;correlation&quot;, target_names) ggplot(data = cor_matrix_long, aes(x=target_name, y=method, fill=correlation)) + geom_tile() + labs(x=NULL, y=NULL) + scale_fill_gradient(high = &quot;red&quot;, low = &quot;white&quot; ) This shows the correlation between the four target variables. Another way to explore this is a contingency table. ## TO BE DONE 7.6 Exploring relationships between variables Typically, relationships between variables are visualized using scatterplots, but other strategies exist. Several are explored here. 7.6.1 The scatterplot A simple scatterplot looking at the relationship between first.sex and num.partners. baseplot_sp &lt;- ggplot(clean_data, aes(x=first.sex, y=num.partners)) + labs(x=&quot;Age of first sex&quot;, y=&quot;Number of sexual partners&quot;) baseplot_sp + geom_point(na.rm=T) Figure 7.3: scatterplot first version Do you notice the strange “outlier,” a subject who had first sex at ten and had 29 sexual partners? More importantly, this is not showing the correct picture: there is a limited number of discrete values for each variable. Only around 100 points are visible where {r nrow(data)} are expected, so apparently many points overlap. This can be improved with alpha parameter to geom_point. Another strategy to improve is by using the geom_jitter() function, and combined with the alpha parameter. Omitting outliers could improve the picture, but will of course not show the complete picture anymore. Here, I choose the transparency option together with a bit of jitter (but not too much to still show the discreteness of the measurements). I also added a trend line in the form of a smoother. The trend line is an addition that is a great visual help to guide your reader. My question to you is: look at the trend line and decide whether it is showing a false pattern. baseplot_sp + geom_jitter(na.rm=T, alpha=0.2, shape=16, size=2, width = 0.2, height = 0.2, color = &quot;darkgreen&quot;) + geom_smooth(method=&quot;loess&quot;, na.rm=T) ## `geom_smooth()` using formula &#39;y ~ x&#39; Figure 7.4: scatterplot with trendline The trend line shows the high impact of a single outlier! In contrast to common belief about age of first sex and “promiscuity,” no apparent relationship is visible in this dataset when you omit the single outlier: clean_data %&gt;% filter(num.partners &lt; 25 ) %&gt;% ggplot(aes(x=first.sex, y=num.partners)) + labs(x=&quot;Age of first sex&quot;, y=&quot;Number of sexual partners&quot;) + geom_jitter(na.rm=T, alpha=0.2, shape=16, size=2, width = 0.2, height = 0.2, color = &quot;darkgreen&quot;) + geom_smooth(method=&quot;loess&quot;, na.rm=T) ## `geom_smooth()` using formula &#39;y ~ x&#39; Figure 7.5: scatterplot with trendline 7.6.1.1 Binning A method not discussed before is binning, With binning, you cluster the individual data points into “buckets.” The amount of cases in a bucket is then displayed using a color gradient. baseplot_sp + geom_bin2d(na.rm=T) Here is a variation of binning - the hexagonal bin (requires package hexbin): library(hexbin) baseplot_sp + geom_hex(na.rm=T) 7.7 Look for patterns with the dependent variable Why would you do that? The dependent variable is the thing you are interested in! Therefore it is a good idea to investigate variables in relation with the dependent variable. clean_data %&gt;% filter(num.partners &lt; 25 ) %&gt;% ggplot(aes(x=first.sex, y=num.preg, color=target.biopsy)) + labs(x=&quot;Age of first sex&quot;, y=&quot;Number of pregnancies&quot;) + geom_jitter(mapping = aes(color=target.biopsy), na.rm=T, width=0.2, height=0.2, alpha=0.5, shape=16, size=0.8) + ylim(0,10) + geom_smooth(method=&quot;loess&quot;, na.rm=T) ## `geom_smooth()` using formula &#39;y ~ x&#39; Again, the heatmap comes in handy. Start with creating the matrix. selection &lt;- c(&quot;num.partners&quot;, &quot;first.sex&quot;, &quot;num.preg&quot;, &quot;horm.contracept.years&quot;, &quot;IUD.years&quot;, &quot;STDs.number&quot;, &quot;target.biopsy&quot;) tmp &lt;- clean_data %&gt;% select(selection) %&gt;% drop_na() ## Note: Using an external vector in selections is ambiguous. ## ℹ Use `all_of(selection)` instead of `selection` to silence this message. ## ℹ See &lt;https://tidyselect.r-lib.org/reference/faq-external-vector.html&gt;. ## This message is displayed once per session. cor_matrix &lt;- cor(tmp) #cor_matrix #colnames(cor_matrix) &lt;- target_names cor_matrix &lt;- as_tibble(cor_matrix) (cor_matrix &lt;- cor_matrix %&gt;% mutate(var1 = selection) %&gt;% select(8, 1:7)) ## # A tibble: 7 x 8 ## var1 num.partners first.sex num.preg horm.contracept… IUD.years STDs.number ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 num.… 1 -0.148 0.0937 0.0209 0.00705 0.0370 ## 2 firs… -0.148 1 -0.0704 0.00434 -0.0357 0.00188 ## 3 num.… 0.0937 -0.0704 1 0.217 0.156 -0.00460 ## 4 horm… 0.0209 0.00434 0.217 1 -0.00885 -0.0175 ## 5 IUD.… 0.00705 -0.0357 0.156 -0.00885 1 0.00262 ## 6 STDs… 0.0370 0.00188 -0.00460 -0.0175 0.00262 1 ## 7 targ… -0.00266 0.0296 0.0452 0.0799 0.0422 0.105 ## # … with 1 more variable: target.biopsy &lt;dbl&gt; And then create the plot. Here I used pivot_longer() instead of gather() to obtain the longer format required for ggplot2. cor_matrix_long &lt;- pivot_longer(data = cor_matrix, cols = selection, names_to = &quot;variable&quot;, values_to = &quot;cor&quot;) ggplot(data = cor_matrix_long, aes(x=var1, y=variable, fill=cor)) + geom_tile() + labs(x=NULL, y=NULL) + scale_fill_gradient(high = &quot;red&quot;, low = &quot;white&quot; ) Figure 7.6: A heatmap pairwise correlation of selected numeric variables As you can see, there is hardly any correlation between these numeric variables, and not with the target (dependent) variable target.biopsy either. This is not a hopeful result when the goal is to be able to predict cervical cancer occurrence. 7.8 Density plots show class distinction A density plot, split over the categories of your target variable, will often quickly reveal whether there is promise in a variable. I’ll demonstrate with the iris dataset since the cervical cancer dataset is not so nice in that respect. ggplot(iris, aes(x=Petal.Length)) + geom_density(aes(color=Species)) In this plot, you can see that Setosa separates quite nicely, by the other two don’t. 7.8.1 The ‘xor’ problem The density approach is not flawless! Consider this: var_x &lt;- c(runif(300, -1, 0), runif(300, 0, 1)) var_y &lt;- c(runif(150, -1, 0), runif(150, 0, 1), runif(150, 0, 1), runif(150, -1, 0)) label = rep(c(rep(&quot;sick&quot;, 150), rep(&quot;healthy&quot;, 150)), 2) df &lt;- data.frame(dosage = var_x, response=var_y, patient_type = label) ggplot(data=df, mapping=aes(x=dosage)) + geom_density(aes(color=patient_type)) This looks like there is little to win, doesn’t it? Here’s another view of the same data! ggplot(df, aes(x=dosage, y=response)) + geom_point() Still not interesting! Give it one more shot: ggplot(df, aes(x=dosage, y=response, color=patient_type)) + geom_point() This is called the XOR problem because the cases follow an “Exclusive Or” rule. 7.9 Advanced Explorations 7.9.1 PCA First PCA will be shown with the iris dataset and then with the cervical cancer dataset. ir.pca &lt;- prcomp(iris[, -5], center = TRUE, scale. = TRUE) print(ir.pca) ## Standard deviations (1, .., p=4): ## [1] 1.708 0.956 0.383 0.144 ## ## Rotation (n x k) = (4 x 4): ## PC1 PC2 PC3 PC4 ## Sepal.Length 0.521 -0.3774 0.720 0.261 ## Sepal.Width -0.269 -0.9233 -0.244 -0.124 ## Petal.Length 0.580 -0.0245 -0.142 -0.801 ## Petal.Width 0.565 -0.0669 -0.634 0.524 The plot method returns a plot of the variances (y-axis) associated with the PCs (x-axis). plot(ir.pca, type = &quot;l&quot;) The summary method describe the importance of the PCs. summary(ir.pca) ## Importance of components: ## PC1 PC2 PC3 PC4 ## Standard deviation 1.71 0.956 0.3831 0.14393 ## Proportion of Variance 0.73 0.229 0.0367 0.00518 ## Cumulative Proportion 0.73 0.958 0.9948 1.00000 The PC plot library(ggbiplot) ## Loading required package: plyr ## ------------------------------------------------------------------------------ ## You have loaded plyr after dplyr - this is likely to cause problems. ## If you need functions from both plyr and dplyr, please load plyr first, then dplyr: ## library(plyr); library(dplyr) ## ------------------------------------------------------------------------------ ## ## Attaching package: &#39;plyr&#39; ## The following objects are masked from &#39;package:dplyr&#39;: ## ## arrange, count, desc, failwith, id, mutate, rename, summarise, ## summarize ## Loading required package: scales ggbiplot(ir.pca, obs.scale = 1, var.scale = 1, groups = iris[,5], ellipse = FALSE, circle = TRUE) + scale_color_discrete(name = &#39;&#39;) + theme(legend.direction = &#39;horizontal&#39;, legend.position = &#39;top&#39;) Now the same procedure for the cervical cancer dataset, with a selection of numerical variables. selection &lt;- c(&quot;num.partners&quot;, &quot;first.sex&quot;, &quot;num.preg&quot;, &quot;horm.contracept.years&quot;, &quot;IUD.years&quot;, &quot;STDs.number&quot;, &quot;target.biopsy&quot;) tmp &lt;- clean_data %&gt;% select(selection) %&gt;% drop_na() tmp ## # A tibble: 676 x 7 ## num.partners first.sex num.preg horm.contracept… IUD.years STDs.number ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 4 15 1 0 0 0 ## 2 1 14 1 0 0 0 ## 3 5 16 4 3 0 0 ## 4 3 21 4 15 0 0 ## 5 3 23 2 0 0 0 ## 6 3 17 6 0 7 0 ## 7 1 26 3 2 7 0 ## 8 1 20 5 0 0 0 ## 9 3 26 4 2 0 0 ## 10 1 17 3 8 0 0 ## # … with 666 more rows, and 1 more variable: target.biopsy &lt;lgl&gt; cc.pca &lt;- prcomp(tmp[, -7], center = TRUE, scale. = TRUE) print(cc.pca) ## Standard deviations (1, .., p=6): ## [1] 1.151 1.052 1.005 0.999 0.918 0.848 ## ## Rotation (n x k) = (6 x 6): ## PC1 PC2 PC3 PC4 PC5 PC6 ## num.partners -0.3816 0.552 0.2388 0.0143 -0.6832 -0.15857 ## first.sex 0.3461 -0.570 -0.0559 0.2939 -0.6801 0.05260 ## num.preg -0.6473 -0.258 -0.0466 0.0622 -0.0273 0.71257 ## horm.contracept.years -0.4427 -0.471 0.4721 0.1730 0.1630 -0.55043 ## IUD.years -0.3457 -0.104 -0.8319 -0.0899 -0.0903 -0.40168 ## STDs.number -0.0113 0.266 -0.1506 0.9336 0.1875 0.00171 The plot method returns a plot of the variances (y-axis) associated with the PCs (x-axis). plot(cc.pca, type = &quot;l&quot;) The summary method describe the importance of the PCs. summary(cc.pca) ## Importance of components: ## PC1 PC2 PC3 PC4 PC5 PC6 ## Standard deviation 1.151 1.052 1.005 0.999 0.918 0.848 ## Proportion of Variance 0.221 0.185 0.168 0.166 0.140 0.120 ## Cumulative Proportion 0.221 0.405 0.574 0.740 0.880 1.000 PC plot ggbiplot(cc.pca, obs.scale = 1, var.scale = 1, groups = tmp$target.biopsy, ellipse = FALSE, circle = TRUE, alpha = 0.5) + scale_color_discrete(name = &#39;&#39;) + theme(legend.direction = &#39;horizontal&#39;, legend.position = &#39;top&#39;) Figure 7.7: A PCA plot of the numerical variables This is very promising neither; no structure in the data, not in general and not in relation with the dependent variable. 7.9.2 Clustering Using clustering, you can sometimes see obvious patterns in the data. Most obvious are: k-Means clustering Hierarchical clustering 7.9.3 k-Means clustering k-means is very sensitive to the scale of your data so you’ll need to normalize it. #km_clusters &lt;- kmeans() 7.9.4 hierarchical clustering "],["publish.html", "Chapter 8 Publishing, Presenting and Reporting 8.1 Introduction 8.2 RMarkdown 8.3 Shiny", " Chapter 8 Publishing, Presenting and Reporting 8.1 Introduction This chapter deals with the way you can disseminate your data, findings and analyses. No in-depth discussions will be presented. This chapter merely outlines the possibilities and refers to external documentation. There are two distinct strategies to follow: RMarkdown and Shiny. The first is at its heart static, with the possibility to embed interactive elements. The latter is a toolbox to create interactive web applications on top of your data, using only R. 8.2 RMarkdown The most extensive overview of the possibilities with RMarkdown are presented in “R Markdown: The Definitive Guide” (https://bookdown.org/yihui/rmarkdown/). RMarkdown is the basis for many output formats: Simple HTML, Word or pdf documents are the most commonly used outputs formats. However, it is also possible to create presentations (e.g. IOslides, Slidy and Beamer). These options are built right into R. Simply go to File → New File → RMarkdown... and select one of the Presentation options. Another interesting “static” output format is the Dashboard which is also discussed in the link above. In summary, these are the output formats to choose from: HTML (with interactive elements using widgets or Shiny) Word Pdf Notebooks Presentations Dashboards Tufte Handouts eBooks using Bookdown Blogs (also general websites) using Blogdown This entire ebook was written using RMarkdown and build using Bookdown. 8.3 Shiny On the other side of the spectrum is Shiny. Shiny is not text-oriented as RMarkdown is, but foremost visualization-oriented. Shiny is a toolbox that makes it possible to build a complete data-exploration website using R code only. There is an excellent book written on shiny: https://mastering-shiny.org/ And this is the official website: https://shiny.rstudio.com/. "],["exercises.html", "Chapter 9 Exercises 9.1 Introduction 9.2 The ggplot2 and tidyr packages 9.3 The tidyr and dplyr packages 9.4 The lubridate package 9.5 The stringr package", " Chapter 9 Exercises 9.1 Introduction These exercises are meant to provide practice of the course material presented in this eBook. There is no single good solution for them, but some are better than others. A good practice is also to try to solve the same exercise in several different ways. The exercises have been marked with a difficulty level, from 1 to five “stars” (❋ to ❋❋❋❋❋). Since I am a big fan of intellectual challenge I have included quite some exercises that transcend the mastery level required at the final assessment. For your peace of mind; if you are able to solve three-star exercises you are really good to go! Most of the datasets referred to from within the exercises can be found in the Github repository with the URL https://github.com/MichielNoback/datasets. This is a direct link. You can download individual datasets from this page, or download the entire repository at once. To download, use the “clone or download” pull down button (green button). If you want to be a pro, use git to clone it… The solutions to the exercises are in the next chapter of this eBook. For all plotting exercises: take care of the details: axis labels, figure title or caption clear names of separating variables. 9.2 The ggplot2 and tidyr packages 9.2.1 Trees The datasets package that is shipped with R has a dataset called trees. A [❋] Create a scatter plot for Heigth as a function of Girth, with blue plotting symbols . B [❋❋] Add a single line representing the linear model of this relationship, without shaded confidence interval. C [❋❋] Have the Volume variable reflected in the size of the plot symbol and change their transparency level to 0.6. 9.2.2 Insect Sprays The datasets package that is shipped with R has a dataset called ?. Type ?InsectSprays to get information on it. A [❋] Create a boxplot, splitting on the spray type. B [❋❋] Create a jitter plot, splitting on the spray type. Have zero jittering on the y-axis and as little as possible on the x-axis. use the alpha parameter and try to find a nice plot symbol. [Extra: Give each spray a different plot color] 9.2.3 Diauxic growth In 1941, Jacques Monod discovered the phenomenon of diauxic growth in bacteria (e.g. E. coli), resulting from the preference for some carbon substrate over others, causing catabolite repression of pathways of less preferred substrates. See The Wikipedia page for details. Diauxic growth (figure from wikipedia) Some of the data used to generate the figure in that original publication are in this course’s data repository (https://github.com/MichielNoback/datasets/diauxic_growth). A direct link to the data file is https://raw.githubusercontent.com/MichielNoback/datasets/master/diauxic_growth/monod_diauxic_growth.csv. A [❋❋] Download the file, load it and attach to variable diauxic. Next, tidy this data into long format so you have three columns left: Time, Substrate and OD. B [❋] Convert the newly created Substrate variable into a factor with nicer labels, i.e. better suited for human reading C [❋❋] Create a line plot with all four growth curves within a single panel. use stat_smooth() and its span = parameter to have a line-and-point visualization. D [❋❋❋] Create a multi-panel plot like the one from the original publication. 9.2.4 Virginia Death Rates The datasets package that is shipped with R has a dataset (a matrix) called VADeaths. Type ?VADeaths to get information on it. First you should convert this matrix into a tibble (a kind of data frame) and the rownames into a real variable using the chunk below (study it so you understand what happens here): library(dplyr) ## %&gt;% is used to pipe results from one operation to the other, just like &#39;|&#39; in Linux. virginia_death_rates &lt;- as_tibble(VADeaths) virginia_death_rates &lt;- virginia_death_rates %&gt;% mutate(&quot;Age Group&quot; = factor(rownames(virginia_death_rates), ordered = TRUE)) %&gt;% select(`Age Group`, everything()) #reorder the columns A [❋❋❋] Pivot this table to long (tidy) format. This should generate a dataframe with four columns: Age Group, Habitat, Gender and DeathRate. B [❋❋] Create a bar chart with the age groups on the x-axis and Rural/Urban and Male/Female using side-by-side bars. 9.2.5 Investigate new visualization [❋❋❋] Go to the R graph Gallery and browse the different sections. Then select one visualization to study in-depth (excluding the ones already demonstrated in this eBook). Work out an example with one of the datasets from either the built-in datasets of R or the Datasets Repo of this course. You can also choose another dataset. For instance, Kaggle (https://www.kaggle.com/) and the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/) have very interesting datasets. Present this visualization in class, addressing these topics: What is this visualization named? What is its purpose; when is it appropriate to use? Why does it appeal to you? Show an example with the R code, explain difficulties and point out caveats. 9.2.6 ToothGrowth [❋❋] The datasets package that is shipped with R has a dataset called ToothGrowth. Create a visualization distinguishing the tooth growth between both supplement and dose. 9.2.7 Puromycin [❋❋❋] Create a scatter plot visualization of the Puromycin dataset. Color by state and add a loess model smoother without error margin. Use the formula = parameter in geom_smooth() for getting a better-fitting regression line. 9.2.8 Global temperature Maybe you have seen this picture of the world temperature over the past 120 years in the media: global_heatmap_s.png We are going to work with this data as well. The global temperature data are located in folder global_temperature (see the Data Repo). There are two data series in file https://raw.githubusercontent.com/MichielNoback/datasets/master/global_temperature/annual.csv. Study the readme file in the same folder to find out more about these datasets. 9.2.8.1 Create a scatter-and-line-plot [❋❋] Create a scatter-and-line graph of both series in a single plot. Annotate well with labels and title. Optionally, add a smoother without error boundaries. 9.2.8.2 Re-create the heatmap [❋❋❋❋] You should try to reproduce the above picture by using the geom_tile() function. Hint: use scale_fill_gradient2(low = \"blue\", mid = \"white\", high = \"red\") and pass 1 as value for y in the mapping function. The theme() function with element_blank() can be used for extra tweaking. 9.2.8.3 Extra practice [❋❋] As extra practice, you could try to answer these questions as well: what is the warmest year? what is the warmest year that both timelines agree upon? which was the coldest decade? what is the 30-year moving average of the temperature? The is also a monthly temperature file. Many explorations can be carried out on that one as well. 9.2.9 Epilepsy drug trial The epilepsy folder (see the Data Repo) contains two files, one of which -epilepsy.csv- is the actual data file. The readme.md file describes the dataset and the columns. Read it carefully before proceeding. 9.2.9.1 Load the data [❋] Load the data and be sure to check the correctness of data types in the columns. The period should be a factor. You can use a downloaded copy, but a direct link to the data file can also be used as argument to read.table(): https://raw.githubusercontent.com/MichielNoback/datasets/master/epilepsy/epilepsy.csv 9.2.9.2 Reorganize the data [❋❋] Reorganize the data so that the dependent variable comes last and the useless entry variable is omitted. This order of variables is required: \"subject\", \"age\", \"base\", \"treatment\", \"period\", \"seizure.rate\" For better readability you should convert the dataframe into a tibble using as_tibble() (tibbles are dealt with in a later chapter: dplyr). 9.2.9.3 Create plots of seizure rates A [❋❋] First, create a boxplot of the seizure rates of both groups, split over both the period and the treatment. To support both of these data dimensions you will either have to use the color aesthetic, or the facet_wrap() function, like this + facet_wrap(. ~ treatment). I suggest you try them both. B [❋❋❋] Next, create the same basic plot as a jitter plot. You can use the facet_wrap(). Compare them and write down some pros and cons of both. C [❋❋❋] Investigate whether an overlay may improve the story this visualization tells, or can you come up with an even better graph? 9.2.9.4 A boxplot after correction [❋❋❋] The base column is the base seizure rate of the subject in an 8-week window prior to the actual trial. To compare before and after you should create a new dataframe where the seizure.rate is summed for the 4 periods. Next, create a final boxplot of these corrected values. 9.2.9.5 Test for statistical significance [❋❋❋] In the previous exercise you have determined the corrected seizure rates. Can you figure out a statistical test to see if the difference is significant? 9.2.9.6 Investigate age-dependency [❋❋❋] Investigate whether there is an age-dependent effect in either the base seizure rate or the effect of the treatment. 9.2.10 Bacterial growth curves with Varioscan Chapter 10, @ref(parsing-complex-data ), ended with a file being saved, data/varioscan/2020-01-30_wide.csv. This file is also present in the varioscan folder of the datasets repo. 9.2.10.1 Load, preprocess and tidy [❋❋❋❋] First load the data; this is straightforward. Next, correct OD values of the “triplicate” experiments using the background signal data from samples without bacteria. Thus, the Red values should be corrected using the Red_w_o values of the same dilution; the White columns using White_w_o and Elution using Eluiton_w_o. This can be done when you apply a trick: transpose the dataframe using t() before carrying out the corrections. After transpose, you should loop the columns and correct using the periodicity of 8 (number of dilutions) You can use this as a start, assuming you loaded the file into growth_data: ## Add rownames to get a hold of them after transpose rownames(growth_data) &lt;- paste0(growth_data$Content, &#39;.&#39;, growth_data$.copy, &#39;.&#39;, rep(LETTERS[1:8], times = 12)) ## transpose tmp &lt;- t(growth_data) ## build a new dataframe/tibble growth_data_corr &lt;- tibble(column_names = rownames(tmp)) ## continue here After these steps, the data should be tidied. 9.2.10.2 Create a growth curve visualization [❋❋❋❋] Create a line plot visualizing the entire experiment. The challenge here is to present a clear picture with tons of dataseries on top of each other. You should realise there are 96 time series that could be plotted. I suggest you also use a custom color pallette reflecting the sample groups in the experiment. The main sample set (the red tulip) should be emphasized. The least interesting controls, columns 4, 8 and 12, should be made less present visually. 9.2.11 The dinos This exercise represents a review of many of basic R operations as well, besides ggplot2. The dinos folder contains an Excel file called jgs2018049_si_001.xlsx. It contains supplementary data to a scientific publication with the title “The Carnian Pluvial Episode and the origin of dinosaurs” [Benton et al., Journal of the Geological Society 175(6) 2018]. One of the figures accompanying the paper is this one: Figure 2.Proportions of early dinosaurs through the Triassic, showing the rapid rise in the late Carnian and early to middle Norian. Two metrics are shown, numbers of specimens and numbers of genera in 12 sampled faunas, in proportion to all tetrapods; the linking line is a moving average. Specimen counts perhaps exaggerate the trend when compared with generic counts, or at least both show different aspects of the same rise in ecological impact of the dinosaurs in the Late Triassic. (Based on data in supplementary material Table S1.) You have to agree this is terrible! They even omitted the legend for CPE which is descried in the paper “Carnian Pluvial Episode (CPE), dated at 232 M Ya.” And what the heck is a “moving average?” Let’s explore and improve. 9.2.11.1 Export to csv We’ll begin by exporting all three tabs to a textual format. Open the excel file and select the tab “Contents.” You can see it is a “codebook” - it contains column names and descriptions for the two other tabs in the excel file. In the File menu, select Save As…. Next, give as name codebook.csv and for File Format, select CSV UTF-8 (Comma-delimited) (.csv). You’ll get a warning -read it!- but select OK. Next, select tab “Skeletons” and repeat to generate skeletons.csv and “Footprints” to generate footprints.csv. Close the Excel file without saving changes to it. We’ll continue with the csv files. Note: R has packages providing functionality to read from Excel, but this is outside the scope of this course, and installing them is often a hassle. 9.2.11.2 Clean up and load codebook.csv Have a look at the contents of codebook.csv in the editor. It is not yet suitable for loading into R. Write down the number of lines describing column headers for the skeletons and the number for footprints. Next, delete the lines that are not really codebook entries. A [❋] Load the resulting file -as character data only!- and assign it to variable codebook. B [❋❋] Give the dataframe column names: variable and description. C [❋❋] Remove the leading space of the second column D [❋❋] Add a column called dataset: a factor with the value skeleton or footprint, depending on the file that is referred to. 9.2.11.3 Write a utility function [❋❋❋] Write a utility function that returns a text label to be used in plotting when given a dataset name and a column name. The label should come from the codebook description variable of course. The dataset parameter should default to skeleton. For example, these calls: get_description(&#39;Dinosaur_gen&#39;, &#39;skeleton&#39;) ##same as get_description(&#39;Dinosaur_gen&#39;) should both return Number of genera of Dinosauria. As extra challenge you could implement some error checking to make the function more robust. 9.2.11.4 Load skeleton data [❋] Load the data in the skeletons.csv file and assign it to variable skeleton. Make sure your data columns have the right type and that you did not overlook NA value or decimal encodings. 9.2.11.5 Plot species versus time [❋❋❋] As a first exploration of the data, create a scatterplot of Total_spec as a function of Midpoint (the midpoint of the archaeological epoch) and have the points colored by Epoch. You should use ggplot2 of course. There is an outlier flattening the picture quite a lot. Can you think of a strategy to make the picture clearer? Another problem is that the x-axis scale is from recent to ancient and this should be reversed. Finally, add a smoother (loess regression) for the entire dataset (not split over the Epochs!) and annotate the plot with nice axis labels, preferably using your previously created utility function. NB: it may be a good idea to tweak the descriptions in the codebook a little bit. Note that for “Million Years Ago” you can use the abbreviations “MYA” or “Ma” (Mega annum). 9.2.11.6 Reproducing the publication figure Reproduce the figure from the introduction of this section, but using ggplot2 instead of base R as they used. This requires some preprocessing steps, especially with the use of pivot_longer(). A [❋❋] You will have to calculate the proportions of Dinosauria (Dinosaur_gen) relative to the total of all “tetrapod” groups (including Archosauromorph_gen, Dinosaur_gen, Synapsid_gen, Parareptile_gen, Temnospondyl_gen). You need this for Specimens (_spec) as well as Genera (_gen). B [❋❋❋] Next, you should extract a single proportion for all Formations represented in a single Midpoint. I realize that this is not an entirely valid operation. Do you know why? Can you figure out how it was done for the existing publication plot? I suggest you use the aggregate() function. You will need the Epoch and Stage of these as well for later aspects of plotting. C [❋❋❋] Now you need to tidy the data. D [❋❋❋❋] Finally you have the data to generate the plot itself, without the moving average. E [❋❋❋❋❋] Add the moving average. A simple moving average (SMA) is the unweighted mean of the previous n data. The paper does not say anything about the “window” (n) used in the moving average, but looking at the original figure it is less that 10 MY wide. Actually, to be honest, I cannot identify the used algorithm by looking at the plot. Can you? Just give it a shot and see how far you get. 9.2.11.7 Make a better figure [❋❋❋] Next, try to make a plot that uses an alternative to the moving average used in the publication. Also, use background coloring (rectangles) to highlight the Epochs within the plot. 9.2.11.8 Use the size aesthetic [❋❋❋❋❋] The “size” aesthetic can be used for indicating the absolute number of specimens/genera, respectively. This adds an extra dimension of information to the plot. 9.3 The tidyr and dplyr packages 9.3.1 Global temperature revisited Reload the global_temp data if it is not in memory anymore. A [❋❋] Which years before 1970 had an anomaly of more than +0.1 degree according to the GCAG model? And according to the GISTEMP model? Notice anything strange? B [❋❋] Which has (have) been the coldest year(s) after 1945? Do both models agree on that? C [❋❋❋] Which were the warmest five years? Select the warmest five for both models. Do they agree on that? To select the top n values after grouping, use filter(row_number() %in% 1:5) D [❋❋❋] Select the years from 1970 and split up the dataset in decades and report the per-decade average temperature for both models. Extra exercise: create a bar plot from this result. 9.3.2 ChickWeight You have seen how to determine the weight gain of the chickens between measurements in the ChickWeight dataset. Here, we look at this dataset again. Starting weight [❋❋]. Which chick had the highest starting weight? List this chick, together with its diet as follows: . Total weight gain [❋❋❋]. Determine the total weight gain for each chick and list all chickens together with their diet and number of weight measurements. Order from high to low weight gain. Average weight gain per diet [❋❋❋❋]. Determine the average weight gain for each diet. Report the Diet, number of chickens on the diet and the average weight gain. 9.3.3 Population numbers The population folder of the datasets repo contains two files. One of them, EDU_DEM.....csv contains population data for all countries of the world (where data is available). We’re going to mangle and explore this data a bit. Besides the following exercises there are many, many other questions you could ask yourself about this dataset, and visualizations to be created. For instance, how about countries and age groups, years with maximum growth, etc. Explore and practice! 9.3.3.1 Load the data [❋] Start by loading the data into a dataframe called population. Since it is a bit large, I suggest you first download it to a local location. 9.3.3.2 Clean up [❋❋] There are quite some uninformative variables - variables with only one value or with only NA. Find out which variables are uninformative and remove these from the dataframe. 9.3.3.3 Create a “wide” yearly report of population totals [❋❋❋] With the dataframe you now have, create a report in wide format for the total population numbers over the available years. So, the years now get their own column and each country has a single row. 9.3.3.4 Create a “wide” yearly report of population change [❋❋❋❋] Next, present the same format (wide), but now with the “population change” instead of the total population. 9.3.3.5 Create a bar plot split on gender [❋❋❋] Create a bar plot of the population numbers of the countries of Western Europe across the available years, split by gender. 9.3.3.6 Highest growth rate [❋❋❋] Which three countries have shown the fastest relative growth rate, measured as percentage change over the the whole of the available time period (2005-2017) relative to its start, over the entire population, for both sexes combined? Extra practice: Do the same for the different age groups. 9.3.4 Rubella and Measles The rubella_measles_cases folder of the datasets repo contains historic data for both measles and rubella. This serves as extra training material (no solutions provided). Try to report, both visually and in tables: cases per per continent cases over time per continent You can also combine with the population data from a previous exercise (folder population) and report measles and rubella per 10000 inhabitants, per country or per continent. Other data are available for exploring relations as well in the population data: age groups, sex, and diseases. 9.4 The lubridate package These exercises can of course only be completed if you also apply your knowledge of the tidyr and dplyr packages. 9.4.1 Female births The datasets repo contains a folder female_births. Its content is fairly simple: it holds the number of daily female births in California in 1959. We’ll have a look at it now. 9.4.1.1 Load the data [❋❋❋❋] Use local caching for this dataset using conditional exectution of download.file(). Load the data and use the lubridate package so that the Date column becomes a Date type. 9.4.1.2 Check for missing rows [❋❋❋] Check if there is really not a single day missing in this reported year. Hint: one option is to look at the lag() between subsequent rows. 9.4.1.3 Report birth numbers [❋❋❋] Report the number of births per month (Jan - Dec) and per weekday (Mon - Sun), as barplots. Is there a month or day that seems to be anomalous, or do you see seasonal trends? Try a statistical test! 9.4.2 Diabetes The datasets repo of this course contains a folder named UCI_diabetes_dataset. The complete dataset contains various types of information for 70 diabetes patients. Study the Readme file of this dataset before proceeding. 9.4.2.1 Create a codebook [❋❋] Copy-and-paste the Code Fields into a file named codebook.txt and load it as a dataframe with name codebook_diab. 9.5 The stringr package "],["exercise-solutions.html", "Chapter 10 Exercise Solutions 10.1 Introduction 10.2 The ggplot2 and tidyr packages 10.3 The tidyr and dplyr packages 10.4 The lubridate package 10.5 The stringr package", " Chapter 10 Exercise Solutions 10.1 Introduction Only solutions chunks are displayed here, not the output - except for the plots. 10.2 The ggplot2 and tidyr packages library(ggplot2) library(tidyr) library(dplyr) library(lubridate) 10.2.1 Trees A [❋] ggplot(data = trees, mapping = aes(x = Girth, y = Height)) + geom_point(color = &quot;blue&quot;) + xlab(&#39;Girth (in)&#39;) + ylab(&#39;Height (ft)&#39;) B [❋❋] ggplot(data = trees, mapping = aes(x = Girth, y = Height)) + geom_point(color = &quot;blue&quot;) + geom_smooth(method = &quot;lm&quot;, se = FALSE) + xlab(&#39;Girth (in)&#39;) + ylab(&#39;Height (ft)&#39;) C [❋❋] Final figure. ggplot(data = trees, mapping = aes(x = Girth, y = Height)) + geom_point(aes(size = Volume), color = &quot;blue&quot;, alpha = 0.6) + geom_smooth(method = &quot;lm&quot;, se = FALSE) + xlab(&#39;Girth (in)&#39;) + ylab(&#39;Height (ft)&#39;) + labs(size = &#39;Volume (ft^3)&#39;) ## `geom_smooth()` using formula &#39;y ~ x&#39; Figure 10.1: Tree girth, height and volume relationship 10.2.2 Insect Sprays The datasets package that is shipped with R has a dataset called ?. Type ?InsectSprays to get information on it. A [❋] ggplot(data = InsectSprays, mapping = aes(x = spray, y = count)) + geom_boxplot() + ylab(&quot;Insect count&quot;) + xlab(&quot;Insect spray&quot;) Figure 10.2: The counts of insects in agricultural experimental units treated with different insecticides B [❋❋] Create a jitter plot, splitting on the spray type and jittering minimally on the y-axis. Make sure that overlapping plot symbols do not hide each other and try to find a nice color. [Extra: Give each spray a different plot symbol.] ggplot(data = InsectSprays, mapping = aes(x = spray, y = count, color = spray)) + geom_jitter(height = 0, width = 0.1, shape = 18, size = 2, alpha = 0.7) + ylab(&quot;Insect count&quot;) + xlab(&quot;Insect spray&quot;) Figure 10.3: The counts of insects in agricultural experimental units treated with different insecticides 10.2.3 Diauxic growth A [❋❋] remote &lt;- &quot;https://raw.githubusercontent.com/MichielNoback/datasets/master/diauxic_growth/monod_diauxic_growth.csv&quot; local &lt;- &quot;../diauxic.csv&quot; #download.file(url = remote, destfile = local) diauxic &lt;- read.table(local, sep = &quot;;&quot;, header = T) diauxic &lt;- pivot_longer(data = diauxic, cols = -1, names_to = &quot;Substrate&quot;, values_to = &quot;OD&quot;) B [❋] diauxic$Substrate &lt;- factor(diauxic$Substrate, levels = c(&quot;GlucMann&quot;, &quot;GlucXyl&quot;, &quot;GlucArab&quot;, &quot;GlucRham&quot;), labels = c(&quot;Glucose Mannose&quot;, &quot;Glucose Xylose&quot;, &quot;Glucose Arabinose&quot;, &quot;Glucose Rhamnose&quot;)) C [❋❋] Create a line plot with all four growth curves within a single graph. ggplot(data = diauxic, mapping = aes(x = Time, y = OD, color = Substrate)) + geom_point() + stat_smooth(method = &quot;loess&quot;, se = FALSE, span = 0.3) + theme_bw() ## `geom_smooth()` using formula &#39;y ~ x&#39; Figure 10.4: Monod’s Diauxic shift experiment. D [❋❋❋] ggplot(data = diauxic, mapping = aes(x = Time, y = OD)) + geom_point() + stat_smooth(method = &quot;loess&quot;, se = FALSE, span = 0.3) + facet_wrap(. ~ Substrate, nrow = 2) + theme_bw() ## `geom_smooth()` using formula &#39;y ~ x&#39; Figure 10.5: Monod’s Diauxic shift experiment. 10.2.4 Virginia Death Rates library(dplyr) ## %&gt;% is used to pipe results from one operation to the other, just like &#39;|&#39; in Linux. virginia_death_rates &lt;- as_tibble(VADeaths) virginia_death_rates &lt;- virginia_death_rates %&gt;% mutate(&quot;Age Group&quot; = factor(rownames(virginia_death_rates), ordered = TRUE)) %&gt;% select(`Age Group`, everything()) #reorder the columns A (❋❋;❋) Pivot this table to long (tidy) format. This should generate a dataframe with four columns: Age Group, Habitat, Gender and DeathRate. virginia_death_rates &lt;- virginia_death_rates %&gt;% pivot_longer(cols = -1, names_to = c(&quot;Habitat&quot;, &quot;Gender&quot;), names_sep = &quot; &quot;, values_to = &quot;DeathRate&quot;) B (❋❋) ggplot(data = virginia_death_rates, aes(Gender)) + geom_bar(aes(weight = DeathRate, fill = Habitat), position = &quot;dodge&quot;) Figure 10.6: Virginia Death rates 10.2.5 Investigate new visualization [❋❋] This assignment has no solution of course. It is included here solely to keep the numbering consistent for both assignment chapters. 10.2.6 ToothGrowth [❋❋] In my opinion, the boxplot is an excellent choice for this visualization. However, geom_jitter() could also work but is more of a hassle to split out on Dose. ggplot(data = ToothGrowth, mapping = aes(x = supp, y = len)) + geom_boxplot(aes(color = as.factor(dose))) + xlab(&quot;Supplement&quot;) + ylab(&quot;Tooth length (mm)&quot;) + labs(color = &quot;Dose (mg/day)&quot;) Figure 10.7: length of odontoblasts with vitamin C 10.2.7 Puromycin [❋❋❋] ggplot(data = Puromycin, mapping = aes(x = conc, y = rate, color = state)) + geom_point() + geom_smooth(method = &quot;loess&quot;, formula = y ~ log(x), se = F) Figure 10.8: reaction velocity versus substrate concentration 10.2.8 Global temperature Load the data. remote_file &lt;- &quot;https://raw.githubusercontent.com/MichielNoback/datasets/master/global_temperature/annual.csv&quot; local_file &lt;- &quot;../annual.csv&quot; if (! file.exists(local_file)) { download.file(remote_file, destfile = local_file) } global_temp &lt;- read.table(&quot;annual.csv&quot;, header = TRUE, sep = &quot;,&quot;) 10.2.8.1 Create a scatter-and-line-plot [❋❋] ggplot(data = global_temp, mapping = aes(x = Year, y = Mean, color = Source)) + geom_point(size = 0.5) + geom_line() + geom_smooth(se = FALSE, method = &quot;loess&quot;) + theme_bw() ## `geom_smooth()` using formula &#39;y ~ x&#39; Figure 10.9: Global temperature anomalies 10.2.8.2 Re-create the heatmap [❋❋❋] ggplot(data = global_temp[global_temp$Source == &quot;GCAG&quot;, ], mapping = aes(x = Year, y = 1)) + geom_tile(aes(fill = Mean), colour = &quot;white&quot;) + scale_fill_gradient2(low = &quot;blue&quot;, mid = &quot;white&quot;, high = &quot;red&quot;) + theme_bw() + theme(axis.text.y = element_blank(), axis.ticks.y = element_blank(), axis.title.y = element_blank()) Figure 10.10: Global temperature anomalies Note: rescaling the temperature from 0 to 1 may yield even better results. 10.2.9 Epilepsy drug trial 10.2.9.1 Load data [❋] epilepsy_file &lt;- &quot;https://raw.githubusercontent.com/MichielNoback/datasets/master/epilepsy/epilepsy.csv&quot; epilepsy &lt;- read.table(epilepsy_file, header = TRUE, sep = &quot;,&quot;) epilepsy$period &lt;- factor(epilepsy$period, ordered=TRUE) 10.2.9.2 Reorganize the data [❋❋] epilepsy &lt;- as_tibble( epilepsy[, c(&quot;subject&quot;,&quot;age&quot;,&quot;base&quot;,&quot;treatment&quot;,&quot;period&quot;,&quot;seizure.rate&quot;)]) 10.2.9.3 Create plots of seizure rates A [❋❋] I used the color aesthetic here: ggplot(data = epilepsy, mapping = aes(x = period, y = seizure.rate, color = treatment)) + geom_boxplot() Figure 10.11: Seizure rates with color This is the alternative with facet_wrap() ggplot(data = epilepsy, mapping = aes(x = period, y = seizure.rate)) + geom_boxplot() + facet_wrap(. ~ treatment) Figure 10.12: Seizure rates with facet wrap B [❋❋❋] Again, this could be done with color or facetting, but in this case shape is also an option (not shown). I used a little trick to separate the period for treated/untreated. tmp &lt;- epilepsy tmp$period &lt;- as.integer(tmp$period) tmp$period_offset &lt;- ifelse(epilepsy$treatment == &quot;placebo&quot;, tmp$period - 0.10, tmp$period + 0.10) #tmp ggplot(data = tmp, mapping = aes(x = period_offset, y = seizure.rate, color = treatment)) + geom_jitter(width = 0.05, alpha = 0.7) Figure 10.13: Seizure rates C [❋❋❋] ggplot(data = epilepsy, mapping = aes(x = period, y = seizure.rate, color = treatment)) + geom_boxplot(aes(fill = treatment), alpha = 0.2) + geom_jitter(data = tmp, aes(x = period_offset), width = 0.05, height = 0, alpha = 0.5, shape = 18, size = 2) + theme_bw() Figure 10.14: Seizure rates Note that it would have been nicer to omit the outliers from the boxplot. This is your challenge. There are many more variations possible! You should really explore some of these as practice. 10.2.9.4 A boxplot after correction [❋❋❋] Correction: epilepsy_corrected &lt;- aggregate( seizure.rate ~ subject + age + base + treatment, data = epilepsy, FUN = sum) epilepsy_corrected$seizure_rate_corrected &lt;- epilepsy_corrected$seizure.rate - epilepsy_corrected$base The plot: ggplot(data = epilepsy_corrected, mapping = aes(x = treatment, y = seizure_rate_corrected)) + geom_boxplot() Figure 10.15: Seizure rates after correction 10.2.9.5 Test for statistical significance [❋❋❋] placebo &lt;- epilepsy_corrected$seizure_rate_corrected[epilepsy_corrected$treatment == &quot;placebo&quot;] treated &lt;- epilepsy_corrected$seizure_rate_corrected[epilepsy_corrected$treatment == &quot;Progabide&quot;] t.test(x = placebo, y = treated) ## ## Welch Two Sample t-test ## ## data: placebo and treated ## t = 0.5, df = 53, p-value = 0.6 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -10.9 17.6 ## sample estimates: ## mean of x mean of y ## 3.607 0.226 ##better: t.test(seizure_rate_corrected ~ treatment, data = epilepsy_corrected) ## ## Welch Two Sample t-test ## ## data: seizure_rate_corrected by treatment ## t = 0.5, df = 53, p-value = 0.6 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -10.9 17.6 ## sample estimates: ## mean in group placebo mean in group Progabide ## 3.607 0.226 Note: you should actually test for normality sing the shapiro.test() function before embarking on a t-test analysis: shapiro.test(treated) shapiro.test(placebo) So this is not the correct test; you should use the Mann-Whitney U test. wilcox.test(seizure_rate_corrected ~ treatment, data = epilepsy_corrected) ## Warning in wilcox.test.default(x = c(5L, 5L, 5L, -3L, 0L, 23L, 3L, 3L, 2L, : ## cannot compute exact p-value with ties ## ## Wilcoxon rank sum test with continuity correction ## ## data: seizure_rate_corrected by treatment ## W = 559, p-value = 0.06 ## alternative hypothesis: true location shift is not equal to 0 10.2.10 Bacterial growth curves with Varioscan 10.2.10.1 Load, preprocess and tidy [❋❋❋❋] Load: growth_data_file &lt;- &quot;data/varioscan/2020-01-30_wide.csv&quot; growth_data &lt;- read.csv(growth_data_file) Preprocess. Applying corrctions to the dataset is not easy as it is; this is much easier when the dataframe is transposed so samples are in columns and timepoints in rows. Also, transposition will force all data into character because the first columns contain character data. To prevent this, they are removed (and stored), leaving only the numerical OD measurements. ## Add rownames to get a hold of them after transpose row_names &lt;- paste0(growth_data$Content, &#39;.&#39;, growth_data$.copy, &#39;.&#39;, rep(LETTERS[1:8], times = 12)) #letters A-H represent the 8 dilutions rownames(growth_data) &lt;- row_names ## remove non-numeric cols but store them first label_columns &lt;- growth_data[, 1:4] tmp &lt;- growth_data[, -(1:4)] ## transpose tmp &lt;- t(tmp) ## build a new dataframe/tibble, empty but with predefined number of rows growth_data_corr &lt;- tibble(.rows = nrow(tmp)) Carry out the corrections; can this be done more efficiently? Take the challenge! ##1 9 17 25 33 41 49 57 65 73 81 89 for (col_select in 1:8) { #Using correction columns 25-32 #Red.1: columns 1 to 8 growth_data_corr[, row_names[col_select]] &lt;- tmp[ , col_select] - tmp[ , col_select + 24] #Red.2: columns 9 to 16 growth_data_corr[, row_names[col_select + 8]] &lt;- tmp[ , col_select + 8] - tmp[ , col_select + 24] #Red.3: columns 17 to 24 growth_data_corr[, row_names[col_select + 16]] &lt;- tmp[, col_select + 16] - tmp[, col_select + 24] #Using correction columns 57-64 #White.1: columns 33 to 40 growth_data_corr[, row_names[col_select + 32]] &lt;- tmp[ , col_select + 32] - tmp[ , col_select + 56] #White.2: columns 41 to 48 growth_data_corr[, row_names[col_select + 40]] &lt;- tmp[ , col_select + 40] - tmp[ , col_select + 56] #White.3: columns 49 to 56 growth_data_corr[, row_names[col_select + 48]] &lt;- tmp[ , col_select + 48] - tmp[ , col_select + 56] #Using correction columns 89-96 #Elution.1: columns 65 to 72 growth_data_corr[, row_names[col_select + 64]] &lt;- tmp[ , col_select + 64] - tmp[ , col_select + 88] #Elution.2: columns 73 to 80 growth_data_corr[, row_names[col_select + 72]] &lt;- tmp[ , col_select + 72] - tmp[ , col_select + 88] #Elution.1: columns 81 to 89 growth_data_corr[, row_names[col_select + 80]] &lt;- tmp[ , col_select + 80] - tmp[ , col_select + 88] } growth_data_corr &lt;- growth_data_corr[, row_names[-c(25:32, 57:64, 89:96)]] Re-attach the ## Transpose back and re-attach the first columns and time series labels growth_data_corr &lt;- t(growth_data_corr) label_columns &lt;- label_columns[-c(25:32, 57:64, 89:96), ] growth_data_corr &lt;- cbind(label_columns, growth_data_corr) names(growth_data_corr) &lt;- c(colnames(label_columns), rownames(tmp)) Tidy: growth_data_tidy &lt;- growth_data_corr %&gt;% pivot_longer(cols = starts_with(&quot;T.&quot;), names_prefix = &quot;T.&quot;, names_to = &quot;Time&quot;, values_to = &quot;OD&quot;) %&gt;% mutate(Time = as.integer(Time)/60, Dilution = factor(Dilution, ordered = T), Copy = factor(.copy, ordered = T)) %&gt;% select(Content, Copy, Dilution, Time, OD) Note that using the names_prefix = \"T.\" argument makes the T. prefix disappear so converting time to a number is easier. This conversion is done in the last step of this workflow, and it is also converted to hours instead of minutes. Dilution and .copy are converted to factors so they can be used as discrete values in ggplot2. 10.2.10.2 Create a growth curve visualization [❋❋❋❋] Let’s first look at the three copies of the same sample. Can they maybe be merged? library(ggplot2) growth_data_tidy %&gt;% filter(Content == &quot;Red&quot;, ) %&gt;% ggplot(mapping = aes(x = Time, y = OD)) + geom_line(aes(color = Dilution, linetype = Copy)) + xlab(&quot;Time (H)&quot;) growth_data %&gt;% filter(.copy == 1 &amp; Dilution %in% c(0, 0.25, 2)) %&gt;% ggplot(mapping = aes(x = Time, y = OD)) + geom_line(aes(linetype = Dilution, color = Content)) + scale_color_manual() 10.2.11 The dinos 10.2.11.1 export to csv [❋❋] This exercise has no code. I exported the data and placed the csv files under data/dinos/ of this repo. 10.2.11.2 clean up and load codebook.csv Non-data lines were deleted. Lines 1-18 relate to skeleton data and 19-29 to footprint data. A [❋] The separator is “:” but it is not possible to define multi-character separators in read.table(). For now, simply use “:” codebook_file &lt;- &quot;data/dinos/codebook.csv&quot; codebook &lt;- read.table(codebook_file, sep = &quot;:&quot;, stringsAsFactors = FALSE) B [❋] names(codebook) &lt;- c(&#39;variable&#39;, &#39;description&#39;) C [❋❋] codebook$description &lt;- sub(&#39;^ &#39;, &#39;&#39;, codebook$description) D [❋❋] codebook$dataset &lt;- c(rep(&#39;skeleton&#39;, 18), rep(&#39;footprint&#39;, 11)) 10.2.11.3 Write a utility function [❋❋❋] get_description &lt;- function(column_name, dataset = &#39;skeleton&#39;) { if (! is.element(dataset, codebook$dataset)) { stop(paste0(&#39;dataset &#39;, dataset, &#39; does not exist&#39;)) } if (! is.element(column_name, codebook$variable)) { stop(paste0(&#39;column &#39;, column_name, &#39; does not exist&#39;)) } ##return the value codebook[codebook$dataset == dataset &amp; codebook$variable == column_name, &quot;description&quot;] } get_description(&#39;Dinosaur_gen&#39;, &#39;skeleton&#39;) ## [1] &quot;Number of genera of Dinosauria&quot; 10.2.11.4 Load skeleton data [❋] skeleton_file &lt;- &quot;data/dinos/skeletons.csv&quot; (skeleton &lt;- read.table(skeleton_file, sep = &quot;;&quot;, header = TRUE, dec = &quot;,&quot;, as.is = 1)) 10.2.11.5 Plot species versus time [❋❋❋] Note the use of the utility function previously created, get_description(). ggplot(data = skeleton, mapping = aes(x = Midpoint, y = log2(Total_spec))) + geom_point(aes(color = Epoch)) + geom_smooth(method = &quot;loess&quot;) + xlab(get_description(&quot;Midpoint&quot;)) + ylab(paste0(&quot;log2 of &quot;, get_description(&quot;Total_spec&quot;))) + scale_x_reverse() ## `geom_smooth()` using formula &#39;y ~ x&#39; Figure 10.16: A first visualization 10.2.11.6 Reproducing the publication figure A [❋❋] We have not yet dealt with dplyr so we’ll use base R for this. Note that the column Total_gen already contains the sum of all tetrapods. skeleton$genera_proportion &lt;- (skeleton$Dinosaur_gen / skeleton$Total_gen) * 100 skeleton$specimen_proportion &lt;- (skeleton$Dinosaur_spec / skeleton$Total_spec) * 100 B [❋❋❋] skeleton_proportions &lt;- aggregate( cbind(genera_proportion, specimen_proportion) ~ Epoch + Stage + Midpoint, data = skeleton, FUN = mean) C [❋❋❋] skeleton_proportions_long &lt;- pivot_longer(data = skeleton_proportions, names_to = &quot;level&quot;, values_to = &quot;Proportion&quot;, cols = ends_with(&quot;_proportion&quot;)) D [❋❋❋❋] First it needs to be flattened for use in ggplot2: ggplot(data = skeleton_proportions_long, mapping = aes(x = Midpoint, y = Proportion, color = level)) + geom_point() + scale_x_reverse() + scale_color_manual(values = c(&quot;blue&quot;, &quot;red&quot;)) + xlab(&quot;Geological age (Myr)&quot;) + annotate(&quot;point&quot;, x = 231, y = 1, shape = 23, size = 5, fill = &quot;goldenrod1&quot;) + annotate(&quot;text&quot;, x = 231, y = -3, label = &quot;CPE&quot;) Figure 10.17: A reproduction E [❋❋❋❋❋] Actually, this is not that simple to do with base R. What you want is a function that calculates the mean over a time period or over a fixed sequence of numbers. We’ll take the mean over a sequence of 3 numbers. This could be a possible solution, using a for loop. rows &lt;- nrow(skeleton_proportions) moving_average_spec &lt;- numeric(rows) moving_average_gen &lt;- numeric(rows) n &lt;- 2 ## reversed traversal! for (i in rows:1) { if (i &gt;= rows - n) { series &lt;- i:rows } else { series &lt;- i:(i+n) ## beware of the parentheses! } moving_average_gen[i] &lt;- mean(skeleton_proportions$genera_prop[series]) moving_average_spec[i] &lt;- mean(skeleton_proportions$specimen_prop[series]) } ##attach to the proportions dataframe skeleton_proportions$specimen_moving_average &lt;- moving_average_spec skeleton_proportions$genera_moving_average &lt;- moving_average_gen #skeleton_proportions &lt;- skeleton_proportions[, c(1:4, 6, 5, 7)] ## recreate the long form. Note the use of `.value` which is a critical aspect! skeleton_prop_avg_long &lt;- pivot_longer(data = skeleton_proportions, cols = matches(&quot;_&quot;), names_pattern = &quot;(genera|specimen)_(proportion|moving_average)&quot;, names_to = c(&quot;level&quot;,&quot;.value&quot;)) Finally, plot: ggplot(data = skeleton_prop_avg_long) + geom_point(mapping = aes(x = Midpoint, y = proportion, color = level)) + geom_line(mapping = aes(x = Midpoint, y = moving_average, color = level)) + scale_x_reverse() + scale_color_manual(values = c(&quot;blue&quot;, &quot;red&quot;)) + xlab(&quot;Geological age (Myr)&quot;) + annotate(&quot;point&quot;, x = 231, y = 1, shape = 23, size = 5, fill = &quot;goldenrod1&quot;) + annotate(&quot;text&quot;, x = 231, y = -3, label = &quot;CPE&quot;) Figure 10.18: A reproduction [THIS IS NOT CORRECT YET!!] 10.2.11.7 Make a better figure [❋❋❋❋] The Loess regression curve is a favorite of mine. ggplot(data = skeleton_prop_avg_long, mapping = aes(x = Midpoint, y = proportion, color = level)) + geom_point() + geom_smooth(method = &quot;loess&quot;, se = FALSE) + scale_x_reverse() + scale_color_manual(values = c(&quot;blue&quot;, &quot;red&quot;)) + xlab(&quot;Geological age (Myr)&quot;) + annotate(&quot;point&quot;, x = 231, y = 1, shape = 23, size = 5, fill = &quot;goldenrod1&quot;) + annotate(&quot;text&quot;, x = 231, y = -3, label = &quot;CPE&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; Figure 10.19: A reproduction 10.2.11.8 Use the size aesthetic [❋❋❋❋❋] We need more data for this, the totals for each “Stage”: ## re-create mean proportions skeleton_proportions &lt;- aggregate( cbind(genera_proportion, specimen_proportion) ~ Epoch + Stage + Midpoint, data = skeleton, FUN = mean) ## first sum the finds: skeleton_totals &lt;- aggregate( cbind(Total_gen, Total_spec) ~ Midpoint, data = skeleton, FUN = sum) tmp &lt;- names(skeleton_totals) tmp[2] &lt;- &quot;genera_totals&quot; tmp[3] &lt;- &quot;specimen_totals&quot; names(skeleton_totals) &lt;- tmp ## combine with proportions skeleton_prop_and_totals &lt;- merge(skeleton_proportions, skeleton_totals, by = &quot;Midpoint&quot;) ## reshuffling is required for pivot_longer to work as expected!!! skeleton_prop_and_totals &lt;- skeleton_prop_and_totals[, c(1:4, 6, 5, 7)] ## again this used to work but not anymore (skeleton_prop_totals_long &lt;- pivot_longer( data = skeleton_prop_and_totals, cols = matches(&quot;_&quot;), names_pattern = &quot;(genera|specimen)_(proportion|totals)&quot;, #values_to = c(&quot;proportion&quot;, &quot;totals&quot;), names_to = c(&quot;level&quot;, &quot;.value&quot;))) ## # A tibble: 42 x 6 ## Midpoint Epoch Stage level proportion totals ## &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; ## 1 185. Jurassic (Ear… Pliensbachian (u) genera 44.4 9 ## 2 185. Jurassic (Ear… Pliensbachian (u) specim… 67.9 28 ## 3 189. Jurassic (Ear… Sinemurian (u) - Pliensbac… genera 42.1 19 ## 4 189. Jurassic (Ear… Sinemurian (u) - Pliensbac… specim… 35.3 85 ## 5 191. Jurassic (Ear… Sinemurian (u) - Pliensbac… genera 77.8 30 ## 6 191. Jurassic (Ear… Sinemurian (u) - Pliensbac… specim… 72.3 424 ## 7 195. Jurassic (Ear… Sinemurian (l,u) genera 33.3 21 ## 8 195. Jurassic (Ear… Sinemurian (l,u) specim… 21.4 42 ## 9 200. Jurassic (Ear… Hettangian genera 60.7 13 ## 10 200. Jurassic (Ear… Hettangian specim… 73.6 152 ## # … with 32 more rows Now for the plot ggplot(data = skeleton_prop_totals_long, mapping = aes(x = Midpoint, y = proportion, color = level, size = totals)) + geom_point() + geom_smooth(method = &quot;loess&quot;, se = FALSE) + scale_x_reverse() + scale_color_manual(values = c(&quot;blue&quot;, &quot;red&quot;)) + xlab(&quot;Geological age (Myr)&quot;) + annotate(&quot;point&quot;, x = 231, y = 1, shape = 23, size = 5, fill = &quot;goldenrod1&quot;) + annotate(&quot;text&quot;, x = 231, y = -3, label = &quot;CPE&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; Figure 10.20: A reproduction 10.3 The tidyr and dplyr packages 10.3.1 Global temperature revisited A [❋❋] global_temp %&gt;% filter(Source == &quot;GCAG&quot; &amp; Year &lt; 1970 &amp; Mean &gt; 0.1) global_temp %&gt;% filter(Source == &quot;GISTEMP&quot; &amp; Year &lt; 1970 &amp; Mean &gt; 0.1) B [❋❋] global_temp %&gt;% filter(Source == &quot;GCAG&quot; &amp; Year &gt; 1945) %&gt;% filter(Mean == min(Mean)) global_temp %&gt;% filter(Source == &quot;GISTEMP&quot; &amp; Year &gt; 1945) %&gt;% filter(Mean == min(Mean)) C [❋❋❋] This is a case for filter(row_number() ...) or slice(): global_temp %&gt;% group_by(Source) %&gt;% arrange(desc(Mean)) %&gt;% slice(1:5) %&gt;% #filter(row_number() %in% 1:5) %&gt;% #works as well ungroup() D [❋❋❋] This is a case for splitting with cut(). Package dplyr has an equivalent named ntile(). cut(global_temp$Year, breaks = seq(min(global_temp$Year), max(global_temp$Year), 10)) global_temp %&gt;% filter(Year &gt;= 1970) %&gt;% mutate(Decade = cut(Year, breaks = seq(1970, 2020, 10), labels = seq(1970, 2010, 10), right = FALSE)) %&gt;% group_by(Decade, Source) %&gt;% summarize(Mean = mean(Mean)) 10.3.2 ChickWeight Starting weight [❋❋]. ChickWeight %&gt;% filter(Time == 0) %&gt;% arrange(desc(weight)) %&gt;% select(Chick, Diet, weight) %&gt;% head(1) Total weight gain [❋❋❋]. ChickWeight %&gt;% group_by(Chick) %&gt;% mutate(weight_gain = weight - lag(weight)) %&gt;% summarize(timepoints = n(), total_weight_gain = sum(weight_gain, na.rm = T), Diet = first(Diet)) %&gt;% arrange(desc(total_weight_gain)) %&gt;% head(4) Note: you could also have selected the first and last measurement for each chic of course, but not simply the last timepoint since several chickens had died by then. Average weight gain per diet [❋❋❋❋]. ChickWeight %&gt;% group_by(Chick) %&gt;% mutate(weight_gain = weight - lag(weight)) %&gt;% summarize(timepoints = n(), total_weight_gain = sum(weight_gain, na.rm = T), Diet = unique(Diet)) %&gt;% ungroup() %&gt;% group_by(Diet) %&gt;% summarize(num_chicks = n(), average_weight_gain = mean(total_weight_gain)) 10.3.3 Population numbers 10.3.3.1 Load the data [❋] pop_data_file &lt;- &quot;https://raw.githubusercontent.com/MichielNoback/datasets/master/population/EDU_DEM_05022020113225751.csv&quot; local_file &lt;- &quot;population_data.csv&quot; if (! file.exists(local_file)) { download.file(pop_data_file, destfile = local_file) } population &lt;- read.table(local_file, header = TRUE, sep=&quot;,&quot;) 10.3.3.2 Clean up [❋❋ Which? str(population) tells me that Unit.Code, Unit and PowerCode are factors with one level. Using table(population$PowerCode.Code, useNA = \"always\") tells me that there are only zeros there. Same for Reference.Period.Code and Reference.Period. The variables Flags and Flag.Codes refer to the same, so one of them can be removed (I choose to remove Flag.Codes). The same counts for SEX/Sex, AGE/Age and YEAR/Year. Select This is my selection: keep &lt;- names(population)[c(1, 2, 4, 6, 8, 15, 16)] keep population &lt;- as_tibble(population[, keep]) #tibble is nicer! head(population) 10.3.3.3 Create a “wide” yearly report of totals [❋❋❋ pop_totals &lt;- dplyr::filter(population, Sex == &quot;Total&quot; &amp; Age == &quot;Total: All age groups&quot;) ##or, using base R #population[population$Sex == &quot;Total&quot; &amp; population$Age == &quot;Total: All age groups&quot;, ] pop_totals_wide &lt;- pivot_wider( data = pop_totals[, c(1, 2, 5, 6)], names_from = Year, values_from = Value) pop_totals_wide 10.3.3.4 Create a “wide” yearly report of population change [❋❋❋❋] You could do this on the pop_totals dataset using the lag() function, after using group_by(), but since we have the wide format, you could also use a simple for loop, iterating the columns by index and subtracting the first from the second. I will demonstrate the only the first. pop_totals %&gt;% group_by(Country) %&gt;% mutate(Pop_change = as.integer(Value - lag(Value))) %&gt;% ungroup() %&gt;% select(-Value) %&gt;% pivot_wider(names_from = Year, values_from = Pop_change) %&gt;% select(-`2005`, -`2010`, -Flag.Codes) ## backticks in above selection are required ## because we are selecting names that are numbers! 10.3.3.5 Create a bar plot [❋❋❋ sel &lt;- population %&gt;% filter(Sex == &quot;Women&quot; | Sex == &quot;Men&quot;) %&gt;% filter(COUNTRY %in% c(&quot;BEL&quot;, &quot;CHE&quot;, &quot;DNK&quot;, &quot;FRA&quot;, &quot;IRL&quot;, &quot;DEU&quot;, &quot;LUX&quot;, &quot;NLD&quot;, &quot;GBR&quot;)) %&gt;% drop_na() ggplot(sel, mapping = aes(Year)) + geom_bar(aes(weight = Value, fill = Sex)) + facet_wrap(. ~ COUNTRY) Figure 10.21: Barplot of population numbers in some European countries 10.3.3.6 Highest growth rate [❋❋❋❋] population %&gt;% filter(Sex == &quot;Total&quot; &amp; Age == &quot;Total: All age groups&quot; &amp; (Year == 2005 | Year == 2017)) %&gt;% group_by(Country) %&gt;% mutate(Change = Value - lag(Value), Previous = lag(Value)) %&gt;% mutate(GrowthRate = Change / Previous * 100) %&gt;% ungroup() %&gt;% select(COUNTRY, Country, GrowthRate ) %&gt;% arrange(desc(GrowthRate)) %&gt;% head(3) 10.3.4 Rubella and Measles No solutions (yet) for this exercise. 10.4 The lubridate package 10.4.1 Female births 10.4.1.1 Load the data [❋❋] remote_file &lt;- &quot;https://raw.githubusercontent.com/MichielNoback/datasets/master/female_births/daily-total-female-births.csv&quot; local_file &lt;- &quot;../female_births.csv&quot; if (! file.exists(local_file)) { download.file(remote_file, destfile = local_file) } female_births &lt;- read.table(local_file, header = TRUE, sep = &quot;,&quot;, stringsAsFactors = F) female_births &lt;- female_births %&gt;% mutate(Date = as_date(female_births$Date)) (female_births &lt;- as_tibble(female_births)) Note that in this case as_tibble(female_births) with the original dataframe would have converted the Date column to date objects as well. 10.4.1.2 Check for missing rows [❋❋❋] female_births %&gt;% mutate(lagged = lag(Date), diff = as.integer(Date - lagged)) %&gt;% filter(is.na(diff) | diff != 1) 10.4.1.3 Report birth numbers [❋❋❋] Report the number of births per month (Jan - Dec) and per weekday (Mon - Sun), as barplots. Is there a month or day that seems to be anomalous, or do you see seasonal trends? Try a statistical test! First create the required columns: female_births &lt;- female_births %&gt;% mutate(Day = wday(Date, label = T), Month = month(Date, label = T)) Report monthly counts, as barplot. #library(ggplot2) female_births %&gt;% ggplot(mapping = aes(x = Month, weight = Births)) + geom_bar(fill = &quot;darkblue&quot;) + theme_bw() Figure 10.22: Monthly birth counts The days library(ggplot2) female_births %&gt;% ggplot(mapping = aes(x = Day, weight = Births)) + geom_bar(fill = &quot;darkblue&quot;) + theme_bw() Figure 10.23: Daily birth counts 10.4.2 Diabetes The datasets repo of this course contains a folder named UCI_diabetes_dataset. The complete dataset contains various types of information for 70 diabetes patients. Study the Readme file of this dataset before proceeding. 10.4.2.1 Create a codebook [❋❋] Copy-and-paste the Code Fields into a file named codebook.txt and load it as a dataframe with name codebook_diab. codebook_diab &lt;- read.table(&quot;data/diabetes_codebook.txt&quot;, sep = &quot;,&quot;, header = TRUE) codebook_diab &lt;- as_tibble(codebook_diab) head(codebook_diab) ## # A tibble: 6 x 2 ## Code Description ## &lt;int&gt; &lt;chr&gt; ## 1 33 Regular insulin dose ## 2 34 NPH insulin dose ## 3 35 UltraLente insulin dose ## 4 48 Unspecified blood glucose measurement ## 5 57 Unspecified blood glucose measurement ## 6 58 Pre-breakfast blood glucose measurement 10.4.2.2 Create a function to load patient data [❋❋❋] Create a function -load_patient_data(&lt;patient-number&gt;)- that can be used to load data for specific patients from file and return this as a dataframe. If you access the remote copies of the files you should cache the data locally so a second requests for the same data will return a local copy of the file. The function should give a friendly error message when there is no data for the specific patient (e.g. patient 99 is not present in the dataset). As extra challenge, you could read the documentation on the tryCatch() function and try to use this for your remote file access. load_patient_data &lt;- function(patient_number) { base_url &lt;-&quot;https://raw.githubusercontent.com/MichielNoback/datasets/master/UCI_diabetes_dataset/data-&quot; if (patient_number &lt;= 9) patient_number &lt;- paste0(&quot;0&quot;, patient_number) remote_file &lt;- paste0(base_url, patient_number) local_file &lt;- paste0(&quot;../diab-patient-&quot;, patient_number) column_names &lt;- c(&quot;Date&quot;, &quot;Time&quot;, &quot;Code&quot;, &quot;Value&quot;) #format: #04-21-1991 9:09 58 100 result &lt;- NULL if (! file.exists(local_file)) { result &lt;- tryCatch( {#success message(paste0(&quot;downloading &quot;, remote_file)) download.file(remote_file, destfile = local_file) patient_data &lt;- read.table(local_file, sep = &quot;\\t&quot;, stringsAsFactors = F) names(patient_data) &lt;- column_names return(patient_data) }, error = function(cond) { message(paste(&quot;Remote file does not seem to exist:&quot;, remote_file)) message(&quot;Here&#39;s the original error message:&quot;) message(cond) return(NULL) }, warning = function(cond) { message(paste(&quot;URL caused a warning:&quot;, remote_file)) message(&quot;Here&#39;s the original warning message:&quot;) message(cond) return(NULL) }) } else { message(paste0(&quot;reading local cache of&quot;, local_file)) result &lt;- read.table(local_file, sep = &quot;\\t&quot;, stringsAsFactors = F) names(result) &lt;- column_names } return(result) } patient8 &lt;- load_patient_data(8) ## reading local cache of../diab-patient-08 head(patient8) ## Date Time Code Value ## 1 07-31-1990 12:09 60 100 ## 2 07-31-1990 17:28 62 190 ## 3 07-31-1990 18:41 62 124 ## 4 08-01-1990 05:58 58 222 ## 5 08-01-1990 12:57 60 339 ## 6 08-01-1990 13:00 68 0 str(patient8) ## &#39;data.frame&#39;: 177 obs. of 4 variables: ## $ Date : chr &quot;07-31-1990&quot; &quot;07-31-1990&quot; &quot;07-31-1990&quot; &quot;08-01-1990&quot; ... ## $ Time : chr &quot;12:09&quot; &quot;17:28&quot; &quot;18:41&quot; &quot;05:58&quot; ... ## $ Code : int 60 62 62 58 60 68 60 61 62 65 ... ## $ Value: int 100 190 124 222 339 0 71 72 193 0 ... B$ 10.5 The stringr package "],["parsing-complex-data.html", "Chapter 11 Parsing complex data", " Chapter 11 Parsing complex data This worked example shows that R can be used to parse complex text data as well. The data that is going to be parsed here is from a machine called Varioscan. It concerns a 96-well plate reading of a growth curve of the bacterium Staphylococcus aureus. The research question was this: \"Do anthocyans from red tulips inhibit growth of the bacterium Staphylococcus aureus? The experimental setup was as follows. Bacteria were grown in Nutrient Broth in a 96-well plate. Growth was measured through Optical Density every 10 minutes. The rows represent dilution series of the test substrate; every row is a twofold dilution of the previous, starting at 2% and ending at 0% (control). A: 2%, B: 1%, C: 0.5, D: 0.25, E: 0.125, F: 0.0625, G: 0.03125, H: 0%. The columns represent an ordered experimental control setup: Columns 1-3: Red tulip with bacteria Column 4: Red tulip without bacteria Columns 5-7: White tulip with bacteria Column 8: White tulip without bacteria Column 9-11: Elution fluid (maltodextrin) with bacteria Column 12: Elution fluid without bacteria The output of this machine has a terrible format that is impossible to process by any of the read.xxxx() functions. The data is segmented in blocks that represent a single measurement of the 96-well plate, and the blocks are separated to represent a 10-minute reading interval. Here is the data for the first complete block with some leading and trailing lines (omitting the first): Photometric1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;; ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;; Plate 1: 1, reading 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;; ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;; Sample;1;2;3;4;5;6;7;8;9;10;11;12;;;;;;;;;;;;;;;;;;;;;;;;;;;;; A;Un_0001 1/1;Un_0009 1/1;Un_0017 1/1;Un_0025 1/1;Un_0033 1/1;Un_0041 1/1;Un_0049 1/1;Un_0057 1/1;Un_0065 1/1;Un_0073 1/1;Un_0081 1/1;Un_0089 1/1;;;;;;;;;;;;;;;;;;;;;;;;;;;;; B;Un_0002 1/1;Un_0010 1/1;Un_0018 1/1;Un_0026 1/1;Un_0034 1/1;Un_0042 1/1;Un_0050 1/1;Un_0058 1/1;Un_0066 1/1;Un_0074 1/1;Un_0082 1/1;Un_0090 1/1;;;;;;;;;;;;;;;;;;;;;;;;;;;;; C;Un_0003 1/1;Un_0011 1/1;Un_0019 1/1;Un_0027 1/1;Un_0035 1/1;Un_0043 1/1;Un_0051 1/1;Un_0059 1/1;Un_0067 1/1;Un_0075 1/1;Un_0083 1/1;Un_0091 1/1;;;;;;;;;;;;;;;;;;;;;;;;;;;;; D;Un_0004 1/1;Un_0012 1/1;Un_0020 1/1;Un_0028 1/1;Un_0036 1/1;Un_0044 1/1;Un_0052 1/1;Un_0060 1/1;Un_0068 1/1;Un_0076 1/1;Un_0084 1/1;Un_0092 1/1;;;;;;;;;;;;;;;;;;;;;;;;;;;;; E;Un_0005 1/1;Un_0013 1/1;Un_0021 1/1;Un_0029 1/1;Un_0037 1/1;Un_0045 1/1;Un_0053 1/1;Un_0061 1/1;Un_0069 1/1;Un_0077 1/1;Un_0085 1/1;Un_0093 1/1;;;;;;;;;;;;;;;;;;;;;;;;;;;;; F;Un_0006 1/1;Un_0014 1/1;Un_0022 1/1;Un_0030 1/1;Un_0038 1/1;Un_0046 1/1;Un_0054 1/1;Un_0062 1/1;Un_0070 1/1;Un_0078 1/1;Un_0086 1/1;Un_0094 1/1;;;;;;;;;;;;;;;;;;;;;;;;;;;;; G;Un_0007 1/1;Un_0015 1/1;Un_0023 1/1;Un_0031 1/1;Un_0039 1/1;Un_0047 1/1;Un_0055 1/1;Un_0063 1/1;Un_0071 1/1;Un_0079 1/1;Un_0087 1/1;Un_0095 1/1;;;;;;;;;;;;;;;;;;;;;;;;;;;;; H;Un_0008 1/1;Un_0016 1/1;Un_0024 1/1;Un_0032 1/1;Un_0040 1/1;Un_0048 1/1;Un_0056 1/1;Un_0064 1/1;Un_0072 1/1;Un_0080 1/1;Un_0088 1/1;Un_0096 1/1;;;;;;;;;;;;;;;;;;;;;;;;;;;;; ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;; Value;1;2;3;4;5;6;7;8;9;10;11;12;;;;;;;;;;;;;;;;;;;;;;;;;;;;; A;0,267552;0,272019;0,263387;0,278566;0,0984628;0,0936923;0,107464;0,105546;0,106577;0,091187;0,101332;0,10287;;;;;;;;;;;;;;;;;;;;;;;;;;;;; B;0,252371;0,246752;0,252471;0,258912;0,109206;0,0992488;0,10794;0,112641;0,112533;0,123722;0,092579;0,101677;;;;;;;;;;;;;;;;;;;;;;;;;;;;; C;0,217063;0,217046;0,213377;0,205835;0,0998421;0,105516;0,10702;0,103577;0,106884;0,0940363;0,0939646;0,0928696;;;;;;;;;;;;;;;;;;;;;;;;;;;;; D;0,215713;0,206423;0,225242;0,207882;0,101945;0,110656;0,101335;0,113771;0,10745;0,0974231;0,0959958;0,102336;;;;;;;;;;;;;;;;;;;;;;;;;;;;; E;0,201186;0,185729;0,191045;0,191651;0,112556;0,120355;0,155579;0,102002;0,101704;0,0888485;0,0985116;0,117562;;;;;;;;;;;;;;;;;;;;;;;;;;;;; F;0,160001;0,151447;0,156737;0,153896;0,11059;0,115471;0,107786;0,106571;0,109566;0,106598;0,0940017;0,100102;;;;;;;;;;;;;;;;;;;;;;;;;;;;; G;0,137258;0,129698;0,128936;0,131165;0,105271;0,112039;0,106057;0,0992461;0,109021;0,120979;0,0926567;0,112816;;;;;;;;;;;;;;;;;;;;;;;;;;;;; H;0,108073;0,10298;0,102386;0,108621;0,123152;0,104467;0,104612;0,0930304;0,0946673;0,103918;0,0982605;0,103816;;;;;;;;;;;;;;;;;;;;;;;;;;;;; ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;; Plate 1: 1, reading 2;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;; ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;; Sample;1;2;3;4;5;6;7;8;9;10;11;12;;;;;;;;;;;;;;;;;;;;;;;;;;;;; A;Un_0001 1/1;Un_0009 1/1;Un_0017 1/1;Un_0025 1/1;Un_0033 1/1;Un_0041 1/1;Un_0049 1/1;Un_0057 1/1;Un_0065 1/1;Un_0073 1/1;Un_0081 1/1;Un_0089 1/1;;;;;;;;;;;;;;;;;;;;;;;;;;;;; Here, a strategy is demonstrated for when your data cannot be simply read into a dataframe directly. First, load all data in a single character vector, on line per element: file &lt;- &quot;data/varioscan/2020-01-30.csv&quot; all_data &lt;- readLines(file) head(all_data[-1]) ##omitting empty lines ## [1] &quot;Photometric1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;&quot; ## [2] &quot;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;&quot; ## [3] &quot;Plate 1: 1, reading 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;&quot; ## [4] &quot;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;&quot; ## [5] &quot;Sample;1;2;3;4;5;6;7;8;9;10;11;12;;;;;;;;;;;;;;;;;;;;;;;;;;;;;&quot; ## [6] &quot; A;Un_0001 1/1;Un_0009 1/1;Un_0017 1/1;Un_0025 1/1;Un_0033 1/1;Un_0041 1/1;Un_0049 1/1;Un_0057 1/1;Un_0065 1/1;Un_0073 1/1;Un_0081 1/1;Un_0089 1/1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;&quot; Next, remove unwanted header lines: all_data &lt;- all_data[-c(1, 2, 3)] What is left is a vector with nice periodicity: recurring blocks of 22 elements with this structure: element 1: reading count element 2: empty line element 3-11: sample layout, which is always the same (so reading is required only once!) element 13-21: actual OD data element 22: empty line Let’s read the first block - the sample layout. Note that there are many empty columns and these are removed as well. samples &lt;- read.table(text = all_data[3:11], sep = &quot;;&quot;, header = T) samples &lt;- samples[, -(14:42)] The experimental setup needs to be added as well. new_names &lt;- c(&quot;Dilution&quot;, rep(&quot;Red&quot;, 3), &quot;Red_w_o&quot;, rep(&quot;White&quot;, 3), &quot;White_w_o&quot;, rep(&quot;Elution&quot;, 3), &quot;Elution_w_o&quot;) names(samples) &lt;- new_names dilutions &lt;- c(2, 1, 0.5, 0.25, 0.125, 0.0625, 0.03125, 0) samples$Dilution &lt;- dilutions Finally, we need a set of columns to add the OD measurements to. The pivot_longer() function is perfect for this. For the next phase, the sample data needs to be ordered by Sample ID, so that is done as well. growth_data &lt;- samples %&gt;% pivot_longer(cols =-1, names_to = &quot;Content&quot;, values_to = &quot;Sample&quot;) %&gt;% arrange(Sample) Note the warning Duplicate column names detected, adding .copy variable. This is not a bug but a feature! In the next step we need to loop the entire file, essentially doing the same thing with all OD measurements: read, flatten and attach. line_count &lt;- length(all_data) time = 0 for(n in seq(from = 13, to = line_count, by = 22)) { values &lt;- read.table(text = all_data[(n+1):(n+8)], sep = &quot;;&quot;, dec = &quot;,&quot;) values &lt;- values[, -(14:42)] values &lt;- as.matrix(values[, -1]) dim(values) &lt;- NULL growth_data[, paste0(&quot;T.&quot;, time)] &lt;- values time &lt;- time + 10 } dim(growth_data) ## [1] 96 112 Now everything is in a nice dataframe, but it is not tidy yet, of course, nor is any other processing performed required for correct visualization and analysis of this dataset. Let’s save it as it is though: write.csv(growth_data, file = &quot;data/varioscan/2020-01-30_wide.csv&quot;, row.names = FALSE) One of the exercises will address this dataset for tidying, processing and visualization. "],["references.html", "References", " References Wickham, Hadley. 2014. “Tidy Data.” The Journal of Statistical Software 59. http://www.jstatsoft.org/v59/i10/. Xie, Yihui. 2015. Dynamic Documents with R and Knitr. 2nd ed. Boca Raton, Florida: Chapman; Hall/CRC. http://yihui.name/knitr/. ———. 2019. Bookdown: Authoring Books and Technical Documents with r Markdown. Boca Raton, Florida: Chapman; Hall/CRC. https://bookdown.org/yihui/bookdown/. "]]
