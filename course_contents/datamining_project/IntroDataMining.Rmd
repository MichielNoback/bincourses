---
title: "What is Data Mining"
author: "Michiel Noback"
date: "27/8/2018"
output: ioslides_presentation
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## What is Data Mining?

![Chihuahua or Muffin?](figures/chihuahua_muffin.jpeg)

## Definitions

"Data mining is the process of discovering patterns in large data sets involving methods at the intersection of machine learning, statistics, and database systems."

"Machine learning (ML) is a field of artificial intelligence that uses statistical techniques to give computer systems the ability to "learn" (e.g., progressively improve performance on a specific task) from data, without being explicitly programmed."

_Wikipedia_

## Machine Learning vs Data Mining

- Still under debate, but...
- Data mining is carried out by a person, in a specific situation, on a particular data set, with a goal in mind.
- Machine learning on the other hand involves the study of algorithms that can extract information automatically (i.e., without on-line human guidance).

## More related terms

- Statistics 
  - more theory-based. focused on testing hypotheses  
- Machine learning
  - more heuristic, focused on building program that learns, more general than Data Mining 
- Knowledge Discovery
  - integrates theory and heuristics 
  - focus on the entire process of discovery, including data cleaning, learning, integration and visualization 
- Data Mining
  - focus on the algorithms to extract patterns from data 
- Distinctions are blurred!  

## In this project

We will focus on both:  

Apply data mining to explore the data in order to assess and improve it, and apply machine learning to obtain a good classifier.

## An example walk-through

In the next series of slides an entire analysis process is followed.  
It uses a fictitious case where loan application should be assessed.  

If the chance is big  of not getting your loan repaid, you obviously do not want to approve it!


## Example: credit assessment

*Given:* a loan application  
*Problem* (for the bank): should the bank approve the loan?  
*Data*: records from previous loans  

## Load and inspect

```{r}
loans <- read.csv("data/loan_demo_data.csv", comment.char="#")
loans$marital.status <- factor(loans$marital.status, 
                              levels = c("s","d","m"),
                              labels = c("single","divorced","married"))
loans$sex <- factor(loans$sex,
                    levels=c("m","f"),
                    labels=c("male","female"))
```

## Load and inspect

```{r}
head(loans)
```

## Visualize

```{r echo=FALSE}
library(ggplot2)
linecolor="#008080"
base_plot <- ggplot(data=loans, mapping = aes(x=age, y=salary, colour=default)) +
  geom_point(size=3, alpha=0.75) +
  scale_color_manual(values=c("blue", "red")) +
  theme(text = element_text(size=16))
base_plot
```

## Look for patterns

```{r echo=FALSE}
plot2 <- base_plot +
  geom_segment(aes(x=28, y=30, xend=68, yend=30), color=linecolor, size=1.2) 
plot2
```

- Above age 30, age is a perfect predictor of loan default  
- Below 30, there is no pattern...  

## Define rules

A first classification rule could be   

<code>
<span style="font-size:32px; color:OliveDrab">
  if income < 30 then default = true  
  else default = false  
</span>
</code>

and this already performs pretty good

## Include more data: sex

```{r echo=FALSE}
plot3 <- ggplot(data=loans, mapping = aes(x=age, y=salary, colour=default, shape=sex)) +
  geom_point(size=3, alpha=0.75) +
  scale_color_manual(values=c("blue", "red")) +
  geom_segment(aes(x=28, y=30, xend=68, yend=30), color=linecolor, size=1.2) +
  geom_segment(aes(x=28, y=20, xend=28, yend=70), color=linecolor, size=1.2) +
  theme(text = element_text(size=16))
plot3
```
- Below age 28, sex is a excellent predictor of loan defection!

## Refine rules

<code>
<span style="font-size:32px; color:OliveDrab">
  if income < 30 and age > 28 
    then default = true  
  if age < 28 and sex = m 
    then default = true  
  else default = false  
</span>
</code>

and now we have an almost perfect predictor

## The essence of Machine Learning

Find patterns in existing data to make predictions on new, unseen data.

![Apple or Orange?](figures/classify_fruit.png)

## New loan applicants

Now two loan applicants appear at you bank:  

- The first is a 56 year old female with an income of 49k$  
- The other is a 27 year old male with an income of 26k$  

Do you approve the loan?  

**You get a 200$ commission for approved loans, but don't get your 1000$ bonus  for a defaulted loan**

## Visualize the new data

```{r echo=FALSE}
plot3 +
  geom_point(mapping = aes(x=56, y=49), shape=19, color="#32CD32", size=3) +
  geom_point(mapping = aes(x=27, y=26), shape=17, color="#32CD32", size=3) 
```

## Assessing the loan applicants

- The 56 year old female with an income of 49k$  
  - Absolutely!  
- The other is a 27 year old male with an income of 26k$   
  - Not sure; need more input  

## Explore other variables

There is one more variable: marital status.

```{r}
plot4 <- ggplot(data=loans, mapping = aes(x=age, y=salary, colour=default, shape=marital.status)) +
  geom_point(size=3, alpha=0.75) +
  scale_color_manual(values=c("blue", "red")) +
  geom_segment(aes(x=28, y=30, xend=68, yend=30), color=linecolor, size=1.2) +
  theme(text = element_text(size=16))
plot4
```

## A different view of the data

Sometimes a different data presentation is better

```{r}
library(knitr)
library(kableExtra)

t <- table(loans$default, loans$marital.status)
kable(t) %>%
  kable_styling("striped", full_width = F)
```

Obviously, marital status is not randomly distributed!  
&nbsp;  
Marital status divorced means a much higher chance of loan defection

## Refine rules further  

We could have arrived on something like this  
  
<code>
<span style="font-size:32px; color:OliveDrab">
  if income < 30 and age > 28  
  &nbsp;&nbsp; then default = true  
  if age < 28 and sex = m  
  &nbsp;&nbsp; then default = true  
  if age < 31 and status is not married  
  &nbsp;&nbsp; then default = true  
  else default = false  
</span>
</code>

## Enter machine learning

Let's train a model using C4.5/J48

```{r echo=TRUE}
library(RWeka)
fit <- J48(default ~ ., data=loans[, -1]) ##leave customer ID out
#ALTERNATIVE
#library(C50)
#fit <- C5.0(default ~ ., data=loans[, -1])
```

Yes, it is this simple....seemingly  

## Have a look at the model

```{r}
fit
```

## Investigate the performance 

```{r}
predictions <- predict(fit, loans)
t2 <- table(loans$default, predictions)

colnames(t2) <- c("predicted no", "predicted yes")
rownames(t2) <- c("actual no", "actual yes")
kable(t2) %>%
  kable_styling("striped", full_width = F)
```

&nbsp;  

An accuracy of `r round(sum(loans$default==predictions) / length(predictions), 2)*100`%!

## And our two applicants  

```{r}
newData <- data.frame(
  salary =c(49, 26), 
  age=c(56, 27), 
  marital.status=c("m", "d"), 
  sex=c("f", "m"))
newData$marital.status <- factor(newData$marital.status, 
                              levels = c("s","d","m"),
                              labels = c("single","divorced","married"))
newData$sex <- factor(newData$sex,
                    levels=c("m","f"),
                    labels=c("male","female"))
```

```{r echo=TRUE}
print(newData)
```

----

```{r echo = TRUE}
predict(fit, newData)
```


## Another example: Iris

![Three Iris species](figures/iris-machinelearning.png)

## Two unknown Iris specimens

```{r echo=FALSE}
levels(iris$Species) <- c(levels(iris$Species), "Unknown1", "Unknown2")
iris <- rbind(iris, data.frame(Sepal.Length = c(5.5, 5),
                               Sepal.Width = c(3.2, 3.7),
                               Petal.Length = c(6, 1.3),
                               Petal.Width = c(2.2, 0.3),
                               Species = c("Unknown1", "Unknown2")))
g <- ggplot(data = iris, aes(x=Sepal.Length, y=Sepal.Width, color=Species)) +
  geom_point(size = 3) +
  labs(x = "Iris sepal length (mm)", y = "Iris sepal width (mm)") +
  theme(text = element_text(size=16))
g
```

Unknown 2 is obviously _setosa_, but Unknown 2 is apparently unclassifyable.  


## Another dimension

```{r echo=FALSE}
g <- ggplot(data=iris, aes(x=Petal.Length, y=Petal.Width, color=Species)) +
  geom_point(size=3) +
  labs(x="Iris petal length (mm)", y="Iris petal width (mm)") +
  theme(text = element_text(size=16))
g
```
Looking from a different 'angle', Unknown 1 is obviously _virginica_!

## Summary

- You have seen the data mining process at work
- The goal is to classify new, unseen instances correctly based on what you have learned from previously collected data
- Different visualizations of your data are an essential part of good research
